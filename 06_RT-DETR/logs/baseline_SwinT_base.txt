WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
name : pre.0.weight, param : torch.Size([96, 3, 4, 4])
name : pre.0.bias, param : torch.Size([96])
name : pre.2.weight, param : torch.Size([96])
name : pre.2.bias, param : torch.Size([96])
name : features.0.0.norm1.weight, param : torch.Size([96])
name : features.0.0.norm1.bias, param : torch.Size([96])
name : features.0.0.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.0.attn.relative_position_index, param : torch.Size([2401])
name : features.0.0.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.0.attn.qkv.bias, param : torch.Size([288])
name : features.0.0.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.0.attn.proj.bias, param : torch.Size([96])
name : features.0.0.norm2.weight, param : torch.Size([96])
name : features.0.0.norm2.bias, param : torch.Size([96])
name : features.0.0.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.0.mlp.0.bias, param : torch.Size([384])
name : features.0.0.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.0.mlp.3.bias, param : torch.Size([96])
name : features.0.0.norm1_skip.weight, param : torch.Size([96])
name : features.0.0.norm1_skip.bias, param : torch.Size([96])
name : features.0.0.norm2_skip.weight, param : torch.Size([96])
name : features.0.0.norm2_skip.bias, param : torch.Size([96])
name : features.0.1.norm1.weight, param : torch.Size([96])
name : features.0.1.norm1.bias, param : torch.Size([96])
name : features.0.1.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.1.attn.relative_position_index, param : torch.Size([2401])
name : features.0.1.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.1.attn.qkv.bias, param : torch.Size([288])
name : features.0.1.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.1.attn.proj.bias, param : torch.Size([96])
name : features.0.1.norm2.weight, param : torch.Size([96])
name : features.0.1.norm2.bias, param : torch.Size([96])
name : features.0.1.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.1.mlp.0.bias, param : torch.Size([384])
name : features.0.1.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.1.mlp.3.bias, param : torch.Size([96])
name : features.1.reduction.weight, param : torch.Size([192, 384])
name : features.1.norm.weight, param : torch.Size([384])
name : features.1.norm.bias, param : torch.Size([384])
name : features.1.norm_skip.weight, param : torch.Size([384])
name : features.1.norm_skip.bias, param : torch.Size([384])
name : features.2.0.norm1.weight, param : torch.Size([192])
name : features.2.0.norm1.bias, param : torch.Size([192])
name : features.2.0.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.0.attn.relative_position_index, param : torch.Size([2401])
name : features.2.0.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.0.attn.qkv.bias, param : torch.Size([576])
name : features.2.0.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.0.attn.proj.bias, param : torch.Size([192])
name : features.2.0.norm2.weight, param : torch.Size([192])
name : features.2.0.norm2.bias, param : torch.Size([192])
name : features.2.0.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.0.mlp.0.bias, param : torch.Size([768])
name : features.2.0.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.0.mlp.3.bias, param : torch.Size([192])
name : features.2.0.norm1_skip.weight, param : torch.Size([192])
name : features.2.0.norm1_skip.bias, param : torch.Size([192])
name : features.2.0.norm2_skip.weight, param : torch.Size([192])
name : features.2.0.norm2_skip.bias, param : torch.Size([192])
name : features.2.1.norm1.weight, param : torch.Size([192])
name : features.2.1.norm1.bias, param : torch.Size([192])
name : features.2.1.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.1.attn.relative_position_index, param : torch.Size([2401])
name : features.2.1.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.1.attn.qkv.bias, param : torch.Size([576])
name : features.2.1.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.1.attn.proj.bias, param : torch.Size([192])
name : features.2.1.norm2.weight, param : torch.Size([192])
name : features.2.1.norm2.bias, param : torch.Size([192])
name : features.2.1.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.1.mlp.0.bias, param : torch.Size([768])
name : features.2.1.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.1.mlp.3.bias, param : torch.Size([192])
name : features.3.reduction.weight, param : torch.Size([384, 768])
name : features.3.norm.weight, param : torch.Size([768])
name : features.3.norm.bias, param : torch.Size([768])
name : features.3.norm_skip.weight, param : torch.Size([768])
name : features.3.norm_skip.bias, param : torch.Size([768])
name : features.4.0.norm1.weight, param : torch.Size([384])
name : features.4.0.norm1.bias, param : torch.Size([384])
name : features.4.0.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.0.attn.relative_position_index, param : torch.Size([2401])
name : features.4.0.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.0.attn.qkv.bias, param : torch.Size([1152])
name : features.4.0.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.0.attn.proj.bias, param : torch.Size([384])
name : features.4.0.norm2.weight, param : torch.Size([384])
name : features.4.0.norm2.bias, param : torch.Size([384])
name : features.4.0.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.0.mlp.0.bias, param : torch.Size([1536])
name : features.4.0.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.0.mlp.3.bias, param : torch.Size([384])
name : features.4.0.norm1_skip.weight, param : torch.Size([384])
name : features.4.0.norm1_skip.bias, param : torch.Size([384])
name : features.4.0.norm2_skip.weight, param : torch.Size([384])
name : features.4.0.norm2_skip.bias, param : torch.Size([384])
name : features.4.1.norm1.weight, param : torch.Size([384])
name : features.4.1.norm1.bias, param : torch.Size([384])
name : features.4.1.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.1.attn.relative_position_index, param : torch.Size([2401])
name : features.4.1.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.1.attn.qkv.bias, param : torch.Size([1152])
name : features.4.1.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.1.attn.proj.bias, param : torch.Size([384])
name : features.4.1.norm2.weight, param : torch.Size([384])
name : features.4.1.norm2.bias, param : torch.Size([384])
name : features.4.1.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.1.mlp.0.bias, param : torch.Size([1536])
name : features.4.1.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.1.mlp.3.bias, param : torch.Size([384])
name : features.4.1.norm1_skip.weight, param : torch.Size([384])
name : features.4.1.norm1_skip.bias, param : torch.Size([384])
name : features.4.1.norm2_skip.weight, param : torch.Size([384])
name : features.4.1.norm2_skip.bias, param : torch.Size([384])
name : features.4.2.norm1.weight, param : torch.Size([384])
name : features.4.2.norm1.bias, param : torch.Size([384])
name : features.4.2.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.2.attn.relative_position_index, param : torch.Size([2401])
name : features.4.2.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.2.attn.qkv.bias, param : torch.Size([1152])
name : features.4.2.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.2.attn.proj.bias, param : torch.Size([384])
name : features.4.2.norm2.weight, param : torch.Size([384])
name : features.4.2.norm2.bias, param : torch.Size([384])
name : features.4.2.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.2.mlp.0.bias, param : torch.Size([1536])
name : features.4.2.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.2.mlp.3.bias, param : torch.Size([384])
name : features.4.2.norm1_skip.weight, param : torch.Size([384])
name : features.4.2.norm1_skip.bias, param : torch.Size([384])
name : features.4.2.norm2_skip.weight, param : torch.Size([384])
name : features.4.2.norm2_skip.bias, param : torch.Size([384])
name : features.4.3.norm1.weight, param : torch.Size([384])
name : features.4.3.norm1.bias, param : torch.Size([384])
name : features.4.3.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.3.attn.relative_position_index, param : torch.Size([2401])
name : features.4.3.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.3.attn.qkv.bias, param : torch.Size([1152])
name : features.4.3.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.3.attn.proj.bias, param : torch.Size([384])
name : features.4.3.norm2.weight, param : torch.Size([384])
name : features.4.3.norm2.bias, param : torch.Size([384])
name : features.4.3.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.3.mlp.0.bias, param : torch.Size([1536])
name : features.4.3.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.3.mlp.3.bias, param : torch.Size([384])
name : features.4.4.norm1.weight, param : torch.Size([384])
name : features.4.4.norm1.bias, param : torch.Size([384])
name : features.4.4.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.4.attn.relative_position_index, param : torch.Size([2401])
name : features.4.4.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.4.attn.qkv.bias, param : torch.Size([1152])
name : features.4.4.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.4.attn.proj.bias, param : torch.Size([384])
name : features.4.4.norm2.weight, param : torch.Size([384])
name : features.4.4.norm2.bias, param : torch.Size([384])
name : features.4.4.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.4.mlp.0.bias, param : torch.Size([1536])
name : features.4.4.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.4.mlp.3.bias, param : torch.Size([384])
name : features.4.5.norm1.weight, param : torch.Size([384])
name : features.4.5.norm1.bias, param : torch.Size([384])
name : features.4.5.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.5.attn.relative_position_index, param : torch.Size([2401])
name : features.4.5.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.5.attn.qkv.bias, param : torch.Size([1152])
name : features.4.5.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.5.attn.proj.bias, param : torch.Size([384])
name : features.4.5.norm2.weight, param : torch.Size([384])
name : features.4.5.norm2.bias, param : torch.Size([384])
name : features.4.5.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.5.mlp.0.bias, param : torch.Size([1536])
name : features.4.5.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.5.mlp.3.bias, param : torch.Size([384])
name : features.5.reduction.weight, param : torch.Size([768, 1536])
name : features.5.norm.weight, param : torch.Size([1536])
name : features.5.norm.bias, param : torch.Size([1536])
name : features.5.norm_skip.weight, param : torch.Size([1536])
name : features.5.norm_skip.bias, param : torch.Size([1536])
name : features.6.0.norm1.weight, param : torch.Size([768])
name : features.6.0.norm1.bias, param : torch.Size([768])
name : features.6.0.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.0.attn.relative_position_index, param : torch.Size([2401])
name : features.6.0.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.0.attn.qkv.bias, param : torch.Size([2304])
name : features.6.0.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.0.attn.proj.bias, param : torch.Size([768])
name : features.6.0.norm2.weight, param : torch.Size([768])
name : features.6.0.norm2.bias, param : torch.Size([768])
name : features.6.0.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.0.mlp.0.bias, param : torch.Size([3072])
name : features.6.0.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.0.mlp.3.bias, param : torch.Size([768])
name : features.6.0.norm1_skip.weight, param : torch.Size([768])
name : features.6.0.norm1_skip.bias, param : torch.Size([768])
name : features.6.0.norm2_skip.weight, param : torch.Size([768])
name : features.6.0.norm2_skip.bias, param : torch.Size([768])
name : features.6.1.norm1.weight, param : torch.Size([768])
name : features.6.1.norm1.bias, param : torch.Size([768])
name : features.6.1.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.1.attn.relative_position_index, param : torch.Size([2401])
name : features.6.1.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.1.attn.qkv.bias, param : torch.Size([2304])
name : features.6.1.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.1.attn.proj.bias, param : torch.Size([768])
name : features.6.1.norm2.weight, param : torch.Size([768])
name : features.6.1.norm2.bias, param : torch.Size([768])
name : features.6.1.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.1.mlp.0.bias, param : torch.Size([3072])
name : features.6.1.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.1.mlp.3.bias, param : torch.Size([768])
name : norm.weight, param : torch.Size([768])
name : norm.bias, param : torch.Size([768])
Load state_dict from /home/hslee/Desktop/RetinaNet-ADN/02_AdaptiveDepthNetwork/pretrained/checkpoint_swin-t-epoch297.pth
self.backbone : SwinTransformer
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): SwinTransformer(
      (pre): Sequential(
        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): Permute()
        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (features): SkippableSequentialStages(
        (0): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.009090909090909092, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PatchMerging(
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (2): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.02727272727272728, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.045454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.06363636363636364, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.08181818181818182, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (6): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.1, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj_swinT): ModuleList(
        (0): Sequential(
          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
start creating model... (in yaml_config.py)
start creating model... (in yaml_config.py)
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.25s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
module.backbone.features.0.0.norm1.weight 96
module.backbone.features.0.0.norm1.bias 96
module.backbone.features.0.0.attn.relative_position_bias_table 507
module.backbone.features.0.0.attn.qkv.weight 27648
module.backbone.features.0.0.attn.qkv.bias 288
module.backbone.features.0.0.attn.proj.weight 9216
module.backbone.features.0.0.attn.proj.bias 96
module.backbone.features.0.0.norm2.weight 96
module.backbone.features.0.0.norm2.bias 96
module.backbone.features.0.0.mlp.0.weight 36864
module.backbone.features.0.0.mlp.0.bias 384
module.backbone.features.0.0.mlp.3.weight 36864
module.backbone.features.0.0.mlp.3.bias 96
module.backbone.features.0.0.norm1_skip.weight 96
module.backbone.features.0.0.norm1_skip.bias 96
module.backbone.features.0.0.norm2_skip.weight 96
module.backbone.features.0.0.norm2_skip.bias 96
module.backbone.features.0.1.norm1.weight 96
module.backbone.features.0.1.norm1.bias 96
module.backbone.features.0.1.attn.relative_position_bias_table 507
module.backbone.features.0.1.attn.qkv.weight 27648
module.backbone.features.0.1.attn.qkv.bias 288
module.backbone.features.0.1.attn.proj.weight 9216
module.backbone.features.0.1.attn.proj.bias 96
module.backbone.features.0.1.norm2.weight 96
module.backbone.features.0.1.norm2.bias 96
module.backbone.features.0.1.mlp.0.weight 36864
module.backbone.features.0.1.mlp.0.bias 384
module.backbone.features.0.1.mlp.3.weight 36864
module.backbone.features.0.1.mlp.3.bias 96
module.backbone.features.1.reduction.weight 73728
module.backbone.features.1.norm.weight 384
module.backbone.features.1.norm.bias 384
module.backbone.features.1.norm_skip.weight 384
module.backbone.features.1.norm_skip.bias 384
module.backbone.features.2.0.norm1.weight 192
module.backbone.features.2.0.norm1.bias 192
module.backbone.features.2.0.attn.relative_position_bias_table 1014
module.backbone.features.2.0.attn.qkv.weight 110592
module.backbone.features.2.0.attn.qkv.bias 576
module.backbone.features.2.0.attn.proj.weight 36864
module.backbone.features.2.0.attn.proj.bias 192
module.backbone.features.2.0.norm2.weight 192
module.backbone.features.2.0.norm2.bias 192
module.backbone.features.2.0.mlp.0.weight 147456
module.backbone.features.2.0.mlp.0.bias 768
module.backbone.features.2.0.mlp.3.weight 147456
module.backbone.features.2.0.mlp.3.bias 192
module.backbone.features.2.0.norm1_skip.weight 192
module.backbone.features.2.0.norm1_skip.bias 192
module.backbone.features.2.0.norm2_skip.weight 192
module.backbone.features.2.0.norm2_skip.bias 192
module.backbone.features.2.1.norm1.weight 192
module.backbone.features.2.1.norm1.bias 192
module.backbone.features.2.1.attn.relative_position_bias_table 1014
module.backbone.features.2.1.attn.qkv.weight 110592
module.backbone.features.2.1.attn.qkv.bias 576
module.backbone.features.2.1.attn.proj.weight 36864
module.backbone.features.2.1.attn.proj.bias 192
module.backbone.features.2.1.norm2.weight 192
module.backbone.features.2.1.norm2.bias 192
module.backbone.features.2.1.mlp.0.weight 147456
module.backbone.features.2.1.mlp.0.bias 768
module.backbone.features.2.1.mlp.3.weight 147456
module.backbone.features.2.1.mlp.3.bias 192
module.backbone.features.3.reduction.weight 294912
module.backbone.features.3.norm.weight 768
module.backbone.features.3.norm.bias 768
module.backbone.features.3.norm_skip.weight 768
module.backbone.features.3.norm_skip.bias 768
module.backbone.features.4.0.norm1.weight 384
module.backbone.features.4.0.norm1.bias 384
module.backbone.features.4.0.attn.relative_position_bias_table 2028
module.backbone.features.4.0.attn.qkv.weight 442368
module.backbone.features.4.0.attn.qkv.bias 1152
module.backbone.features.4.0.attn.proj.weight 147456
module.backbone.features.4.0.attn.proj.bias 384
module.backbone.features.4.0.norm2.weight 384
module.backbone.features.4.0.norm2.bias 384
module.backbone.features.4.0.mlp.0.weight 589824
module.backbone.features.4.0.mlp.0.bias 1536
module.backbone.features.4.0.mlp.3.weight 589824
module.backbone.features.4.0.mlp.3.bias 384
module.backbone.features.4.0.norm1_skip.weight 384
module.backbone.features.4.0.norm1_skip.bias 384
module.backbone.features.4.0.norm2_skip.weight 384
module.backbone.features.4.0.norm2_skip.bias 384
module.backbone.features.4.1.norm1.weight 384
module.backbone.features.4.1.norm1.bias 384
module.backbone.features.4.1.attn.relative_position_bias_table 2028
module.backbone.features.4.1.attn.qkv.weight 442368
module.backbone.features.4.1.attn.qkv.bias 1152
module.backbone.features.4.1.attn.proj.weight 147456
module.backbone.features.4.1.attn.proj.bias 384
module.backbone.features.4.1.norm2.weight 384
module.backbone.features.4.1.norm2.bias 384
module.backbone.features.4.1.mlp.0.weight 589824
module.backbone.features.4.1.mlp.0.bias 1536
module.backbone.features.4.1.mlp.3.weight 589824
module.backbone.features.4.1.mlp.3.bias 384
module.backbone.features.4.1.norm1_skip.weight 384
module.backbone.features.4.1.norm1_skip.bias 384
module.backbone.features.4.1.norm2_skip.weight 384
module.backbone.features.4.1.norm2_skip.bias 384
module.backbone.features.4.2.norm1.weight 384
module.backbone.features.4.2.norm1.bias 384
module.backbone.features.4.2.attn.relative_position_bias_table 2028
module.backbone.features.4.2.attn.qkv.weight 442368
module.backbone.features.4.2.attn.qkv.bias 1152
module.backbone.features.4.2.attn.proj.weight 147456
module.backbone.features.4.2.attn.proj.bias 384
module.backbone.features.4.2.norm2.weight 384
module.backbone.features.4.2.norm2.bias 384
module.backbone.features.4.2.mlp.0.weight 589824
module.backbone.features.4.2.mlp.0.bias 1536
module.backbone.features.4.2.mlp.3.weight 589824
module.backbone.features.4.2.mlp.3.bias 384
module.backbone.features.4.2.norm1_skip.weight 384
module.backbone.features.4.2.norm1_skip.bias 384
module.backbone.features.4.2.norm2_skip.weight 384
module.backbone.features.4.2.norm2_skip.bias 384
module.backbone.features.4.3.norm1.weight 384
module.backbone.features.4.3.norm1.bias 384
module.backbone.features.4.3.attn.relative_position_bias_table 2028
module.backbone.features.4.3.attn.qkv.weight 442368
module.backbone.features.4.3.attn.qkv.bias 1152
module.backbone.features.4.3.attn.proj.weight 147456
module.backbone.features.4.3.attn.proj.bias 384
module.backbone.features.4.3.norm2.weight 384
module.backbone.features.4.3.norm2.bias 384
module.backbone.features.4.3.mlp.0.weight 589824
module.backbone.features.4.3.mlp.0.bias 1536
module.backbone.features.4.3.mlp.3.weight 589824
module.backbone.features.4.3.mlp.3.bias 384
module.backbone.features.4.4.norm1.weight 384
module.backbone.features.4.4.norm1.bias 384
module.backbone.features.4.4.attn.relative_position_bias_table 2028
module.backbone.features.4.4.attn.qkv.weight 442368
module.backbone.features.4.4.attn.qkv.bias 1152
module.backbone.features.4.4.attn.proj.weight 147456
module.backbone.features.4.4.attn.proj.bias 384
module.backbone.features.4.4.norm2.weight 384
module.backbone.features.4.4.norm2.bias 384
module.backbone.features.4.4.mlp.0.weight 589824
module.backbone.features.4.4.mlp.0.bias 1536
module.backbone.features.4.4.mlp.3.weight 589824
module.backbone.features.4.4.mlp.3.bias 384
module.backbone.features.4.5.norm1.weight 384
module.backbone.features.4.5.norm1.bias 384
module.backbone.features.4.5.attn.relative_position_bias_table 2028
module.backbone.features.4.5.attn.qkv.weight 442368
module.backbone.features.4.5.attn.qkv.bias 1152
module.backbone.features.4.5.attn.proj.weight 147456
module.backbone.features.4.5.attn.proj.bias 384
module.backbone.features.4.5.norm2.weight 384
module.backbone.features.4.5.norm2.bias 384
module.backbone.features.4.5.mlp.0.weight 589824
module.backbone.features.4.5.mlp.0.bias 1536
module.backbone.features.4.5.mlp.3.weight 589824
module.backbone.features.4.5.mlp.3.bias 384
module.backbone.features.5.reduction.weight 1179648
module.backbone.features.5.norm.weight 1536
module.backbone.features.5.norm.bias 1536
module.backbone.features.5.norm_skip.weight 1536
module.backbone.features.5.norm_skip.bias 1536
module.backbone.features.6.0.norm1.weight 768
module.backbone.features.6.0.norm1.bias 768
module.backbone.features.6.0.attn.relative_position_bias_table 4056
module.backbone.features.6.0.attn.qkv.weight 1769472
module.backbone.features.6.0.attn.qkv.bias 2304
module.backbone.features.6.0.attn.proj.weight 589824
module.backbone.features.6.0.attn.proj.bias 768
module.backbone.features.6.0.norm2.weight 768
module.backbone.features.6.0.norm2.bias 768
module.backbone.features.6.0.mlp.0.weight 2359296
module.backbone.features.6.0.mlp.0.bias 3072
module.backbone.features.6.0.mlp.3.weight 2359296
module.backbone.features.6.0.mlp.3.bias 768
module.backbone.features.6.0.norm1_skip.weight 768
module.backbone.features.6.0.norm1_skip.bias 768
module.backbone.features.6.0.norm2_skip.weight 768
module.backbone.features.6.0.norm2_skip.bias 768
module.backbone.features.6.1.norm1.weight 768
module.backbone.features.6.1.norm1.bias 768
module.backbone.features.6.1.attn.relative_position_bias_table 4056
module.backbone.features.6.1.attn.qkv.weight 1769472
module.backbone.features.6.1.attn.qkv.bias 2304
module.backbone.features.6.1.attn.proj.weight 589824
module.backbone.features.6.1.attn.proj.bias 768
module.backbone.features.6.1.norm2.weight 768
module.backbone.features.6.1.norm2.bias 768
module.backbone.features.6.1.mlp.0.weight 2359296
module.backbone.features.6.1.mlp.0.bias 3072
module.backbone.features.6.1.mlp.3.weight 2359296
module.backbone.features.6.1.mlp.3.bias 768
module.backbone.norm.weight 768
module.backbone.norm.bias 768
module.decoder.input_proj.0.conv.weight 65536
module.decoder.input_proj.0.norm.weight 256
module.decoder.input_proj.0.norm.bias 256
module.decoder.input_proj.1.conv.weight 65536
module.decoder.input_proj.1.norm.weight 256
module.decoder.input_proj.1.norm.bias 256
module.decoder.input_proj.2.conv.weight 65536
module.decoder.input_proj.2.norm.weight 256
module.decoder.input_proj.2.norm.bias 256
module.decoder.decoder.layers.0.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.0.self_attn.in_proj_bias 768
module.decoder.decoder.layers.0.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.0.self_attn.out_proj.bias 256
module.decoder.decoder.layers.0.norm1.weight 256
module.decoder.decoder.layers.0.norm1.bias 256
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.0.norm2.weight 256
module.decoder.decoder.layers.0.norm2.bias 256
module.decoder.decoder.layers.0.linear1.weight 262144
module.decoder.decoder.layers.0.linear1.bias 1024
module.decoder.decoder.layers.0.linear2.weight 262144
module.decoder.decoder.layers.0.linear2.bias 256
module.decoder.decoder.layers.0.norm3.weight 256
module.decoder.decoder.layers.0.norm3.bias 256
module.decoder.decoder.layers.1.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.1.self_attn.in_proj_bias 768
module.decoder.decoder.layers.1.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.1.self_attn.out_proj.bias 256
module.decoder.decoder.layers.1.norm1.weight 256
module.decoder.decoder.layers.1.norm1.bias 256
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.1.norm2.weight 256
module.decoder.decoder.layers.1.norm2.bias 256
module.decoder.decoder.layers.1.linear1.weight 262144
module.decoder.decoder.layers.1.linear1.bias 1024
module.decoder.decoder.layers.1.linear2.weight 262144
module.decoder.decoder.layers.1.linear2.bias 256
module.decoder.decoder.layers.1.norm3.weight 256
module.decoder.decoder.layers.1.norm3.bias 256
module.decoder.decoder.layers.2.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.2.self_attn.in_proj_bias 768
module.decoder.decoder.layers.2.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.2.self_attn.out_proj.bias 256
module.decoder.decoder.layers.2.norm1.weight 256
module.decoder.decoder.layers.2.norm1.bias 256
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.2.norm2.weight 256
module.decoder.decoder.layers.2.norm2.bias 256
module.decoder.decoder.layers.2.linear1.weight 262144
module.decoder.decoder.layers.2.linear1.bias 1024
module.decoder.decoder.layers.2.linear2.weight 262144
module.decoder.decoder.layers.2.linear2.bias 256
module.decoder.decoder.layers.2.norm3.weight 256
module.decoder.decoder.layers.2.norm3.bias 256
module.decoder.decoder.layers.3.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.3.self_attn.in_proj_bias 768
module.decoder.decoder.layers.3.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.3.self_attn.out_proj.bias 256
module.decoder.decoder.layers.3.norm1.weight 256
module.decoder.decoder.layers.3.norm1.bias 256
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.3.norm2.weight 256
module.decoder.decoder.layers.3.norm2.bias 256
module.decoder.decoder.layers.3.linear1.weight 262144
module.decoder.decoder.layers.3.linear1.bias 1024
module.decoder.decoder.layers.3.linear2.weight 262144
module.decoder.decoder.layers.3.linear2.bias 256
module.decoder.decoder.layers.3.norm3.weight 256
module.decoder.decoder.layers.3.norm3.bias 256
module.decoder.decoder.layers.4.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.4.self_attn.in_proj_bias 768
module.decoder.decoder.layers.4.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.4.self_attn.out_proj.bias 256
module.decoder.decoder.layers.4.norm1.weight 256
module.decoder.decoder.layers.4.norm1.bias 256
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.4.norm2.weight 256
module.decoder.decoder.layers.4.norm2.bias 256
module.decoder.decoder.layers.4.linear1.weight 262144
module.decoder.decoder.layers.4.linear1.bias 1024
module.decoder.decoder.layers.4.linear2.weight 262144
module.decoder.decoder.layers.4.linear2.bias 256
module.decoder.decoder.layers.4.norm3.weight 256
module.decoder.decoder.layers.4.norm3.bias 256
module.decoder.decoder.layers.5.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.5.self_attn.in_proj_bias 768
module.decoder.decoder.layers.5.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.5.self_attn.out_proj.bias 256
module.decoder.decoder.layers.5.norm1.weight 256
module.decoder.decoder.layers.5.norm1.bias 256
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.5.norm2.weight 256
module.decoder.decoder.layers.5.norm2.bias 256
module.decoder.decoder.layers.5.linear1.weight 262144
module.decoder.decoder.layers.5.linear1.bias 1024
module.decoder.decoder.layers.5.linear2.weight 262144
module.decoder.decoder.layers.5.linear2.bias 256
module.decoder.decoder.layers.5.norm3.weight 256
module.decoder.decoder.layers.5.norm3.bias 256
module.decoder.denoising_class_embed.weight 20736
module.decoder.query_pos_head.layers.0.weight 2048
module.decoder.query_pos_head.layers.0.bias 512
module.decoder.query_pos_head.layers.1.weight 131072
module.decoder.query_pos_head.layers.1.bias 256
module.decoder.enc_output.0.weight 65536
module.decoder.enc_output.0.bias 256
module.decoder.enc_output.1.weight 256
module.decoder.enc_output.1.bias 256
module.decoder.enc_score_head.weight 20480
module.decoder.enc_score_head.bias 80
module.decoder.enc_bbox_head.layers.0.weight 65536
module.decoder.enc_bbox_head.layers.0.bias 256
module.decoder.enc_bbox_head.layers.1.weight 65536
module.decoder.enc_bbox_head.layers.1.bias 256
module.decoder.enc_bbox_head.layers.2.weight 1024
module.decoder.enc_bbox_head.layers.2.bias 4
module.decoder.dec_score_head.0.weight 20480
module.decoder.dec_score_head.0.bias 80
module.decoder.dec_score_head.1.weight 20480
module.decoder.dec_score_head.1.bias 80
module.decoder.dec_score_head.2.weight 20480
module.decoder.dec_score_head.2.bias 80
module.decoder.dec_score_head.3.weight 20480
module.decoder.dec_score_head.3.bias 80
module.decoder.dec_score_head.4.weight 20480
module.decoder.dec_score_head.4.bias 80
module.decoder.dec_score_head.5.weight 20480
module.decoder.dec_score_head.5.bias 80
module.decoder.dec_bbox_head.0.layers.0.weight 65536
module.decoder.dec_bbox_head.0.layers.0.bias 256
module.decoder.dec_bbox_head.0.layers.1.weight 65536
module.decoder.dec_bbox_head.0.layers.1.bias 256
module.decoder.dec_bbox_head.0.layers.2.weight 1024
module.decoder.dec_bbox_head.0.layers.2.bias 4
module.decoder.dec_bbox_head.1.layers.0.weight 65536
module.decoder.dec_bbox_head.1.layers.0.bias 256
module.decoder.dec_bbox_head.1.layers.1.weight 65536
module.decoder.dec_bbox_head.1.layers.1.bias 256
module.decoder.dec_bbox_head.1.layers.2.weight 1024
module.decoder.dec_bbox_head.1.layers.2.bias 4
module.decoder.dec_bbox_head.2.layers.0.weight 65536
module.decoder.dec_bbox_head.2.layers.0.bias 256
module.decoder.dec_bbox_head.2.layers.1.weight 65536
module.decoder.dec_bbox_head.2.layers.1.bias 256
module.decoder.dec_bbox_head.2.layers.2.weight 1024
module.decoder.dec_bbox_head.2.layers.2.bias 4
module.decoder.dec_bbox_head.3.layers.0.weight 65536
module.decoder.dec_bbox_head.3.layers.0.bias 256
module.decoder.dec_bbox_head.3.layers.1.weight 65536
module.decoder.dec_bbox_head.3.layers.1.bias 256
module.decoder.dec_bbox_head.3.layers.2.weight 1024
module.decoder.dec_bbox_head.3.layers.2.bias 4
module.decoder.dec_bbox_head.4.layers.0.weight 65536
module.decoder.dec_bbox_head.4.layers.0.bias 256
module.decoder.dec_bbox_head.4.layers.1.weight 65536
module.decoder.dec_bbox_head.4.layers.1.bias 256
module.decoder.dec_bbox_head.4.layers.2.weight 1024
module.decoder.dec_bbox_head.4.layers.2.bias 4
module.decoder.dec_bbox_head.5.layers.0.weight 65536
module.decoder.dec_bbox_head.5.layers.0.bias 256
module.decoder.dec_bbox_head.5.layers.1.weight 65536
module.decoder.dec_bbox_head.5.layers.1.bias 256
module.decoder.dec_bbox_head.5.layers.2.weight 1024
module.decoder.dec_bbox_head.5.layers.2.bias 4
module.encoder.input_proj_swinT.0.0.weight 49152
module.encoder.input_proj_swinT.0.1.weight 256
module.encoder.input_proj_swinT.0.1.bias 256
module.encoder.input_proj_swinT.1.0.weight 98304
module.encoder.input_proj_swinT.1.1.weight 256
module.encoder.input_proj_swinT.1.1.bias 256
module.encoder.input_proj_swinT.2.0.weight 196608
module.encoder.input_proj_swinT.2.1.weight 256
module.encoder.input_proj_swinT.2.1.bias 256
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
module.encoder.encoder.0.layers.0.linear1.weight 262144
module.encoder.encoder.0.layers.0.linear1.bias 1024
module.encoder.encoder.0.layers.0.linear2.weight 262144
module.encoder.encoder.0.layers.0.linear2.bias 256
module.encoder.encoder.0.layers.0.norm1.weight 256
module.encoder.encoder.0.layers.0.norm1.bias 256
module.encoder.encoder.0.layers.0.norm2.weight 256
module.encoder.encoder.0.layers.0.norm2.bias 256
module.encoder.lateral_convs.0.conv.weight 65536
module.encoder.lateral_convs.0.norm.weight 256
module.encoder.lateral_convs.0.norm.bias 256
module.encoder.lateral_convs.1.conv.weight 65536
module.encoder.lateral_convs.1.norm.weight 256
module.encoder.lateral_convs.1.norm.bias 256
module.encoder.fpn_blocks.0.conv1.conv.weight 131072
module.encoder.fpn_blocks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.conv2.conv.weight 131072
module.encoder.fpn_blocks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.fpn_blocks.1.conv1.conv.weight 131072
module.encoder.fpn_blocks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.conv2.conv.weight 131072
module.encoder.fpn_blocks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
module.encoder.downsample_convs.0.conv.weight 589824
module.encoder.downsample_convs.0.norm.weight 256
module.encoder.downsample_convs.0.norm.bias 256
module.encoder.downsample_convs.1.conv.weight 589824
module.encoder.downsample_convs.1.norm.weight 256
module.encoder.downsample_convs.1.norm.bias 256
module.encoder.pan_blocks.0.conv1.conv.weight 131072
module.encoder.pan_blocks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.conv2.conv.weight 131072
module.encoder.pan_blocks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.pan_blocks.1.conv1.conv.weight 131072
module.encoder.pan_blocks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.conv2.conv.weight 131072
module.encoder.pan_blocks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 46372582
super_config : [False, False, False, False]
base_config : [True, True, True, True]
Epoch: [0]  [    0/14786]  eta: 8:13:11  lr: 0.000010  loss: 46.6510 (46.6510)  loss_bbox: 1.8098 (1.8098)  loss_bbox_aux_0: 1.7764 (1.7764)  loss_bbox_aux_1: 1.8481 (1.8481)  loss_bbox_aux_2: 1.8161 (1.8161)  loss_bbox_aux_3: 1.7910 (1.7910)  loss_bbox_aux_4: 1.7899 (1.7899)  loss_bbox_aux_5: 1.8498 (1.8498)  loss_bbox_dn_0: 1.0893 (1.0893)  loss_bbox_dn_1: 1.0893 (1.0893)  loss_bbox_dn_2: 1.0893 (1.0893)  loss_bbox_dn_3: 1.0893 (1.0893)  loss_bbox_dn_4: 1.0893 (1.0893)  loss_bbox_dn_5: 1.0893 (1.0893)  loss_giou: 1.6590 (1.6590)  loss_giou_aux_0: 1.7045 (1.7045)  loss_giou_aux_1: 1.6659 (1.6659)  loss_giou_aux_2: 1.6688 (1.6688)  loss_giou_aux_3: 1.6704 (1.6704)  loss_giou_aux_4: 1.6571 (1.6571)  loss_giou_aux_5: 1.7026 (1.7026)  loss_giou_dn_0: 1.3093 (1.3093)  loss_giou_dn_1: 1.3093 (1.3093)  loss_giou_dn_2: 1.3093 (1.3093)  loss_giou_dn_3: 1.3093 (1.3093)  loss_giou_dn_4: 1.3093 (1.3093)  loss_giou_dn_5: 1.3093 (1.3093)  loss_vfl: 0.3377 (0.3377)  loss_vfl_aux_0: 0.3433 (0.3433)  loss_vfl_aux_1: 0.3388 (0.3388)  loss_vfl_aux_2: 0.3704 (0.3704)  loss_vfl_aux_3: 0.3828 (0.3828)  loss_vfl_aux_4: 0.3656 (0.3656)  loss_vfl_aux_5: 0.3623 (0.3623)  loss_vfl_dn_0: 0.8901 (0.8901)  loss_vfl_dn_1: 0.9135 (0.9135)  loss_vfl_dn_2: 0.8517 (0.8517)  loss_vfl_dn_3: 0.9084 (0.9084)  loss_vfl_dn_4: 0.8829 (0.8829)  loss_vfl_dn_5: 0.9020 (0.9020)  time: 2.0013  data: 0.5260  max mem: 4353
Epoch: [0]  [  100/14786]  eta: 1:11:19  lr: 0.000010  loss: 36.5368 (40.3234)  loss_bbox: 0.8115 (1.1723)  loss_bbox_aux_0: 0.8353 (1.2335)  loss_bbox_aux_1: 0.8218 (1.2098)  loss_bbox_aux_2: 0.8177 (1.1921)  loss_bbox_aux_3: 0.8036 (1.1829)  loss_bbox_aux_4: 0.8170 (1.1760)  loss_bbox_aux_5: 0.8933 (1.2812)  loss_bbox_dn_0: 0.7754 (0.9076)  loss_bbox_dn_1: 0.7752 (0.9170)  loss_bbox_dn_2: 0.7759 (0.9275)  loss_bbox_dn_3: 0.7759 (0.9365)  loss_bbox_dn_4: 0.7750 (0.9436)  loss_bbox_dn_5: 0.7735 (0.9486)  loss_giou: 1.3574 (1.4585)  loss_giou_aux_0: 1.3942 (1.4986)  loss_giou_aux_1: 1.3823 (1.4790)  loss_giou_aux_2: 1.3707 (1.4709)  loss_giou_aux_3: 1.3630 (1.4658)  loss_giou_aux_4: 1.3602 (1.4612)  loss_giou_aux_5: 1.4027 (1.5247)  loss_giou_dn_0: 1.3505 (1.3364)  loss_giou_dn_1: 1.3479 (1.3373)  loss_giou_dn_2: 1.3452 (1.3411)  loss_giou_dn_3: 1.3462 (1.3468)  loss_giou_dn_4: 1.3439 (1.3548)  loss_giou_dn_5: 1.3448 (1.3626)  loss_vfl: 0.6714 (0.6438)  loss_vfl_aux_0: 0.6335 (0.5683)  loss_vfl_aux_1: 0.6469 (0.5928)  loss_vfl_aux_2: 0.6691 (0.6134)  loss_vfl_aux_3: 0.6708 (0.6280)  loss_vfl_aux_4: 0.6754 (0.6343)  loss_vfl_aux_5: 0.5808 (0.5331)  loss_vfl_dn_0: 0.5067 (0.6180)  loss_vfl_dn_1: 0.5135 (0.6070)  loss_vfl_dn_2: 0.5218 (0.6028)  loss_vfl_dn_3: 0.5260 (0.6112)  loss_vfl_dn_4: 0.5344 (0.6079)  loss_vfl_dn_5: 0.5220 (0.5963)  time: 0.2687  data: 0.0093  max mem: 6592
Epoch: [0]  [  200/14786]  eta: 1:07:11  lr: 0.000010  loss: 35.7779 (38.7184)  loss_bbox: 0.7122 (0.9845)  loss_bbox_aux_0: 0.7914 (1.0470)  loss_bbox_aux_1: 0.7543 (1.0253)  loss_bbox_aux_2: 0.7419 (1.0073)  loss_bbox_aux_3: 0.7316 (0.9964)  loss_bbox_aux_4: 0.7172 (0.9885)  loss_bbox_aux_5: 0.8032 (1.0920)  loss_bbox_dn_0: 0.7113 (0.8640)  loss_bbox_dn_1: 0.6903 (0.8626)  loss_bbox_dn_2: 0.6874 (0.8648)  loss_bbox_dn_3: 0.6891 (0.8675)  loss_bbox_dn_4: 0.6900 (0.8698)  loss_bbox_dn_5: 0.6885 (0.8713)  loss_giou: 1.2613 (1.4029)  loss_giou_aux_0: 1.3264 (1.4517)  loss_giou_aux_1: 1.2677 (1.4292)  loss_giou_aux_2: 1.2516 (1.4188)  loss_giou_aux_3: 1.2534 (1.4107)  loss_giou_aux_4: 1.2452 (1.4063)  loss_giou_aux_5: 1.3651 (1.4795)  loss_giou_dn_0: 1.3246 (1.3340)  loss_giou_dn_1: 1.3046 (1.3305)  loss_giou_dn_2: 1.2962 (1.3346)  loss_giou_dn_3: 1.3125 (1.3417)  loss_giou_dn_4: 1.3199 (1.3506)  loss_giou_dn_5: 1.3247 (1.3598)  loss_vfl: 0.9943 (0.7536)  loss_vfl_aux_0: 0.7537 (0.6550)  loss_vfl_aux_1: 0.8761 (0.6836)  loss_vfl_aux_2: 0.9098 (0.7085)  loss_vfl_aux_3: 0.9388 (0.7296)  loss_vfl_aux_4: 0.9318 (0.7438)  loss_vfl_aux_5: 0.6647 (0.6138)  loss_vfl_dn_0: 0.4912 (0.5685)  loss_vfl_dn_1: 0.5031 (0.5677)  loss_vfl_dn_2: 0.5232 (0.5741)  loss_vfl_dn_3: 0.5386 (0.5798)  loss_vfl_dn_4: 0.5454 (0.5794)  loss_vfl_dn_5: 0.5551 (0.5696)  time: 0.2643  data: 0.0100  max mem: 6599
Epoch: [0]  [  300/14786]  eta: 1:05:36  lr: 0.000010  loss: 36.1365 (37.9859)  loss_bbox: 0.6421 (0.8837)  loss_bbox_aux_0: 0.7373 (0.9514)  loss_bbox_aux_1: 0.6898 (0.9225)  loss_bbox_aux_2: 0.6663 (0.9034)  loss_bbox_aux_3: 0.6590 (0.8941)  loss_bbox_aux_4: 0.6500 (0.8868)  loss_bbox_aux_5: 0.8210 (1.0153)  loss_bbox_dn_0: 0.8534 (0.8708)  loss_bbox_dn_1: 0.8281 (0.8618)  loss_bbox_dn_2: 0.8146 (0.8609)  loss_bbox_dn_3: 0.8106 (0.8622)  loss_bbox_dn_4: 0.8109 (0.8633)  loss_bbox_dn_5: 0.8118 (0.8642)  loss_giou: 1.1848 (1.3319)  loss_giou_aux_0: 1.2336 (1.3851)  loss_giou_aux_1: 1.2340 (1.3587)  loss_giou_aux_2: 1.1883 (1.3477)  loss_giou_aux_3: 1.1904 (1.3392)  loss_giou_aux_4: 1.2150 (1.3353)  loss_giou_aux_5: 1.3260 (1.4342)  loss_giou_dn_0: 1.2645 (1.3195)  loss_giou_dn_1: 1.2282 (1.3109)  loss_giou_dn_2: 1.2300 (1.3129)  loss_giou_dn_3: 1.2278 (1.3187)  loss_giou_dn_4: 1.2334 (1.3255)  loss_giou_dn_5: 1.2352 (1.3324)  loss_vfl: 1.0151 (0.8561)  loss_vfl_aux_0: 0.8015 (0.7378)  loss_vfl_aux_1: 0.8734 (0.7754)  loss_vfl_aux_2: 0.9352 (0.8070)  loss_vfl_aux_3: 0.9584 (0.8279)  loss_vfl_aux_4: 1.0161 (0.8458)  loss_vfl_aux_5: 0.7171 (0.6702)  loss_vfl_dn_0: 0.4923 (0.5418)  loss_vfl_dn_1: 0.5377 (0.5533)  loss_vfl_dn_2: 0.5649 (0.5649)  loss_vfl_dn_3: 0.5651 (0.5715)  loss_vfl_dn_4: 0.5788 (0.5739)  loss_vfl_dn_5: 0.5864 (0.5679)  time: 0.2629  data: 0.0091  max mem: 6599
Epoch: [0]  [  400/14786]  eta: 1:04:30  lr: 0.000010  loss: 34.6727 (37.3344)  loss_bbox: 0.5597 (0.8123)  loss_bbox_aux_0: 0.6264 (0.8814)  loss_bbox_aux_1: 0.5760 (0.8500)  loss_bbox_aux_2: 0.5679 (0.8311)  loss_bbox_aux_3: 0.5662 (0.8220)  loss_bbox_aux_4: 0.5617 (0.8156)  loss_bbox_aux_5: 0.7717 (0.9574)  loss_bbox_dn_0: 0.7349 (0.8600)  loss_bbox_dn_1: 0.7156 (0.8454)  loss_bbox_dn_2: 0.7111 (0.8421)  loss_bbox_dn_3: 0.7134 (0.8421)  loss_bbox_dn_4: 0.7165 (0.8424)  loss_bbox_dn_5: 0.7186 (0.8430)  loss_giou: 1.0798 (1.2786)  loss_giou_aux_0: 1.0990 (1.3359)  loss_giou_aux_1: 1.0947 (1.3072)  loss_giou_aux_2: 1.0932 (1.2942)  loss_giou_aux_3: 1.0917 (1.2859)  loss_giou_aux_4: 1.0921 (1.2818)  loss_giou_aux_5: 1.3062 (1.3969)  loss_giou_dn_0: 1.2342 (1.3044)  loss_giou_dn_1: 1.2244 (1.2920)  loss_giou_dn_2: 1.2268 (1.2920)  loss_giou_dn_3: 1.2283 (1.2967)  loss_giou_dn_4: 1.2269 (1.3024)  loss_giou_dn_5: 1.2300 (1.3084)  loss_vfl: 1.1329 (0.9295)  loss_vfl_aux_0: 1.0035 (0.7941)  loss_vfl_aux_1: 1.0539 (0.8365)  loss_vfl_aux_2: 1.0951 (0.8732)  loss_vfl_aux_3: 1.1245 (0.8961)  loss_vfl_aux_4: 1.1216 (0.9145)  loss_vfl_aux_5: 0.7476 (0.7106)  loss_vfl_dn_0: 0.4870 (0.5286)  loss_vfl_dn_1: 0.5162 (0.5478)  loss_vfl_dn_2: 0.5404 (0.5642)  loss_vfl_dn_3: 0.5559 (0.5716)  loss_vfl_dn_4: 0.5644 (0.5749)  loss_vfl_dn_5: 0.5631 (0.5715)  time: 0.2596  data: 0.0096  max mem: 6599
Epoch: [0]  [  500/14786]  eta: 1:05:05  lr: 0.000010  loss: 34.5645 (36.7326)  loss_bbox: 0.5341 (0.7574)  loss_bbox_aux_0: 0.5923 (0.8268)  loss_bbox_aux_1: 0.5623 (0.7934)  loss_bbox_aux_2: 0.5488 (0.7752)  loss_bbox_aux_3: 0.5423 (0.7665)  loss_bbox_aux_4: 0.5419 (0.7610)  loss_bbox_aux_5: 0.7071 (0.9072)  loss_bbox_dn_0: 0.8276 (0.8417)  loss_bbox_dn_1: 0.7658 (0.8226)  loss_bbox_dn_2: 0.7499 (0.8175)  loss_bbox_dn_3: 0.7458 (0.8168)  loss_bbox_dn_4: 0.7462 (0.8167)  loss_bbox_dn_5: 0.7483 (0.8172)  loss_giou: 0.9372 (1.2414)  loss_giou_aux_0: 1.0610 (1.3011)  loss_giou_aux_1: 0.9648 (1.2699)  loss_giou_aux_2: 0.9592 (1.2565)  loss_giou_aux_3: 0.9538 (1.2485)  loss_giou_aux_4: 0.9435 (1.2447)  loss_giou_aux_5: 1.1505 (1.3686)  loss_giou_dn_0: 1.1919 (1.2912)  loss_giou_dn_1: 1.1279 (1.2742)  loss_giou_dn_2: 1.1073 (1.2721)  loss_giou_dn_3: 1.1073 (1.2755)  loss_giou_dn_4: 1.1092 (1.2802)  loss_giou_dn_5: 1.1115 (1.2853)  loss_vfl: 1.2445 (0.9771)  loss_vfl_aux_0: 1.0893 (0.8319)  loss_vfl_aux_1: 1.1157 (0.8801)  loss_vfl_aux_2: 1.2071 (0.9192)  loss_vfl_aux_3: 1.1760 (0.9410)  loss_vfl_aux_4: 1.1964 (0.9602)  loss_vfl_aux_5: 0.9691 (0.7415)  loss_vfl_dn_0: 0.4808 (0.5190)  loss_vfl_dn_1: 0.5561 (0.5449)  loss_vfl_dn_2: 0.5889 (0.5642)  loss_vfl_dn_3: 0.6126 (0.5729)  loss_vfl_dn_4: 0.6298 (0.5769)  loss_vfl_dn_5: 0.6162 (0.5745)  time: 0.2638  data: 0.0099  max mem: 6599
Epoch: [0]  [  600/14786]  eta: 1:04:09  lr: 0.000010  loss: 32.7632 (36.3272)  loss_bbox: 0.4425 (0.7236)  loss_bbox_aux_0: 0.5400 (0.7934)  loss_bbox_aux_1: 0.5065 (0.7589)  loss_bbox_aux_2: 0.4838 (0.7407)  loss_bbox_aux_3: 0.4567 (0.7322)  loss_bbox_aux_4: 0.4425 (0.7269)  loss_bbox_aux_5: 0.6711 (0.8773)  loss_bbox_dn_0: 0.7237 (0.8289)  loss_bbox_dn_1: 0.6785 (0.8065)  loss_bbox_dn_2: 0.6629 (0.8002)  loss_bbox_dn_3: 0.6562 (0.7992)  loss_bbox_dn_4: 0.6524 (0.7991)  loss_bbox_dn_5: 0.6525 (0.7996)  loss_giou: 1.0224 (1.2134)  loss_giou_aux_0: 1.0646 (1.2766)  loss_giou_aux_1: 1.0218 (1.2423)  loss_giou_aux_2: 1.0293 (1.2286)  loss_giou_aux_3: 1.0120 (1.2208)  loss_giou_aux_4: 1.0371 (1.2169)  loss_giou_aux_5: 1.2159 (1.3497)  loss_giou_dn_0: 1.2050 (1.2780)  loss_giou_dn_1: 1.1508 (1.2571)  loss_giou_dn_2: 1.1310 (1.2527)  loss_giou_dn_3: 1.1232 (1.2548)  loss_giou_dn_4: 1.1202 (1.2586)  loss_giou_dn_5: 1.1210 (1.2630)  loss_vfl: 1.0667 (1.0132)  loss_vfl_aux_0: 0.9969 (0.8579)  loss_vfl_aux_1: 0.9955 (0.9114)  loss_vfl_aux_2: 1.0495 (0.9525)  loss_vfl_aux_3: 1.0366 (0.9736)  loss_vfl_aux_4: 1.0387 (0.9938)  loss_vfl_aux_5: 0.8518 (0.7629)  loss_vfl_dn_0: 0.4545 (0.5139)  loss_vfl_dn_1: 0.5122 (0.5440)  loss_vfl_dn_2: 0.5484 (0.5664)  loss_vfl_dn_3: 0.5647 (0.5764)  loss_vfl_dn_4: 0.5804 (0.5815)  loss_vfl_dn_5: 0.5777 (0.5809)  time: 0.2624  data: 0.0085  max mem: 6602
Epoch: [0]  [  700/14786]  eta: 1:03:18  lr: 0.000010  loss: 32.3954 (35.8660)  loss_bbox: 0.4629 (0.6889)  loss_bbox_aux_0: 0.5493 (0.7581)  loss_bbox_aux_1: 0.5025 (0.7229)  loss_bbox_aux_2: 0.4923 (0.7054)  loss_bbox_aux_3: 0.4776 (0.6974)  loss_bbox_aux_4: 0.4688 (0.6922)  loss_bbox_aux_5: 0.6021 (0.8438)  loss_bbox_dn_0: 0.6812 (0.8166)  loss_bbox_dn_1: 0.6568 (0.7924)  loss_bbox_dn_2: 0.6365 (0.7858)  loss_bbox_dn_3: 0.6327 (0.7846)  loss_bbox_dn_4: 0.6309 (0.7846)  loss_bbox_dn_5: 0.6305 (0.7850)  loss_giou: 1.0269 (1.1850)  loss_giou_aux_0: 1.0957 (1.2504)  loss_giou_aux_1: 1.0601 (1.2142)  loss_giou_aux_2: 1.0460 (1.2002)  loss_giou_aux_3: 1.0361 (1.1924)  loss_giou_aux_4: 1.0288 (1.1886)  loss_giou_aux_5: 1.2074 (1.3280)  loss_giou_dn_0: 1.1824 (1.2673)  loss_giou_dn_1: 1.1412 (1.2430)  loss_giou_dn_2: 1.1255 (1.2365)  loss_giou_dn_3: 1.1234 (1.2374)  loss_giou_dn_4: 1.1237 (1.2404)  loss_giou_dn_5: 1.1246 (1.2442)  loss_vfl: 1.0575 (1.0390)  loss_vfl_aux_0: 0.9406 (0.8793)  loss_vfl_aux_1: 0.9740 (0.9354)  loss_vfl_aux_2: 0.9799 (0.9756)  loss_vfl_aux_3: 0.9943 (0.9957)  loss_vfl_aux_4: 1.0320 (1.0166)  loss_vfl_aux_5: 0.7907 (0.7844)  loss_vfl_dn_0: 0.4620 (0.5084)  loss_vfl_dn_1: 0.5087 (0.5407)  loss_vfl_dn_2: 0.5384 (0.5650)  loss_vfl_dn_3: 0.5656 (0.5760)  loss_vfl_dn_4: 0.5681 (0.5818)  loss_vfl_dn_5: 0.5703 (0.5824)  time: 0.2562  data: 0.0088  max mem: 6602
Epoch: [0]  [  800/14786]  eta: 1:02:40  lr: 0.000010  loss: 33.2437 (35.5081)  loss_bbox: 0.4840 (0.6598)  loss_bbox_aux_0: 0.5480 (0.7298)  loss_bbox_aux_1: 0.5469 (0.6935)  loss_bbox_aux_2: 0.4953 (0.6759)  loss_bbox_aux_3: 0.4914 (0.6681)  loss_bbox_aux_4: 0.4909 (0.6633)  loss_bbox_aux_5: 0.6306 (0.8182)  loss_bbox_dn_0: 0.7279 (0.8092)  loss_bbox_dn_1: 0.6942 (0.7831)  loss_bbox_dn_2: 0.6659 (0.7758)  loss_bbox_dn_3: 0.6577 (0.7745)  loss_bbox_dn_4: 0.6568 (0.7744)  loss_bbox_dn_5: 0.6569 (0.7748)  loss_giou: 0.9562 (1.1551)  loss_giou_aux_0: 1.0300 (1.2238)  loss_giou_aux_1: 0.9766 (1.1849)  loss_giou_aux_2: 0.9568 (1.1703)  loss_giou_aux_3: 0.9513 (1.1626)  loss_giou_aux_4: 0.9583 (1.1588)  loss_giou_aux_5: 1.2281 (1.3066)  loss_giou_dn_0: 1.1792 (1.2559)  loss_giou_dn_1: 1.1287 (1.2283)  loss_giou_dn_2: 1.1084 (1.2197)  loss_giou_dn_3: 1.1073 (1.2197)  loss_giou_dn_4: 1.1064 (1.2222)  loss_giou_dn_5: 1.1055 (1.2256)  loss_vfl: 1.1517 (1.0677)  loss_vfl_aux_0: 0.9753 (0.9055)  loss_vfl_aux_1: 1.0201 (0.9645)  loss_vfl_aux_2: 1.0349 (1.0040)  loss_vfl_aux_3: 1.0589 (1.0241)  loss_vfl_aux_4: 1.0799 (1.0443)  loss_vfl_aux_5: 0.9041 (0.8084)  loss_vfl_dn_0: 0.4746 (0.5054)  loss_vfl_dn_1: 0.5214 (0.5395)  loss_vfl_dn_2: 0.5563 (0.5651)  loss_vfl_dn_3: 0.5877 (0.5774)  loss_vfl_dn_4: 0.5825 (0.5833)  loss_vfl_dn_5: 0.5865 (0.5847)  time: 0.2639  data: 0.0091  max mem: 6602
Epoch: [0]  [  900/14786]  eta: 1:01:52  lr: 0.000010  loss: 32.6947 (35.2024)  loss_bbox: 0.4587 (0.6377)  loss_bbox_aux_0: 0.4964 (0.7068)  loss_bbox_aux_1: 0.4927 (0.6706)  loss_bbox_aux_2: 0.4740 (0.6534)  loss_bbox_aux_3: 0.4580 (0.6456)  loss_bbox_aux_4: 0.4720 (0.6414)  loss_bbox_aux_5: 0.6341 (0.7988)  loss_bbox_dn_0: 0.7048 (0.8019)  loss_bbox_dn_1: 0.6698 (0.7742)  loss_bbox_dn_2: 0.6516 (0.7663)  loss_bbox_dn_3: 0.6435 (0.7648)  loss_bbox_dn_4: 0.6437 (0.7647)  loss_bbox_dn_5: 0.6428 (0.7650)  loss_giou: 1.0202 (1.1319)  loss_giou_aux_0: 1.0820 (1.2028)  loss_giou_aux_1: 1.0497 (1.1624)  loss_giou_aux_2: 1.0413 (1.1469)  loss_giou_aux_3: 1.0128 (1.1394)  loss_giou_aux_4: 1.0153 (1.1354)  loss_giou_aux_5: 1.1684 (1.2910)  loss_giou_dn_0: 1.1834 (1.2459)  loss_giou_dn_1: 1.1262 (1.2153)  loss_giou_dn_2: 1.1132 (1.2047)  loss_giou_dn_3: 1.1023 (1.2039)  loss_giou_dn_4: 1.0981 (1.2058)  loss_giou_dn_5: 1.0976 (1.2088)  loss_vfl: 1.0621 (1.0869)  loss_vfl_aux_0: 1.0070 (0.9287)  loss_vfl_aux_1: 1.0329 (0.9870)  loss_vfl_aux_2: 1.0576 (1.0248)  loss_vfl_aux_3: 1.0267 (1.0431)  loss_vfl_aux_4: 1.0572 (1.0627)  loss_vfl_aux_5: 0.8743 (0.8263)  loss_vfl_dn_0: 0.4684 (0.5032)  loss_vfl_dn_1: 0.5076 (0.5386)  loss_vfl_dn_2: 0.5433 (0.5657)  loss_vfl_dn_3: 0.5501 (0.5788)  loss_vfl_dn_4: 0.5615 (0.5850)  loss_vfl_dn_5: 0.5653 (0.5864)  time: 0.2510  data: 0.0093  max mem: 6602
Epoch: [0]  [ 1000/14786]  eta: 1:01:21  lr: 0.000010  loss: 31.5552 (34.8657)  loss_bbox: 0.3835 (0.6168)  loss_bbox_aux_0: 0.4522 (0.6846)  loss_bbox_aux_1: 0.4175 (0.6489)  loss_bbox_aux_2: 0.4078 (0.6320)  loss_bbox_aux_3: 0.3878 (0.6245)  loss_bbox_aux_4: 0.3909 (0.6203)  loss_bbox_aux_5: 0.5747 (0.7777)  loss_bbox_dn_0: 0.6499 (0.7896)  loss_bbox_dn_1: 0.6370 (0.7611)  loss_bbox_dn_2: 0.6292 (0.7529)  loss_bbox_dn_3: 0.6228 (0.7514)  loss_bbox_dn_4: 0.6248 (0.7512)  loss_bbox_dn_5: 0.6249 (0.7515)  loss_giou: 0.9464 (1.1159)  loss_giou_aux_0: 1.0091 (1.1877)  loss_giou_aux_1: 0.9691 (1.1469)  loss_giou_aux_2: 0.9554 (1.1308)  loss_giou_aux_3: 0.9457 (1.1233)  loss_giou_aux_4: 0.9450 (1.1195)  loss_giou_aux_5: 1.1334 (1.2788)  loss_giou_dn_0: 1.1806 (1.2389)  loss_giou_dn_1: 1.1286 (1.2058)  loss_giou_dn_2: 1.0912 (1.1937)  loss_giou_dn_3: 1.0755 (1.1921)  loss_giou_dn_4: 1.0748 (1.1936)  loss_giou_dn_5: 1.0747 (1.1964)  loss_vfl: 1.1824 (1.0955)  loss_vfl_aux_0: 1.0099 (0.9405)  loss_vfl_aux_1: 1.0889 (0.9972)  loss_vfl_aux_2: 1.1214 (1.0352)  loss_vfl_aux_3: 1.1673 (1.0522)  loss_vfl_aux_4: 1.1825 (1.0712)  loss_vfl_aux_5: 0.9131 (0.8389)  loss_vfl_dn_0: 0.4577 (0.5002)  loss_vfl_dn_1: 0.4996 (0.5358)  loss_vfl_dn_2: 0.5359 (0.5642)  loss_vfl_dn_3: 0.5643 (0.5784)  loss_vfl_dn_4: 0.5814 (0.5848)  loss_vfl_dn_5: 0.5769 (0.5858)  time: 0.2761  data: 0.0092  max mem: 6602
Epoch: [0]  [ 1100/14786]  eta: 1:00:55  lr: 0.000010  loss: 31.5650 (34.6336)  loss_bbox: 0.3992 (0.6015)  loss_bbox_aux_0: 0.4845 (0.6692)  loss_bbox_aux_1: 0.4497 (0.6335)  loss_bbox_aux_2: 0.4182 (0.6164)  loss_bbox_aux_3: 0.4113 (0.6089)  loss_bbox_aux_4: 0.4084 (0.6048)  loss_bbox_aux_5: 0.5683 (0.7642)  loss_bbox_dn_0: 0.6582 (0.7867)  loss_bbox_dn_1: 0.6168 (0.7564)  loss_bbox_dn_2: 0.6032 (0.7477)  loss_bbox_dn_3: 0.5980 (0.7460)  loss_bbox_dn_4: 0.6004 (0.7457)  loss_bbox_dn_5: 0.6004 (0.7460)  loss_giou: 0.8895 (1.0965)  loss_giou_aux_0: 1.0044 (1.1699)  loss_giou_aux_1: 0.9934 (1.1281)  loss_giou_aux_2: 0.9243 (1.1114)  loss_giou_aux_3: 0.9027 (1.1039)  loss_giou_aux_4: 0.8940 (1.1000)  loss_giou_aux_5: 1.1527 (1.2642)  loss_giou_dn_0: 1.1399 (1.2294)  loss_giou_dn_1: 1.0774 (1.1932)  loss_giou_dn_2: 1.0481 (1.1793)  loss_giou_dn_3: 1.0341 (1.1770)  loss_giou_dn_4: 1.0279 (1.1780)  loss_giou_dn_5: 1.0282 (1.1806)  loss_vfl: 1.1394 (1.1094)  loss_vfl_aux_0: 0.9670 (0.9565)  loss_vfl_aux_1: 1.0471 (1.0128)  loss_vfl_aux_2: 1.1040 (1.0509)  loss_vfl_aux_3: 1.0934 (1.0667)  loss_vfl_aux_4: 1.1271 (1.0858)  loss_vfl_aux_5: 0.9008 (0.8563)  loss_vfl_dn_0: 0.4919 (0.4990)  loss_vfl_dn_1: 0.5290 (0.5354)  loss_vfl_dn_2: 0.5780 (0.5651)  loss_vfl_dn_3: 0.6103 (0.5809)  loss_vfl_dn_4: 0.6110 (0.5877)  loss_vfl_dn_5: 0.6062 (0.5886)  time: 0.2691  data: 0.0091  max mem: 6602
Epoch: [0]  [ 1200/14786]  eta: 1:00:26  lr: 0.000010  loss: 31.6638 (34.3468)  loss_bbox: 0.4682 (0.5855)  loss_bbox_aux_0: 0.5541 (0.6533)  loss_bbox_aux_1: 0.5201 (0.6171)  loss_bbox_aux_2: 0.4944 (0.6003)  loss_bbox_aux_3: 0.5044 (0.5928)  loss_bbox_aux_4: 0.4859 (0.5888)  loss_bbox_aux_5: 0.6483 (0.7486)  loss_bbox_dn_0: 0.6917 (0.7758)  loss_bbox_dn_1: 0.6591 (0.7447)  loss_bbox_dn_2: 0.6380 (0.7356)  loss_bbox_dn_3: 0.6272 (0.7338)  loss_bbox_dn_4: 0.6213 (0.7335)  loss_bbox_dn_5: 0.6207 (0.7338)  loss_giou: 0.9158 (1.0833)  loss_giou_aux_0: 0.9841 (1.1576)  loss_giou_aux_1: 0.9230 (1.1151)  loss_giou_aux_2: 0.9133 (1.0982)  loss_giou_aux_3: 0.9078 (1.0905)  loss_giou_aux_4: 0.8967 (1.0867)  loss_giou_aux_5: 1.1579 (1.2546)  loss_giou_dn_0: 1.1224 (1.2224)  loss_giou_dn_1: 1.0492 (1.1841)  loss_giou_dn_2: 1.0222 (1.1689)  loss_giou_dn_3: 1.0111 (1.1660)  loss_giou_dn_4: 1.0148 (1.1667)  loss_giou_dn_5: 1.0171 (1.1692)  loss_vfl: 1.0967 (1.1149)  loss_vfl_aux_0: 1.0443 (0.9638)  loss_vfl_aux_1: 1.1116 (1.0202)  loss_vfl_aux_2: 1.0949 (1.0570)  loss_vfl_aux_3: 1.1180 (1.0728)  loss_vfl_aux_4: 1.1202 (1.0916)  loss_vfl_aux_5: 0.9597 (0.8654)  loss_vfl_dn_0: 0.4777 (0.4975)  loss_vfl_dn_1: 0.5048 (0.5338)  loss_vfl_dn_2: 0.5337 (0.5641)  loss_vfl_dn_3: 0.5513 (0.5813)  loss_vfl_dn_4: 0.5545 (0.5884)  loss_vfl_dn_5: 0.5595 (0.5891)  time: 0.2675  data: 0.0093  max mem: 6602
Epoch: [0]  [ 1300/14786]  eta: 0:59:57  lr: 0.000010  loss: 31.5972 (34.1160)  loss_bbox: 0.4280 (0.5720)  loss_bbox_aux_0: 0.4881 (0.6396)  loss_bbox_aux_1: 0.4662 (0.6033)  loss_bbox_aux_2: 0.4453 (0.5866)  loss_bbox_aux_3: 0.4492 (0.5792)  loss_bbox_aux_4: 0.4277 (0.5753)  loss_bbox_aux_5: 0.6695 (0.7357)  loss_bbox_dn_0: 0.7150 (0.7706)  loss_bbox_dn_1: 0.6532 (0.7381)  loss_bbox_dn_2: 0.6306 (0.7285)  loss_bbox_dn_3: 0.6241 (0.7265)  loss_bbox_dn_4: 0.6244 (0.7261)  loss_bbox_dn_5: 0.6245 (0.7264)  loss_giou: 0.9033 (1.0692)  loss_giou_aux_0: 0.9786 (1.1442)  loss_giou_aux_1: 0.9422 (1.1013)  loss_giou_aux_2: 0.9061 (1.0839)  loss_giou_aux_3: 0.9055 (1.0762)  loss_giou_aux_4: 0.9060 (1.0725)  loss_giou_aux_5: 1.0990 (1.2423)  loss_giou_dn_0: 1.1428 (1.2161)  loss_giou_dn_1: 1.0831 (1.1756)  loss_giou_dn_2: 1.0587 (1.1592)  loss_giou_dn_3: 1.0501 (1.1557)  loss_giou_dn_4: 1.0495 (1.1562)  loss_giou_dn_5: 1.0493 (1.1585)  loss_vfl: 1.1155 (1.1220)  loss_vfl_aux_0: 1.0233 (0.9736)  loss_vfl_aux_1: 1.1150 (1.0294)  loss_vfl_aux_2: 1.0996 (1.0656)  loss_vfl_aux_3: 1.1283 (1.0815)  loss_vfl_aux_4: 1.1524 (1.0990)  loss_vfl_aux_5: 0.9368 (0.8783)  loss_vfl_dn_0: 0.4691 (0.4961)  loss_vfl_dn_1: 0.4920 (0.5318)  loss_vfl_dn_2: 0.5272 (0.5624)  loss_vfl_dn_3: 0.5445 (0.5804)  loss_vfl_dn_4: 0.5553 (0.5881)  loss_vfl_dn_5: 0.5537 (0.5890)  time: 0.2643  data: 0.0097  max mem: 6602
Epoch: [0]  [ 1400/14786]  eta: 0:59:27  lr: 0.000010  loss: 31.0063 (33.9123)  loss_bbox: 0.3333 (0.5592)  loss_bbox_aux_0: 0.4192 (0.6268)  loss_bbox_aux_1: 0.3552 (0.5905)  loss_bbox_aux_2: 0.3513 (0.5738)  loss_bbox_aux_3: 0.3464 (0.5662)  loss_bbox_aux_4: 0.3377 (0.5624)  loss_bbox_aux_5: 0.5447 (0.7235)  loss_bbox_dn_0: 0.7534 (0.7658)  loss_bbox_dn_1: 0.7111 (0.7321)  loss_bbox_dn_2: 0.6852 (0.7221)  loss_bbox_dn_3: 0.6729 (0.7200)  loss_bbox_dn_4: 0.6743 (0.7196)  loss_bbox_dn_5: 0.6746 (0.7198)  loss_giou: 0.8135 (1.0552)  loss_giou_aux_0: 0.8970 (1.1310)  loss_giou_aux_1: 0.8470 (1.0879)  loss_giou_aux_2: 0.8259 (1.0700)  loss_giou_aux_3: 0.8079 (1.0622)  loss_giou_aux_4: 0.8150 (1.0585)  loss_giou_aux_5: 1.0384 (1.2310)  loss_giou_dn_0: 1.1117 (1.2097)  loss_giou_dn_1: 1.0388 (1.1670)  loss_giou_dn_2: 1.0097 (1.1495)  loss_giou_dn_3: 0.9952 (1.1455)  loss_giou_dn_4: 0.9853 (1.1458)  loss_giou_dn_5: 0.9857 (1.1479)  loss_vfl: 1.1870 (1.1314)  loss_vfl_aux_0: 1.1102 (0.9838)  loss_vfl_aux_1: 1.1432 (1.0396)  loss_vfl_aux_2: 1.1761 (1.0760)  loss_vfl_aux_3: 1.1852 (1.0917)  loss_vfl_aux_4: 1.1652 (1.1084)  loss_vfl_aux_5: 1.0922 (0.8907)  loss_vfl_dn_0: 0.4808 (0.4954)  loss_vfl_dn_1: 0.5051 (0.5308)  loss_vfl_dn_2: 0.5338 (0.5619)  loss_vfl_dn_3: 0.5583 (0.5807)  loss_vfl_dn_4: 0.5727 (0.5891)  loss_vfl_dn_5: 0.5851 (0.5900)  time: 0.2574  data: 0.0086  max mem: 6602
Epoch: [0]  [ 1500/14786]  eta: 0:58:56  lr: 0.000010  loss: 29.9024 (33.6985)  loss_bbox: 0.3388 (0.5476)  loss_bbox_aux_0: 0.3979 (0.6150)  loss_bbox_aux_1: 0.3605 (0.5786)  loss_bbox_aux_2: 0.3457 (0.5620)  loss_bbox_aux_3: 0.3443 (0.5548)  loss_bbox_aux_4: 0.3440 (0.5510)  loss_bbox_aux_5: 0.5238 (0.7121)  loss_bbox_dn_0: 0.6119 (0.7601)  loss_bbox_dn_1: 0.6054 (0.7258)  loss_bbox_dn_2: 0.5789 (0.7156)  loss_bbox_dn_3: 0.5788 (0.7134)  loss_bbox_dn_4: 0.5836 (0.7130)  loss_bbox_dn_5: 0.5833 (0.7132)  loss_giou: 0.8784 (1.0446)  loss_giou_aux_0: 0.9559 (1.1205)  loss_giou_aux_1: 0.9227 (1.0775)  loss_giou_aux_2: 0.8951 (1.0594)  loss_giou_aux_3: 0.8723 (1.0515)  loss_giou_aux_4: 0.8793 (1.0477)  loss_giou_aux_5: 1.0391 (1.2220)  loss_giou_dn_0: 1.1368 (1.2049)  loss_giou_dn_1: 1.0607 (1.1608)  loss_giou_dn_2: 1.0308 (1.1423)  loss_giou_dn_3: 1.0218 (1.1379)  loss_giou_dn_4: 1.0215 (1.1380)  loss_giou_dn_5: 1.0223 (1.1400)  loss_vfl: 1.1384 (1.1338)  loss_vfl_aux_0: 1.0388 (0.9886)  loss_vfl_aux_1: 1.1044 (1.0438)  loss_vfl_aux_2: 1.1104 (1.0797)  loss_vfl_aux_3: 1.1465 (1.0951)  loss_vfl_aux_4: 1.1751 (1.1112)  loss_vfl_aux_5: 0.9840 (0.8975)  loss_vfl_dn_0: 0.4748 (0.4942)  loss_vfl_dn_1: 0.4927 (0.5288)  loss_vfl_dn_2: 0.5359 (0.5600)  loss_vfl_dn_3: 0.5633 (0.5795)  loss_vfl_dn_4: 0.5767 (0.5882)  loss_vfl_dn_5: 0.5795 (0.5892)  time: 0.2589  data: 0.0083  max mem: 6602
Epoch: [0]  [ 1600/14786]  eta: 0:58:19  lr: 0.000010  loss: 31.5933 (33.5022)  loss_bbox: 0.3734 (0.5370)  loss_bbox_aux_0: 0.4767 (0.6041)  loss_bbox_aux_1: 0.4295 (0.5678)  loss_bbox_aux_2: 0.3883 (0.5512)  loss_bbox_aux_3: 0.3793 (0.5441)  loss_bbox_aux_4: 0.3742 (0.5403)  loss_bbox_aux_5: 0.6138 (0.7018)  loss_bbox_dn_0: 0.8426 (0.7550)  loss_bbox_dn_1: 0.7696 (0.7198)  loss_bbox_dn_2: 0.7509 (0.7092)  loss_bbox_dn_3: 0.7443 (0.7069)  loss_bbox_dn_4: 0.7426 (0.7065)  loss_bbox_dn_5: 0.7421 (0.7067)  loss_giou: 0.7687 (1.0338)  loss_giou_aux_0: 0.8474 (1.1101)  loss_giou_aux_1: 0.7962 (1.0668)  loss_giou_aux_2: 0.7825 (1.0487)  loss_giou_aux_3: 0.7727 (1.0407)  loss_giou_aux_4: 0.7684 (1.0370)  loss_giou_aux_5: 1.0253 (1.2127)  loss_giou_dn_0: 1.0910 (1.2002)  loss_giou_dn_1: 1.0096 (1.1547)  loss_giou_dn_2: 0.9664 (1.1354)  loss_giou_dn_3: 0.9536 (1.1306)  loss_giou_dn_4: 0.9516 (1.1305)  loss_giou_dn_5: 0.9523 (1.1324)  loss_vfl: 1.3840 (1.1371)  loss_vfl_aux_0: 1.1941 (0.9942)  loss_vfl_aux_1: 1.2288 (1.0487)  loss_vfl_aux_2: 1.2751 (1.0837)  loss_vfl_aux_3: 1.3348 (1.0994)  loss_vfl_aux_4: 1.3396 (1.1151)  loss_vfl_aux_5: 1.0174 (0.9050)  loss_vfl_dn_0: 0.4899 (0.4932)  loss_vfl_dn_1: 0.5276 (0.5274)  loss_vfl_dn_2: 0.5515 (0.5588)  loss_vfl_dn_3: 0.5842 (0.5788)  loss_vfl_dn_4: 0.5932 (0.5879)  loss_vfl_dn_5: 0.5897 (0.5890)  time: 0.2500  data: 0.0076  max mem: 6602
Epoch: [0]  [ 1700/14786]  eta: 0:57:45  lr: 0.000010  loss: 31.5810 (33.3256)  loss_bbox: 0.4016 (0.5279)  loss_bbox_aux_0: 0.4714 (0.5948)  loss_bbox_aux_1: 0.4419 (0.5583)  loss_bbox_aux_2: 0.4137 (0.5419)  loss_bbox_aux_3: 0.4157 (0.5350)  loss_bbox_aux_4: 0.4100 (0.5313)  loss_bbox_aux_5: 0.5861 (0.6929)  loss_bbox_dn_0: 0.7526 (0.7508)  loss_bbox_dn_1: 0.7109 (0.7145)  loss_bbox_dn_2: 0.6905 (0.7036)  loss_bbox_dn_3: 0.6859 (0.7012)  loss_bbox_dn_4: 0.6863 (0.7007)  loss_bbox_dn_5: 0.6860 (0.7009)  loss_giou: 0.7873 (1.0235)  loss_giou_aux_0: 0.8920 (1.0999)  loss_giou_aux_1: 0.8013 (1.0563)  loss_giou_aux_2: 0.8001 (1.0383)  loss_giou_aux_3: 0.7726 (1.0303)  loss_giou_aux_4: 0.7725 (1.0265)  loss_giou_aux_5: 1.0110 (1.2036)  loss_giou_dn_0: 1.1099 (1.1951)  loss_giou_dn_1: 1.0166 (1.1476)  loss_giou_dn_2: 0.9713 (1.1274)  loss_giou_dn_3: 0.9631 (1.1222)  loss_giou_dn_4: 0.9574 (1.1220)  loss_giou_dn_5: 0.9573 (1.1238)  loss_vfl: 1.2274 (1.1410)  loss_vfl_aux_0: 1.0388 (1.0002)  loss_vfl_aux_1: 1.1429 (1.0553)  loss_vfl_aux_2: 1.2145 (1.0890)  loss_vfl_aux_3: 1.2158 (1.1039)  loss_vfl_aux_4: 1.2182 (1.1192)  loss_vfl_aux_5: 1.0539 (0.9138)  loss_vfl_dn_0: 0.4926 (0.4926)  loss_vfl_dn_1: 0.5072 (0.5265)  loss_vfl_dn_2: 0.5431 (0.5579)  loss_vfl_dn_3: 0.5710 (0.5784)  loss_vfl_dn_4: 0.5936 (0.5880)  loss_vfl_dn_5: 0.6025 (0.5895)  time: 0.2462  data: 0.0078  max mem: 6602
Epoch: [0]  [ 1800/14786]  eta: 0:57:19  lr: 0.000010  loss: 28.9612 (33.1809)  loss_bbox: 0.3067 (0.5199)  loss_bbox_aux_0: 0.3552 (0.5870)  loss_bbox_aux_1: 0.3396 (0.5504)  loss_bbox_aux_2: 0.3170 (0.5339)  loss_bbox_aux_3: 0.3100 (0.5270)  loss_bbox_aux_4: 0.3073 (0.5233)  loss_bbox_aux_5: 0.4495 (0.6861)  loss_bbox_dn_0: 0.5381 (0.7494)  loss_bbox_dn_1: 0.5275 (0.7118)  loss_bbox_dn_2: 0.5256 (0.7003)  loss_bbox_dn_3: 0.5267 (0.6977)  loss_bbox_dn_4: 0.5274 (0.6972)  loss_bbox_dn_5: 0.5279 (0.6974)  loss_giou: 0.8334 (1.0127)  loss_giou_aux_0: 0.9017 (1.0897)  loss_giou_aux_1: 0.8779 (1.0459)  loss_giou_aux_2: 0.8447 (1.0276)  loss_giou_aux_3: 0.8442 (1.0197)  loss_giou_aux_4: 0.8301 (1.0158)  loss_giou_aux_5: 1.0186 (1.1946)  loss_giou_dn_0: 1.1235 (1.1902)  loss_giou_dn_1: 1.0364 (1.1408)  loss_giou_dn_2: 0.9952 (1.1197)  loss_giou_dn_3: 0.9688 (1.1140)  loss_giou_dn_4: 0.9618 (1.1135)  loss_giou_dn_5: 0.9624 (1.1153)  loss_vfl: 1.1461 (1.1468)  loss_vfl_aux_0: 1.0543 (1.0072)  loss_vfl_aux_1: 1.0918 (1.0618)  loss_vfl_aux_2: 1.1041 (1.0950)  loss_vfl_aux_3: 1.1294 (1.1097)  loss_vfl_aux_4: 1.1376 (1.1248)  loss_vfl_aux_5: 1.0026 (0.9222)  loss_vfl_dn_0: 0.4697 (0.4921)  loss_vfl_dn_1: 0.5003 (0.5257)  loss_vfl_dn_2: 0.5360 (0.5574)  loss_vfl_dn_3: 0.5629 (0.5787)  loss_vfl_dn_4: 0.5730 (0.5886)  loss_vfl_dn_5: 0.5787 (0.5903)  time: 0.2609  data: 0.0088  max mem: 6602
Epoch: [0]  [ 1900/14786]  eta: 0:56:49  lr: 0.000010  loss: 29.5092 (33.0213)  loss_bbox: 0.3447 (0.5122)  loss_bbox_aux_0: 0.4257 (0.5790)  loss_bbox_aux_1: 0.3667 (0.5423)  loss_bbox_aux_2: 0.3596 (0.5261)  loss_bbox_aux_3: 0.3458 (0.5192)  loss_bbox_aux_4: 0.3458 (0.5156)  loss_bbox_aux_5: 0.5148 (0.6784)  loss_bbox_dn_0: 0.6508 (0.7454)  loss_bbox_dn_1: 0.5917 (0.7067)  loss_bbox_dn_2: 0.5840 (0.6948)  loss_bbox_dn_3: 0.5793 (0.6921)  loss_bbox_dn_4: 0.5797 (0.6915)  loss_bbox_dn_5: 0.5795 (0.6917)  loss_giou: 0.7979 (1.0042)  loss_giou_aux_0: 0.8940 (1.0815)  loss_giou_aux_1: 0.8178 (1.0374)  loss_giou_aux_2: 0.7999 (1.0191)  loss_giou_aux_3: 0.8045 (1.0112)  loss_giou_aux_4: 0.7956 (1.0073)  loss_giou_aux_5: 1.0186 (1.1873)  loss_giou_dn_0: 1.1129 (1.1861)  loss_giou_dn_1: 1.0134 (1.1354)  loss_giou_dn_2: 0.9748 (1.1135)  loss_giou_dn_3: 0.9583 (1.1075)  loss_giou_dn_4: 0.9545 (1.1069)  loss_giou_dn_5: 0.9538 (1.1085)  loss_vfl: 1.1022 (1.1486)  loss_vfl_aux_0: 1.0358 (1.0116)  loss_vfl_aux_1: 1.1539 (1.0655)  loss_vfl_aux_2: 1.1339 (1.0976)  loss_vfl_aux_3: 1.1337 (1.1119)  loss_vfl_aux_4: 1.0867 (1.1269)  loss_vfl_aux_5: 1.0395 (0.9289)  loss_vfl_dn_0: 0.4894 (0.4915)  loss_vfl_dn_1: 0.5037 (0.5248)  loss_vfl_dn_2: 0.5341 (0.5565)  loss_vfl_dn_3: 0.5606 (0.5781)  loss_vfl_dn_4: 0.5836 (0.5884)  loss_vfl_dn_5: 0.5943 (0.5902)  time: 0.2573  data: 0.0081  max mem: 6602
Epoch: [0]  [ 2000/14786]  eta: 0:56:21  lr: 0.000010  loss: 29.0941 (32.8607)  loss_bbox: 0.3317 (0.5048)  loss_bbox_aux_0: 0.3810 (0.5710)  loss_bbox_aux_1: 0.3563 (0.5345)  loss_bbox_aux_2: 0.3450 (0.5184)  loss_bbox_aux_3: 0.3540 (0.5117)  loss_bbox_aux_4: 0.3280 (0.5081)  loss_bbox_aux_5: 0.5123 (0.6703)  loss_bbox_dn_0: 0.5437 (0.7403)  loss_bbox_dn_1: 0.4842 (0.7010)  loss_bbox_dn_2: 0.4634 (0.6889)  loss_bbox_dn_3: 0.4580 (0.6861)  loss_bbox_dn_4: 0.4563 (0.6855)  loss_bbox_dn_5: 0.4563 (0.6857)  loss_giou: 0.8983 (0.9967)  loss_giou_aux_0: 1.0117 (1.0739)  loss_giou_aux_1: 0.9572 (1.0300)  loss_giou_aux_2: 0.9252 (1.0117)  loss_giou_aux_3: 0.9204 (1.0039)  loss_giou_aux_4: 0.9079 (0.9999)  loss_giou_aux_5: 1.1025 (1.1801)  loss_giou_dn_0: 1.1313 (1.1825)  loss_giou_dn_1: 1.0488 (1.1304)  loss_giou_dn_2: 1.0162 (1.1079)  loss_giou_dn_3: 1.0097 (1.1016)  loss_giou_dn_4: 1.0094 (1.1009)  loss_giou_dn_5: 1.0089 (1.1025)  loss_vfl: 1.0719 (1.1494)  loss_vfl_aux_0: 0.9382 (1.0148)  loss_vfl_aux_1: 0.9902 (1.0680)  loss_vfl_aux_2: 1.0202 (1.0994)  loss_vfl_aux_3: 1.0261 (1.1131)  loss_vfl_aux_4: 1.0760 (1.1282)  loss_vfl_aux_5: 0.9143 (0.9345)  loss_vfl_dn_0: 0.4568 (0.4909)  loss_vfl_dn_1: 0.4777 (0.5237)  loss_vfl_dn_2: 0.5120 (0.5554)  loss_vfl_dn_3: 0.5409 (0.5772)  loss_vfl_dn_4: 0.5566 (0.5879)  loss_vfl_dn_5: 0.5585 (0.5899)  time: 0.2644  data: 0.0095  max mem: 6602
Epoch: [0]  [ 2100/14786]  eta: 0:56:02  lr: 0.000010  loss: 30.1210 (32.7214)  loss_bbox: 0.3679 (0.4972)  loss_bbox_aux_0: 0.4404 (0.5634)  loss_bbox_aux_1: 0.3997 (0.5269)  loss_bbox_aux_2: 0.3772 (0.5107)  loss_bbox_aux_3: 0.3747 (0.5040)  loss_bbox_aux_4: 0.3790 (0.5006)  loss_bbox_aux_5: 0.5587 (0.6634)  loss_bbox_dn_0: 0.6791 (0.7376)  loss_bbox_dn_1: 0.6165 (0.6973)  loss_bbox_dn_2: 0.5880 (0.6849)  loss_bbox_dn_3: 0.5822 (0.6821)  loss_bbox_dn_4: 0.5851 (0.6815)  loss_bbox_dn_5: 0.5853 (0.6816)  loss_giou: 0.8692 (0.9882)  loss_giou_aux_0: 0.9331 (1.0655)  loss_giou_aux_1: 0.9055 (1.0216)  loss_giou_aux_2: 0.9016 (1.0033)  loss_giou_aux_3: 0.8915 (0.9955)  loss_giou_aux_4: 0.8883 (0.9914)  loss_giou_aux_5: 1.0829 (1.1726)  loss_giou_dn_0: 1.0915 (1.1788)  loss_giou_dn_1: 1.0326 (1.1252)  loss_giou_dn_2: 1.0053 (1.1020)  loss_giou_dn_3: 1.0023 (1.0954)  loss_giou_dn_4: 1.0007 (1.0946)  loss_giou_dn_5: 1.0007 (1.0962)  loss_vfl: 1.0280 (1.1525)  loss_vfl_aux_0: 0.9980 (1.0197)  loss_vfl_aux_1: 1.0130 (1.0725)  loss_vfl_aux_2: 1.0088 (1.1034)  loss_vfl_aux_3: 1.0294 (1.1169)  loss_vfl_aux_4: 1.0280 (1.1315)  loss_vfl_aux_5: 0.8986 (0.9404)  loss_vfl_dn_0: 0.4843 (0.4905)  loss_vfl_dn_1: 0.4992 (0.5231)  loss_vfl_dn_2: 0.5334 (0.5547)  loss_vfl_dn_3: 0.5502 (0.5769)  loss_vfl_dn_4: 0.5707 (0.5878)  loss_vfl_dn_5: 0.5724 (0.5900)  time: 0.3243  data: 0.0078  max mem: 6602
Epoch: [0]  [ 2200/14786]  eta: 0:55:40  lr: 0.000010  loss: 29.1613 (32.5931)  loss_bbox: 0.3470 (0.4913)  loss_bbox_aux_0: 0.4177 (0.5572)  loss_bbox_aux_1: 0.3553 (0.5206)  loss_bbox_aux_2: 0.3456 (0.5046)  loss_bbox_aux_3: 0.3652 (0.4980)  loss_bbox_aux_4: 0.3575 (0.4946)  loss_bbox_aux_5: 0.4812 (0.6574)  loss_bbox_dn_0: 0.6016 (0.7351)  loss_bbox_dn_1: 0.5681 (0.6941)  loss_bbox_dn_2: 0.5484 (0.6814)  loss_bbox_dn_3: 0.5472 (0.6784)  loss_bbox_dn_4: 0.5469 (0.6778)  loss_bbox_dn_5: 0.5471 (0.6779)  loss_giou: 0.8388 (0.9811)  loss_giou_aux_0: 0.9256 (1.0584)  loss_giou_aux_1: 0.8853 (1.0143)  loss_giou_aux_2: 0.8365 (0.9962)  loss_giou_aux_3: 0.8354 (0.9884)  loss_giou_aux_4: 0.8291 (0.9843)  loss_giou_aux_5: 1.0789 (1.1661)  loss_giou_dn_0: 1.1042 (1.1749)  loss_giou_dn_1: 1.0156 (1.1200)  loss_giou_dn_2: 0.9900 (1.0961)  loss_giou_dn_3: 0.9786 (1.0892)  loss_giou_dn_4: 0.9740 (1.0883)  loss_giou_dn_5: 0.9737 (1.0898)  loss_vfl: 1.1029 (1.1546)  loss_vfl_aux_0: 0.9491 (1.0227)  loss_vfl_aux_1: 1.0693 (1.0757)  loss_vfl_aux_2: 1.1135 (1.1058)  loss_vfl_aux_3: 1.0869 (1.1191)  loss_vfl_aux_4: 1.0959 (1.1337)  loss_vfl_aux_5: 0.9588 (0.9459)  loss_vfl_dn_0: 0.4709 (0.4901)  loss_vfl_dn_1: 0.5027 (0.5224)  loss_vfl_dn_2: 0.5251 (0.5539)  loss_vfl_dn_3: 0.5552 (0.5764)  loss_vfl_dn_4: 0.5726 (0.5874)  loss_vfl_dn_5: 0.5800 (0.5900)  time: 0.2647  data: 0.0082  max mem: 6602
Epoch: [0]  [ 2300/14786]  eta: 0:55:11  lr: 0.000010  loss: 29.6092 (32.4842)  loss_bbox: 0.3758 (0.4863)  loss_bbox_aux_0: 0.4260 (0.5519)  loss_bbox_aux_1: 0.3889 (0.5154)  loss_bbox_aux_2: 0.3783 (0.4994)  loss_bbox_aux_3: 0.3779 (0.4929)  loss_bbox_aux_4: 0.3720 (0.4895)  loss_bbox_aux_5: 0.4889 (0.6521)  loss_bbox_dn_0: 0.6834 (0.7332)  loss_bbox_dn_1: 0.6348 (0.6916)  loss_bbox_dn_2: 0.6206 (0.6786)  loss_bbox_dn_3: 0.6108 (0.6756)  loss_bbox_dn_4: 0.6072 (0.6750)  loss_bbox_dn_5: 0.6070 (0.6751)  loss_giou: 0.7900 (0.9746)  loss_giou_aux_0: 0.8875 (1.0519)  loss_giou_aux_1: 0.8215 (1.0079)  loss_giou_aux_2: 0.7906 (0.9898)  loss_giou_aux_3: 0.7865 (0.9819)  loss_giou_aux_4: 0.7969 (0.9777)  loss_giou_aux_5: 0.9976 (1.1601)  loss_giou_dn_0: 1.0830 (1.1717)  loss_giou_dn_1: 0.9950 (1.1158)  loss_giou_dn_2: 0.9488 (1.0915)  loss_giou_dn_3: 0.9316 (1.0843)  loss_giou_dn_4: 0.9282 (1.0833)  loss_giou_dn_5: 0.9280 (1.0848)  loss_vfl: 1.1358 (1.1563)  loss_vfl_aux_0: 1.0714 (1.0259)  loss_vfl_aux_1: 1.1080 (1.0784)  loss_vfl_aux_2: 1.1162 (1.1079)  loss_vfl_aux_3: 1.1292 (1.1214)  loss_vfl_aux_4: 1.1241 (1.1356)  loss_vfl_aux_5: 1.0382 (0.9502)  loss_vfl_dn_0: 0.4754 (0.4897)  loss_vfl_dn_1: 0.5097 (0.5217)  loss_vfl_dn_2: 0.5355 (0.5529)  loss_vfl_dn_3: 0.5669 (0.5757)  loss_vfl_dn_4: 0.5726 (0.5869)  loss_vfl_dn_5: 0.5864 (0.5898)  time: 0.2679  data: 0.0090  max mem: 6602
Epoch: [0]  [ 2400/14786]  eta: 0:54:45  lr: 0.000010  loss: 30.1101 (32.3563)  loss_bbox: 0.3131 (0.4800)  loss_bbox_aux_0: 0.3652 (0.5455)  loss_bbox_aux_1: 0.3296 (0.5089)  loss_bbox_aux_2: 0.3383 (0.4931)  loss_bbox_aux_3: 0.3277 (0.4866)  loss_bbox_aux_4: 0.3283 (0.4833)  loss_bbox_aux_5: 0.5207 (0.6466)  loss_bbox_dn_0: 0.6930 (0.7305)  loss_bbox_dn_1: 0.6275 (0.6880)  loss_bbox_dn_2: 0.5975 (0.6749)  loss_bbox_dn_3: 0.5899 (0.6718)  loss_bbox_dn_4: 0.5861 (0.6711)  loss_bbox_dn_5: 0.5859 (0.6713)  loss_giou: 0.8241 (0.9661)  loss_giou_aux_0: 0.8875 (1.0435)  loss_giou_aux_1: 0.8443 (0.9994)  loss_giou_aux_2: 0.8333 (0.9813)  loss_giou_aux_3: 0.8391 (0.9733)  loss_giou_aux_4: 0.8367 (0.9693)  loss_giou_aux_5: 0.9901 (1.1528)  loss_giou_dn_0: 1.0908 (1.1680)  loss_giou_dn_1: 0.9972 (1.1107)  loss_giou_dn_2: 0.9610 (1.0858)  loss_giou_dn_3: 0.9492 (1.0783)  loss_giou_dn_4: 0.9471 (1.0773)  loss_giou_dn_5: 0.9468 (1.0787)  loss_vfl: 1.1556 (1.1592)  loss_vfl_aux_0: 1.1072 (1.0308)  loss_vfl_aux_1: 1.1322 (1.0828)  loss_vfl_aux_2: 1.1072 (1.1116)  loss_vfl_aux_3: 1.1452 (1.1251)  loss_vfl_aux_4: 1.1360 (1.1388)  loss_vfl_aux_5: 1.0228 (0.9561)  loss_vfl_dn_0: 0.4783 (0.4896)  loss_vfl_dn_1: 0.4993 (0.5213)  loss_vfl_dn_2: 0.5210 (0.5523)  loss_vfl_dn_3: 0.5479 (0.5755)  loss_vfl_dn_4: 0.5672 (0.5870)  loss_vfl_dn_5: 0.5773 (0.5901)  time: 0.2730  data: 0.0094  max mem: 6602
Epoch: [0]  [ 2500/14786]  eta: 0:54:17  lr: 0.000010  loss: 29.1696 (32.2291)  loss_bbox: 0.3495 (0.4743)  loss_bbox_aux_0: 0.4091 (0.5394)  loss_bbox_aux_1: 0.3835 (0.5029)  loss_bbox_aux_2: 0.3581 (0.4873)  loss_bbox_aux_3: 0.3572 (0.4808)  loss_bbox_aux_4: 0.3424 (0.4775)  loss_bbox_aux_5: 0.4894 (0.6405)  loss_bbox_dn_0: 0.6293 (0.7270)  loss_bbox_dn_1: 0.5669 (0.6841)  loss_bbox_dn_2: 0.5547 (0.6707)  loss_bbox_dn_3: 0.5567 (0.6676)  loss_bbox_dn_4: 0.5564 (0.6669)  loss_bbox_dn_5: 0.5558 (0.6670)  loss_giou: 0.6920 (0.9600)  loss_giou_aux_0: 0.7643 (1.0373)  loss_giou_aux_1: 0.7185 (0.9933)  loss_giou_aux_2: 0.7173 (0.9753)  loss_giou_aux_3: 0.6887 (0.9672)  loss_giou_aux_4: 0.7008 (0.9632)  loss_giou_aux_5: 0.9408 (1.1469)  loss_giou_dn_0: 1.0610 (1.1652)  loss_giou_dn_1: 0.9685 (1.1069)  loss_giou_dn_2: 0.9224 (1.0816)  loss_giou_dn_3: 0.9101 (1.0738)  loss_giou_dn_4: 0.9033 (1.0727)  loss_giou_dn_5: 0.9028 (1.0741)  loss_vfl: 1.2745 (1.1597)  loss_vfl_aux_0: 1.1575 (1.0327)  loss_vfl_aux_1: 1.2406 (1.0843)  loss_vfl_aux_2: 1.2748 (1.1127)  loss_vfl_aux_3: 1.2612 (1.1261)  loss_vfl_aux_4: 1.2645 (1.1396)  loss_vfl_aux_5: 1.1030 (0.9594)  loss_vfl_dn_0: 0.4886 (0.4891)  loss_vfl_dn_1: 0.5054 (0.5204)  loss_vfl_dn_2: 0.5372 (0.5511)  loss_vfl_dn_3: 0.5636 (0.5745)  loss_vfl_dn_4: 0.5853 (0.5863)  loss_vfl_dn_5: 0.6004 (0.5897)  time: 0.2654  data: 0.0099  max mem: 6602
Epoch: [0]  [ 2600/14786]  eta: 0:53:49  lr: 0.000010  loss: 28.3545 (32.1007)  loss_bbox: 0.2862 (0.4685)  loss_bbox_aux_0: 0.3336 (0.5332)  loss_bbox_aux_1: 0.3123 (0.4968)  loss_bbox_aux_2: 0.2813 (0.4813)  loss_bbox_aux_3: 0.2779 (0.4748)  loss_bbox_aux_4: 0.2801 (0.4716)  loss_bbox_aux_5: 0.4353 (0.6345)  loss_bbox_dn_0: 0.5310 (0.7235)  loss_bbox_dn_1: 0.5042 (0.6799)  loss_bbox_dn_2: 0.4948 (0.6664)  loss_bbox_dn_3: 0.4890 (0.6632)  loss_bbox_dn_4: 0.4875 (0.6624)  loss_bbox_dn_5: 0.4874 (0.6626)  loss_giou: 0.7719 (0.9538)  loss_giou_aux_0: 0.8177 (1.0307)  loss_giou_aux_1: 0.7878 (0.9870)  loss_giou_aux_2: 0.7705 (0.9690)  loss_giou_aux_3: 0.7708 (0.9609)  loss_giou_aux_4: 0.7746 (0.9569)  loss_giou_aux_5: 0.9371 (1.1411)  loss_giou_dn_0: 1.1068 (1.1617)  loss_giou_dn_1: 0.9975 (1.1023)  loss_giou_dn_2: 0.9667 (1.0765)  loss_giou_dn_3: 0.9583 (1.0685)  loss_giou_dn_4: 0.9602 (1.0673)  loss_giou_dn_5: 0.9593 (1.0686)  loss_vfl: 1.1466 (1.1607)  loss_vfl_aux_0: 1.0906 (1.0359)  loss_vfl_aux_1: 1.0964 (1.0866)  loss_vfl_aux_2: 1.1071 (1.1145)  loss_vfl_aux_3: 1.1326 (1.1276)  loss_vfl_aux_4: 1.1390 (1.1411)  loss_vfl_aux_5: 1.0654 (0.9637)  loss_vfl_dn_0: 0.4789 (0.4889)  loss_vfl_dn_1: 0.5154 (0.5199)  loss_vfl_dn_2: 0.5351 (0.5503)  loss_vfl_dn_3: 0.5511 (0.5737)  loss_vfl_dn_4: 0.5646 (0.5858)  loss_vfl_dn_5: 0.5762 (0.5893)  time: 0.2636  data: 0.0088  max mem: 6602
Epoch: [0]  [ 2700/14786]  eta: 0:53:18  lr: 0.000010  loss: 29.0325 (31.9919)  loss_bbox: 0.3562 (0.4636)  loss_bbox_aux_0: 0.3815 (0.5280)  loss_bbox_aux_1: 0.3581 (0.4917)  loss_bbox_aux_2: 0.3570 (0.4762)  loss_bbox_aux_3: 0.3497 (0.4698)  loss_bbox_aux_4: 0.3464 (0.4667)  loss_bbox_aux_5: 0.4858 (0.6296)  loss_bbox_dn_0: 0.6389 (0.7213)  loss_bbox_dn_1: 0.5998 (0.6769)  loss_bbox_dn_2: 0.5863 (0.6631)  loss_bbox_dn_3: 0.5825 (0.6598)  loss_bbox_dn_4: 0.5814 (0.6590)  loss_bbox_dn_5: 0.5816 (0.6592)  loss_giou: 0.7756 (0.9472)  loss_giou_aux_0: 0.8488 (1.0241)  loss_giou_aux_1: 0.8294 (0.9805)  loss_giou_aux_2: 0.7976 (0.9625)  loss_giou_aux_3: 0.7791 (0.9544)  loss_giou_aux_4: 0.7798 (0.9503)  loss_giou_aux_5: 0.9540 (1.1353)  loss_giou_dn_0: 1.0726 (1.1585)  loss_giou_dn_1: 0.9950 (1.0977)  loss_giou_dn_2: 0.9651 (1.0715)  loss_giou_dn_3: 0.9502 (1.0632)  loss_giou_dn_4: 0.9457 (1.0619)  loss_giou_dn_5: 0.9463 (1.0632)  loss_vfl: 1.1840 (1.1628)  loss_vfl_aux_0: 1.0896 (1.0395)  loss_vfl_aux_1: 1.1844 (1.0899)  loss_vfl_aux_2: 1.1923 (1.1171)  loss_vfl_aux_3: 1.2094 (1.1301)  loss_vfl_aux_4: 1.1907 (1.1433)  loss_vfl_aux_5: 1.0411 (0.9682)  loss_vfl_dn_0: 0.4708 (0.4888)  loss_vfl_dn_1: 0.4933 (0.5195)  loss_vfl_dn_2: 0.5169 (0.5496)  loss_vfl_dn_3: 0.5527 (0.5732)  loss_vfl_dn_4: 0.5671 (0.5855)  loss_vfl_dn_5: 0.5845 (0.5892)  time: 0.2383  data: 0.0081  max mem: 6602
Epoch: [0]  [ 2800/14786]  eta: 0:52:49  lr: 0.000010  loss: 28.5754 (31.8876)  loss_bbox: 0.2782 (0.4593)  loss_bbox_aux_0: 0.3415 (0.5236)  loss_bbox_aux_1: 0.3110 (0.4871)  loss_bbox_aux_2: 0.2814 (0.4718)  loss_bbox_aux_3: 0.2844 (0.4654)  loss_bbox_aux_4: 0.2896 (0.4624)  loss_bbox_aux_5: 0.4605 (0.6256)  loss_bbox_dn_0: 0.6521 (0.7198)  loss_bbox_dn_1: 0.5956 (0.6747)  loss_bbox_dn_2: 0.5776 (0.6606)  loss_bbox_dn_3: 0.5733 (0.6572)  loss_bbox_dn_4: 0.5691 (0.6565)  loss_bbox_dn_5: 0.5682 (0.6566)  loss_giou: 0.7128 (0.9408)  loss_giou_aux_0: 0.8041 (1.0178)  loss_giou_aux_1: 0.7316 (0.9739)  loss_giou_aux_2: 0.7245 (0.9561)  loss_giou_aux_3: 0.7280 (0.9479)  loss_giou_aux_4: 0.7110 (0.9439)  loss_giou_aux_5: 0.9029 (1.1296)  loss_giou_dn_0: 1.0389 (1.1551)  loss_giou_dn_1: 0.9501 (1.0933)  loss_giou_dn_2: 0.9099 (1.0665)  loss_giou_dn_3: 0.8922 (1.0580)  loss_giou_dn_4: 0.8894 (1.0567)  loss_giou_dn_5: 0.8894 (1.0579)  loss_vfl: 1.1577 (1.1638)  loss_vfl_aux_0: 1.0701 (1.0422)  loss_vfl_aux_1: 1.1301 (1.0926)  loss_vfl_aux_2: 1.1536 (1.1192)  loss_vfl_aux_3: 1.1552 (1.1315)  loss_vfl_aux_4: 1.1333 (1.1445)  loss_vfl_aux_5: 1.0909 (0.9721)  loss_vfl_dn_0: 0.4849 (0.4887)  loss_vfl_dn_1: 0.4986 (0.5191)  loss_vfl_dn_2: 0.5187 (0.5489)  loss_vfl_dn_3: 0.5389 (0.5725)  loss_vfl_dn_4: 0.5550 (0.5851)  loss_vfl_dn_5: 0.5758 (0.5892)  time: 0.2489  data: 0.0077  max mem: 6602
Epoch: [0]  [ 2900/14786]  eta: 0:52:21  lr: 0.000010  loss: 29.0346 (31.7811)  loss_bbox: 0.2935 (0.4547)  loss_bbox_aux_0: 0.3503 (0.5189)  loss_bbox_aux_1: 0.3062 (0.4823)  loss_bbox_aux_2: 0.2954 (0.4672)  loss_bbox_aux_3: 0.2935 (0.4608)  loss_bbox_aux_4: 0.2994 (0.4579)  loss_bbox_aux_5: 0.4880 (0.6214)  loss_bbox_dn_0: 0.6668 (0.7180)  loss_bbox_dn_1: 0.5888 (0.6723)  loss_bbox_dn_2: 0.5774 (0.6580)  loss_bbox_dn_3: 0.5711 (0.6545)  loss_bbox_dn_4: 0.5696 (0.6537)  loss_bbox_dn_5: 0.5699 (0.6538)  loss_giou: 0.6664 (0.9346)  loss_giou_aux_0: 0.7445 (1.0115)  loss_giou_aux_1: 0.6935 (0.9677)  loss_giou_aux_2: 0.6704 (0.9498)  loss_giou_aux_3: 0.6805 (0.9417)  loss_giou_aux_4: 0.6826 (0.9377)  loss_giou_aux_5: 0.9512 (1.1241)  loss_giou_dn_0: 1.0436 (1.1518)  loss_giou_dn_1: 0.9382 (1.0889)  loss_giou_dn_2: 0.9021 (1.0617)  loss_giou_dn_3: 0.8836 (1.0529)  loss_giou_dn_4: 0.8771 (1.0515)  loss_giou_dn_5: 0.8785 (1.0527)  loss_vfl: 1.2158 (1.1651)  loss_vfl_aux_0: 1.1608 (1.0449)  loss_vfl_aux_1: 1.2295 (1.0949)  loss_vfl_aux_2: 1.2420 (1.1213)  loss_vfl_aux_3: 1.2107 (1.1332)  loss_vfl_aux_4: 1.2107 (1.1460)  loss_vfl_aux_5: 1.1081 (0.9755)  loss_vfl_dn_0: 0.4782 (0.4886)  loss_vfl_dn_1: 0.5023 (0.5186)  loss_vfl_dn_2: 0.5119 (0.5480)  loss_vfl_dn_3: 0.5257 (0.5717)  loss_vfl_dn_4: 0.5388 (0.5843)  loss_vfl_dn_5: 0.5517 (0.5886)  time: 0.2685  data: 0.0091  max mem: 6602
Epoch: [0]  [ 3000/14786]  eta: 0:51:53  lr: 0.000010  loss: 28.3106 (31.6784)  loss_bbox: 0.3068 (0.4501)  loss_bbox_aux_0: 0.3499 (0.5140)  loss_bbox_aux_1: 0.3100 (0.4775)  loss_bbox_aux_2: 0.3112 (0.4625)  loss_bbox_aux_3: 0.3006 (0.4562)  loss_bbox_aux_4: 0.3136 (0.4533)  loss_bbox_aux_5: 0.4757 (0.6169)  loss_bbox_dn_0: 0.5868 (0.7157)  loss_bbox_dn_1: 0.5203 (0.6694)  loss_bbox_dn_2: 0.5219 (0.6550)  loss_bbox_dn_3: 0.5338 (0.6515)  loss_bbox_dn_4: 0.5317 (0.6506)  loss_bbox_dn_5: 0.5316 (0.6507)  loss_giou: 0.7442 (0.9290)  loss_giou_aux_0: 0.8261 (1.0058)  loss_giou_aux_1: 0.7556 (0.9621)  loss_giou_aux_2: 0.7474 (0.9442)  loss_giou_aux_3: 0.7454 (0.9362)  loss_giou_aux_4: 0.7494 (0.9321)  loss_giou_aux_5: 1.0014 (1.1190)  loss_giou_dn_0: 1.0577 (1.1486)  loss_giou_dn_1: 0.9739 (1.0847)  loss_giou_dn_2: 0.9316 (1.0571)  loss_giou_dn_3: 0.9160 (1.0481)  loss_giou_dn_4: 0.9114 (1.0466)  loss_giou_dn_5: 0.9115 (1.0478)  loss_vfl: 1.1211 (1.1662)  loss_vfl_aux_0: 1.1323 (1.0478)  loss_vfl_aux_1: 1.0658 (1.0975)  loss_vfl_aux_2: 1.1178 (1.1231)  loss_vfl_aux_3: 1.0971 (1.1348)  loss_vfl_aux_4: 1.0977 (1.1472)  loss_vfl_aux_5: 1.0391 (0.9786)  loss_vfl_dn_0: 0.4803 (0.4887)  loss_vfl_dn_1: 0.5049 (0.5183)  loss_vfl_dn_2: 0.5151 (0.5475)  loss_vfl_dn_3: 0.5369 (0.5712)  loss_vfl_dn_4: 0.5464 (0.5841)  loss_vfl_dn_5: 0.5563 (0.5886)  time: 0.2625  data: 0.0102  max mem: 6602
Epoch: [0]  [ 3100/14786]  eta: 0:51:25  lr: 0.000010  loss: 28.1522 (31.5804)  loss_bbox: 0.3009 (0.4464)  loss_bbox_aux_0: 0.3595 (0.5099)  loss_bbox_aux_1: 0.3066 (0.4735)  loss_bbox_aux_2: 0.2956 (0.4587)  loss_bbox_aux_3: 0.3050 (0.4525)  loss_bbox_aux_4: 0.3021 (0.4497)  loss_bbox_aux_5: 0.4807 (0.6127)  loss_bbox_dn_0: 0.6297 (0.7136)  loss_bbox_dn_1: 0.5353 (0.6667)  loss_bbox_dn_2: 0.5052 (0.6520)  loss_bbox_dn_3: 0.4944 (0.6485)  loss_bbox_dn_4: 0.4911 (0.6476)  loss_bbox_dn_5: 0.4909 (0.6477)  loss_giou: 0.7753 (0.9245)  loss_giou_aux_0: 0.8321 (1.0011)  loss_giou_aux_1: 0.7850 (0.9575)  loss_giou_aux_2: 0.7654 (0.9396)  loss_giou_aux_3: 0.7725 (0.9316)  loss_giou_aux_4: 0.7806 (0.9277)  loss_giou_aux_5: 0.9500 (1.1145)  loss_giou_dn_0: 1.0507 (1.1458)  loss_giou_dn_1: 0.9710 (1.0809)  loss_giou_dn_2: 0.9389 (1.0531)  loss_giou_dn_3: 0.9218 (1.0439)  loss_giou_dn_4: 0.9189 (1.0423)  loss_giou_dn_5: 0.9190 (1.0435)  loss_vfl: 1.0823 (1.1656)  loss_vfl_aux_0: 1.0327 (1.0497)  loss_vfl_aux_1: 1.1053 (1.0987)  loss_vfl_aux_2: 1.0561 (1.1233)  loss_vfl_aux_3: 1.0671 (1.1346)  loss_vfl_aux_4: 1.0484 (1.1467)  loss_vfl_aux_5: 0.9592 (0.9814)  loss_vfl_dn_0: 0.4822 (0.4886)  loss_vfl_dn_1: 0.4928 (0.5179)  loss_vfl_dn_2: 0.5024 (0.5466)  loss_vfl_dn_3: 0.5292 (0.5703)  loss_vfl_dn_4: 0.5508 (0.5834)  loss_vfl_dn_5: 0.5684 (0.5882)  time: 0.2562  data: 0.0093  max mem: 6602
Epoch: [0]  [ 3200/14786]  eta: 0:50:57  lr: 0.000010  loss: 28.0280 (31.4852)  loss_bbox: 0.3178 (0.4429)  loss_bbox_aux_0: 0.3397 (0.5059)  loss_bbox_aux_1: 0.3282 (0.4696)  loss_bbox_aux_2: 0.3238 (0.4550)  loss_bbox_aux_3: 0.3169 (0.4488)  loss_bbox_aux_4: 0.3103 (0.4460)  loss_bbox_aux_5: 0.4295 (0.6087)  loss_bbox_dn_0: 0.6537 (0.7112)  loss_bbox_dn_1: 0.5743 (0.6638)  loss_bbox_dn_2: 0.5653 (0.6490)  loss_bbox_dn_3: 0.5644 (0.6454)  loss_bbox_dn_4: 0.5675 (0.6445)  loss_bbox_dn_5: 0.5676 (0.6446)  loss_giou: 0.7600 (0.9200)  loss_giou_aux_0: 0.8343 (0.9963)  loss_giou_aux_1: 0.7951 (0.9527)  loss_giou_aux_2: 0.7854 (0.9351)  loss_giou_aux_3: 0.7665 (0.9271)  loss_giou_aux_4: 0.7577 (0.9231)  loss_giou_aux_5: 0.9425 (1.1102)  loss_giou_dn_0: 1.0850 (1.1431)  loss_giou_dn_1: 0.9750 (1.0774)  loss_giou_dn_2: 0.9176 (1.0492)  loss_giou_dn_3: 0.9077 (1.0398)  loss_giou_dn_4: 0.9015 (1.0383)  loss_giou_dn_5: 0.9022 (1.0394)  loss_vfl: 1.1448 (1.1656)  loss_vfl_aux_0: 1.0636 (1.0512)  loss_vfl_aux_1: 1.1173 (1.1001)  loss_vfl_aux_2: 1.0866 (1.1237)  loss_vfl_aux_3: 1.0988 (1.1350)  loss_vfl_aux_4: 1.1370 (1.1469)  loss_vfl_aux_5: 0.9795 (0.9833)  loss_vfl_dn_0: 0.4882 (0.4886)  loss_vfl_dn_1: 0.5126 (0.5175)  loss_vfl_dn_2: 0.5196 (0.5459)  loss_vfl_dn_3: 0.5426 (0.5696)  loss_vfl_dn_4: 0.5570 (0.5828)  loss_vfl_dn_5: 0.5718 (0.5878)  time: 0.2601  data: 0.0086  max mem: 6602
Epoch: [0]  [ 3300/14786]  eta: 0:50:30  lr: 0.000010  loss: 27.6143 (31.3861)  loss_bbox: 0.3091 (0.4392)  loss_bbox_aux_0: 0.3570 (0.5018)  loss_bbox_aux_1: 0.3270 (0.4656)  loss_bbox_aux_2: 0.3148 (0.4513)  loss_bbox_aux_3: 0.3169 (0.4451)  loss_bbox_aux_4: 0.3129 (0.4423)  loss_bbox_aux_5: 0.4212 (0.6044)  loss_bbox_dn_0: 0.5385 (0.7081)  loss_bbox_dn_1: 0.4594 (0.6604)  loss_bbox_dn_2: 0.4395 (0.6455)  loss_bbox_dn_3: 0.4300 (0.6418)  loss_bbox_dn_4: 0.4284 (0.6410)  loss_bbox_dn_5: 0.4282 (0.6410)  loss_giou: 0.7714 (0.9163)  loss_giou_aux_0: 0.8348 (0.9926)  loss_giou_aux_1: 0.8138 (0.9490)  loss_giou_aux_2: 0.7905 (0.9315)  loss_giou_aux_3: 0.7866 (0.9234)  loss_giou_aux_4: 0.7794 (0.9195)  loss_giou_aux_5: 0.9807 (1.1066)  loss_giou_dn_0: 1.0771 (1.1405)  loss_giou_dn_1: 0.9697 (1.0740)  loss_giou_dn_2: 0.9333 (1.0456)  loss_giou_dn_3: 0.9163 (1.0361)  loss_giou_dn_4: 0.9138 (1.0345)  loss_giou_dn_5: 0.9140 (1.0356)  loss_vfl: 1.0923 (1.1645)  loss_vfl_aux_0: 1.0733 (1.0518)  loss_vfl_aux_1: 1.1161 (1.1003)  loss_vfl_aux_2: 1.1270 (1.1234)  loss_vfl_aux_3: 1.0971 (1.1343)  loss_vfl_aux_4: 1.0678 (1.1460)  loss_vfl_aux_5: 0.9767 (0.9850)  loss_vfl_dn_0: 0.4772 (0.4885)  loss_vfl_dn_1: 0.5035 (0.5170)  loss_vfl_dn_2: 0.5263 (0.5450)  loss_vfl_dn_3: 0.5394 (0.5686)  loss_vfl_dn_4: 0.5505 (0.5819)  loss_vfl_dn_5: 0.5523 (0.5871)  time: 0.2626  data: 0.0082  max mem: 6602
Epoch: [0]  [ 3400/14786]  eta: 0:50:02  lr: 0.000010  loss: 28.1451 (31.2898)  loss_bbox: 0.3431 (0.4360)  loss_bbox_aux_0: 0.3974 (0.4981)  loss_bbox_aux_1: 0.3777 (0.4620)  loss_bbox_aux_2: 0.3642 (0.4478)  loss_bbox_aux_3: 0.3555 (0.4418)  loss_bbox_aux_4: 0.3447 (0.4390)  loss_bbox_aux_5: 0.5068 (0.6005)  loss_bbox_dn_0: 0.5631 (0.7053)  loss_bbox_dn_1: 0.5003 (0.6571)  loss_bbox_dn_2: 0.4923 (0.6420)  loss_bbox_dn_3: 0.4964 (0.6383)  loss_bbox_dn_4: 0.4995 (0.6375)  loss_bbox_dn_5: 0.4994 (0.6376)  loss_giou: 0.7473 (0.9125)  loss_giou_aux_0: 0.8406 (0.9885)  loss_giou_aux_1: 0.7826 (0.9451)  loss_giou_aux_2: 0.7708 (0.9276)  loss_giou_aux_3: 0.7620 (0.9196)  loss_giou_aux_4: 0.7462 (0.9157)  loss_giou_aux_5: 0.9798 (1.1028)  loss_giou_dn_0: 1.0457 (1.1377)  loss_giou_dn_1: 0.9462 (1.0705)  loss_giou_dn_2: 0.9183 (1.0418)  loss_giou_dn_3: 0.9032 (1.0322)  loss_giou_dn_4: 0.9010 (1.0305)  loss_giou_dn_5: 0.9015 (1.0316)  loss_vfl: 1.1316 (1.1636)  loss_vfl_aux_0: 1.1016 (1.0525)  loss_vfl_aux_1: 1.1212 (1.1007)  loss_vfl_aux_2: 1.1129 (1.1233)  loss_vfl_aux_3: 1.1257 (1.1339)  loss_vfl_aux_4: 1.1261 (1.1453)  loss_vfl_aux_5: 0.9978 (0.9862)  loss_vfl_dn_0: 0.4797 (0.4884)  loss_vfl_dn_1: 0.4941 (0.5167)  loss_vfl_dn_2: 0.5217 (0.5443)  loss_vfl_dn_3: 0.5438 (0.5679)  loss_vfl_dn_4: 0.5604 (0.5814)  loss_vfl_dn_5: 0.5701 (0.5869)  time: 0.2561  data: 0.0082  max mem: 6602
Epoch: [0]  [ 3500/14786]  eta: 0:49:34  lr: 0.000010  loss: 27.6554 (31.2061)  loss_bbox: 0.3419 (0.4330)  loss_bbox_aux_0: 0.3801 (0.4948)  loss_bbox_aux_1: 0.3625 (0.4588)  loss_bbox_aux_2: 0.3523 (0.4447)  loss_bbox_aux_3: 0.3472 (0.4388)  loss_bbox_aux_4: 0.3450 (0.4360)  loss_bbox_aux_5: 0.4571 (0.5972)  loss_bbox_dn_0: 0.4890 (0.7029)  loss_bbox_dn_1: 0.4522 (0.6545)  loss_bbox_dn_2: 0.4460 (0.6395)  loss_bbox_dn_3: 0.4464 (0.6357)  loss_bbox_dn_4: 0.4475 (0.6348)  loss_bbox_dn_5: 0.4476 (0.6349)  loss_giou: 0.9036 (0.9092)  loss_giou_aux_0: 0.9259 (0.9850)  loss_giou_aux_1: 0.9004 (0.9417)  loss_giou_aux_2: 0.8975 (0.9242)  loss_giou_aux_3: 0.9089 (0.9163)  loss_giou_aux_4: 0.9016 (0.9123)  loss_giou_aux_5: 1.0782 (1.0994)  loss_giou_dn_0: 1.0876 (1.1355)  loss_giou_dn_1: 1.0030 (1.0675)  loss_giou_dn_2: 0.9843 (1.0387)  loss_giou_dn_3: 0.9778 (1.0289)  loss_giou_dn_4: 0.9788 (1.0272)  loss_giou_dn_5: 0.9795 (1.0283)  loss_vfl: 0.9234 (1.1626)  loss_vfl_aux_0: 0.9004 (1.0528)  loss_vfl_aux_1: 0.9147 (1.1007)  loss_vfl_aux_2: 0.9032 (1.1228)  loss_vfl_aux_3: 0.9253 (1.1333)  loss_vfl_aux_4: 0.9269 (1.1445)  loss_vfl_aux_5: 0.8785 (0.9870)  loss_vfl_dn_0: 0.4656 (0.4883)  loss_vfl_dn_1: 0.4833 (0.5163)  loss_vfl_dn_2: 0.4905 (0.5436)  loss_vfl_dn_3: 0.5110 (0.5673)  loss_vfl_dn_4: 0.5338 (0.5808)  loss_vfl_dn_5: 0.5403 (0.5864)  time: 0.2585  data: 0.0073  max mem: 6602
Epoch: [0]  [ 3600/14786]  eta: 0:49:06  lr: 0.000010  loss: 27.4071 (31.1193)  loss_bbox: 0.2968 (0.4296)  loss_bbox_aux_0: 0.3022 (0.4909)  loss_bbox_aux_1: 0.2634 (0.4551)  loss_bbox_aux_2: 0.2671 (0.4413)  loss_bbox_aux_3: 0.2935 (0.4354)  loss_bbox_aux_4: 0.2818 (0.4326)  loss_bbox_aux_5: 0.4205 (0.5933)  loss_bbox_dn_0: 0.6242 (0.7007)  loss_bbox_dn_1: 0.5504 (0.6518)  loss_bbox_dn_2: 0.5264 (0.6366)  loss_bbox_dn_3: 0.5090 (0.6328)  loss_bbox_dn_4: 0.5080 (0.6320)  loss_bbox_dn_5: 0.5080 (0.6321)  loss_giou: 0.7408 (0.9052)  loss_giou_aux_0: 0.8106 (0.9807)  loss_giou_aux_1: 0.7727 (0.9374)  loss_giou_aux_2: 0.7515 (0.9201)  loss_giou_aux_3: 0.7619 (0.9122)  loss_giou_aux_4: 0.7548 (0.9083)  loss_giou_aux_5: 0.9501 (1.0953)  loss_giou_dn_0: 1.0498 (1.1330)  loss_giou_dn_1: 0.9611 (1.0643)  loss_giou_dn_2: 0.9179 (1.0352)  loss_giou_dn_3: 0.9126 (1.0253)  loss_giou_dn_4: 0.9080 (1.0236)  loss_giou_dn_5: 0.9081 (1.0246)  loss_vfl: 1.1639 (1.1624)  loss_vfl_aux_0: 1.1313 (1.0546)  loss_vfl_aux_1: 1.1327 (1.1020)  loss_vfl_aux_2: 1.1703 (1.1235)  loss_vfl_aux_3: 1.1365 (1.1334)  loss_vfl_aux_4: 1.1678 (1.1444)  loss_vfl_aux_5: 1.0648 (0.9892)  loss_vfl_dn_0: 0.4877 (0.4883)  loss_vfl_dn_1: 0.5001 (0.5160)  loss_vfl_dn_2: 0.5218 (0.5430)  loss_vfl_dn_3: 0.5306 (0.5666)  loss_vfl_dn_4: 0.5510 (0.5803)  loss_vfl_dn_5: 0.5666 (0.5862)  time: 0.2583  data: 0.0093  max mem: 6602
Epoch: [0]  [ 3700/14786]  eta: 0:48:39  lr: 0.000010  loss: 26.8478 (31.0320)  loss_bbox: 0.2660 (0.4263)  loss_bbox_aux_0: 0.3190 (0.4873)  loss_bbox_aux_1: 0.2725 (0.4516)  loss_bbox_aux_2: 0.2744 (0.4379)  loss_bbox_aux_3: 0.2681 (0.4320)  loss_bbox_aux_4: 0.2642 (0.4293)  loss_bbox_aux_5: 0.4046 (0.5897)  loss_bbox_dn_0: 0.5396 (0.6983)  loss_bbox_dn_1: 0.4776 (0.6490)  loss_bbox_dn_2: 0.4602 (0.6337)  loss_bbox_dn_3: 0.4618 (0.6298)  loss_bbox_dn_4: 0.4632 (0.6290)  loss_bbox_dn_5: 0.4633 (0.6291)  loss_giou: 0.6894 (0.9007)  loss_giou_aux_0: 0.7557 (0.9761)  loss_giou_aux_1: 0.7197 (0.9329)  loss_giou_aux_2: 0.6975 (0.9156)  loss_giou_aux_3: 0.6832 (0.9077)  loss_giou_aux_4: 0.6888 (0.9039)  loss_giou_aux_5: 0.9105 (1.0912)  loss_giou_dn_0: 1.0165 (1.1303)  loss_giou_dn_1: 0.9062 (1.0607)  loss_giou_dn_2: 0.8598 (1.0314)  loss_giou_dn_3: 0.8452 (1.0214)  loss_giou_dn_4: 0.8419 (1.0196)  loss_giou_dn_5: 0.8423 (1.0206)  loss_vfl: 1.1565 (1.1629)  loss_vfl_aux_0: 1.1570 (1.0563)  loss_vfl_aux_1: 1.1919 (1.1035)  loss_vfl_aux_2: 1.1861 (1.1246)  loss_vfl_aux_3: 1.1723 (1.1345)  loss_vfl_aux_4: 1.2027 (1.1452)  loss_vfl_aux_5: 1.0746 (0.9913)  loss_vfl_dn_0: 0.4982 (0.4884)  loss_vfl_dn_1: 0.5225 (0.5158)  loss_vfl_dn_2: 0.5438 (0.5425)  loss_vfl_dn_3: 0.5750 (0.5660)  loss_vfl_dn_4: 0.5903 (0.5799)  loss_vfl_dn_5: 0.6159 (0.5860)  time: 0.2610  data: 0.0090  max mem: 6602
Epoch: [0]  [ 3800/14786]  eta: 0:48:20  lr: 0.000010  loss: 27.0002 (30.9508)  loss_bbox: 0.2740 (0.4234)  loss_bbox_aux_0: 0.3161 (0.4841)  loss_bbox_aux_1: 0.2880 (0.4484)  loss_bbox_aux_2: 0.2802 (0.4348)  loss_bbox_aux_3: 0.2747 (0.4290)  loss_bbox_aux_4: 0.2750 (0.4262)  loss_bbox_aux_5: 0.4293 (0.5864)  loss_bbox_dn_0: 0.5599 (0.6963)  loss_bbox_dn_1: 0.4950 (0.6466)  loss_bbox_dn_2: 0.4683 (0.6312)  loss_bbox_dn_3: 0.4655 (0.6274)  loss_bbox_dn_4: 0.4659 (0.6265)  loss_bbox_dn_5: 0.4658 (0.6266)  loss_giou: 0.7185 (0.8975)  loss_giou_aux_0: 0.7983 (0.9727)  loss_giou_aux_1: 0.7431 (0.9295)  loss_giou_aux_2: 0.7339 (0.9123)  loss_giou_aux_3: 0.7212 (0.9044)  loss_giou_aux_4: 0.7174 (0.9005)  loss_giou_aux_5: 0.9197 (1.0880)  loss_giou_dn_0: 1.0535 (1.1281)  loss_giou_dn_1: 0.9592 (1.0579)  loss_giou_dn_2: 0.9302 (1.0284)  loss_giou_dn_3: 0.9169 (1.0183)  loss_giou_dn_4: 0.9137 (1.0165)  loss_giou_dn_5: 0.9137 (1.0175)  loss_vfl: 1.0848 (1.1619)  loss_vfl_aux_0: 1.0504 (1.0567)  loss_vfl_aux_1: 1.0931 (1.1038)  loss_vfl_aux_2: 1.0566 (1.1244)  loss_vfl_aux_3: 1.0287 (1.1339)  loss_vfl_aux_4: 1.0607 (1.1444)  loss_vfl_aux_5: 0.9474 (0.9920)  loss_vfl_dn_0: 0.4791 (0.4883)  loss_vfl_dn_1: 0.4971 (0.5155)  loss_vfl_dn_2: 0.5062 (0.5417)  loss_vfl_dn_3: 0.5147 (0.5651)  loss_vfl_dn_4: 0.5304 (0.5791)  loss_vfl_dn_5: 0.5380 (0.5854)  time: 0.3244  data: 0.0096  max mem: 6603
Epoch: [0]  [ 3900/14786]  eta: 0:47:51  lr: 0.000010  loss: 26.3703 (30.8615)  loss_bbox: 0.2791 (0.4199)  loss_bbox_aux_0: 0.3132 (0.4804)  loss_bbox_aux_1: 0.3025 (0.4448)  loss_bbox_aux_2: 0.2863 (0.4313)  loss_bbox_aux_3: 0.2871 (0.4256)  loss_bbox_aux_4: 0.2930 (0.4228)  loss_bbox_aux_5: 0.4332 (0.5826)  loss_bbox_dn_0: 0.5354 (0.6939)  loss_bbox_dn_1: 0.4767 (0.6437)  loss_bbox_dn_2: 0.4488 (0.6283)  loss_bbox_dn_3: 0.4457 (0.6243)  loss_bbox_dn_4: 0.4474 (0.6235)  loss_bbox_dn_5: 0.4474 (0.6235)  loss_giou: 0.7739 (0.8935)  loss_giou_aux_0: 0.8079 (0.9685)  loss_giou_aux_1: 0.7720 (0.9254)  loss_giou_aux_2: 0.7708 (0.9083)  loss_giou_aux_3: 0.7793 (0.9004)  loss_giou_aux_4: 0.7798 (0.8966)  loss_giou_aux_5: 0.9155 (1.0840)  loss_giou_dn_0: 1.0206 (1.1257)  loss_giou_dn_1: 0.9221 (1.0547)  loss_giou_dn_2: 0.8900 (1.0250)  loss_giou_dn_3: 0.8756 (1.0147)  loss_giou_dn_4: 0.8762 (1.0129)  loss_giou_dn_5: 0.8756 (1.0139)  loss_vfl: 1.0295 (1.1614)  loss_vfl_aux_0: 0.9807 (1.0580)  loss_vfl_aux_1: 1.0184 (1.1047)  loss_vfl_aux_2: 0.9748 (1.1245)  loss_vfl_aux_3: 0.9934 (1.1338)  loss_vfl_aux_4: 1.0017 (1.1441)  loss_vfl_aux_5: 0.9828 (0.9938)  loss_vfl_dn_0: 0.4843 (0.4884)  loss_vfl_dn_1: 0.4998 (0.5153)  loss_vfl_dn_2: 0.5097 (0.5412)  loss_vfl_dn_3: 0.5286 (0.5646)  loss_vfl_dn_4: 0.5425 (0.5786)  loss_vfl_dn_5: 0.5530 (0.5851)  time: 0.2475  data: 0.0085  max mem: 6603
Epoch: [0]  [ 4000/14786]  eta: 0:47:25  lr: 0.000010  loss: 26.1127 (30.7782)  loss_bbox: 0.2584 (0.4171)  loss_bbox_aux_0: 0.2904 (0.4771)  loss_bbox_aux_1: 0.2664 (0.4416)  loss_bbox_aux_2: 0.2606 (0.4284)  loss_bbox_aux_3: 0.2648 (0.4227)  loss_bbox_aux_4: 0.2554 (0.4199)  loss_bbox_aux_5: 0.3875 (0.5793)  loss_bbox_dn_0: 0.5226 (0.6911)  loss_bbox_dn_1: 0.4481 (0.6407)  loss_bbox_dn_2: 0.4258 (0.6252)  loss_bbox_dn_3: 0.4266 (0.6212)  loss_bbox_dn_4: 0.4274 (0.6204)  loss_bbox_dn_5: 0.4276 (0.6204)  loss_giou: 0.7341 (0.8905)  loss_giou_aux_0: 0.8170 (0.9653)  loss_giou_aux_1: 0.7775 (0.9222)  loss_giou_aux_2: 0.7659 (0.9053)  loss_giou_aux_3: 0.7425 (0.8974)  loss_giou_aux_4: 0.7488 (0.8936)  loss_giou_aux_5: 0.9478 (1.0812)  loss_giou_dn_0: 1.0109 (1.1236)  loss_giou_dn_1: 0.9127 (1.0520)  loss_giou_dn_2: 0.8684 (1.0221)  loss_giou_dn_3: 0.8520 (1.0117)  loss_giou_dn_4: 0.8512 (1.0099)  loss_giou_dn_5: 0.8513 (1.0109)  loss_vfl: 1.1079 (1.1604)  loss_vfl_aux_0: 1.0433 (1.0582)  loss_vfl_aux_1: 1.1189 (1.1048)  loss_vfl_aux_2: 1.0846 (1.1241)  loss_vfl_aux_3: 1.0849 (1.1333)  loss_vfl_aux_4: 1.0888 (1.1433)  loss_vfl_aux_5: 0.9656 (0.9939)  loss_vfl_dn_0: 0.4876 (0.4883)  loss_vfl_dn_1: 0.5018 (0.5149)  loss_vfl_dn_2: 0.5071 (0.5405)  loss_vfl_dn_3: 0.5205 (0.5636)  loss_vfl_dn_4: 0.5443 (0.5778)  loss_vfl_dn_5: 0.5625 (0.5844)  time: 0.2694  data: 0.0081  max mem: 6603
Epoch: [0]  [ 4100/14786]  eta: 0:46:58  lr: 0.000010  loss: 26.9916 (30.7157)  loss_bbox: 0.2889 (0.4153)  loss_bbox_aux_0: 0.3566 (0.4749)  loss_bbox_aux_1: 0.3127 (0.4396)  loss_bbox_aux_2: 0.3046 (0.4264)  loss_bbox_aux_3: 0.2965 (0.4208)  loss_bbox_aux_4: 0.2934 (0.4182)  loss_bbox_aux_5: 0.4546 (0.5771)  loss_bbox_dn_0: 0.5516 (0.6897)  loss_bbox_dn_1: 0.4779 (0.6390)  loss_bbox_dn_2: 0.4524 (0.6233)  loss_bbox_dn_3: 0.4493 (0.6193)  loss_bbox_dn_4: 0.4451 (0.6185)  loss_bbox_dn_5: 0.4448 (0.6185)  loss_giou: 0.7177 (0.8877)  loss_giou_aux_0: 0.7775 (0.9623)  loss_giou_aux_1: 0.7592 (0.9194)  loss_giou_aux_2: 0.7393 (0.9025)  loss_giou_aux_3: 0.7423 (0.8946)  loss_giou_aux_4: 0.7369 (0.8908)  loss_giou_aux_5: 0.9067 (1.0783)  loss_giou_dn_0: 1.0116 (1.1214)  loss_giou_dn_1: 0.9043 (1.0493)  loss_giou_dn_2: 0.8655 (1.0192)  loss_giou_dn_3: 0.8531 (1.0087)  loss_giou_dn_4: 0.8512 (1.0068)  loss_giou_dn_5: 0.8509 (1.0078)  loss_vfl: 1.0745 (1.1599)  loss_vfl_aux_0: 1.0970 (1.0592)  loss_vfl_aux_1: 1.1311 (1.1055)  loss_vfl_aux_2: 1.0852 (1.1241)  loss_vfl_aux_3: 1.0359 (1.1329)  loss_vfl_aux_4: 1.0853 (1.1427)  loss_vfl_aux_5: 1.0387 (0.9948)  loss_vfl_dn_0: 0.4869 (0.4883)  loss_vfl_dn_1: 0.5011 (0.5146)  loss_vfl_dn_2: 0.5119 (0.5399)  loss_vfl_dn_3: 0.5312 (0.5630)  loss_vfl_dn_4: 0.5505 (0.5773)  loss_vfl_dn_5: 0.5693 (0.5840)  time: 0.2668  data: 0.0105  max mem: 6603
Epoch: [0]  [ 4200/14786]  eta: 0:46:32  lr: 0.000010  loss: 27.2678 (30.6401)  loss_bbox: 0.3054 (0.4127)  loss_bbox_aux_0: 0.3455 (0.4719)  loss_bbox_aux_1: 0.3308 (0.4367)  loss_bbox_aux_2: 0.3068 (0.4237)  loss_bbox_aux_3: 0.3122 (0.4181)  loss_bbox_aux_4: 0.3078 (0.4155)  loss_bbox_aux_5: 0.4175 (0.5742)  loss_bbox_dn_0: 0.4971 (0.6876)  loss_bbox_dn_1: 0.4632 (0.6365)  loss_bbox_dn_2: 0.4561 (0.6207)  loss_bbox_dn_3: 0.4550 (0.6167)  loss_bbox_dn_4: 0.4564 (0.6158)  loss_bbox_dn_5: 0.4562 (0.6158)  loss_giou: 0.8182 (0.8850)  loss_giou_aux_0: 0.8927 (0.9594)  loss_giou_aux_1: 0.8384 (0.9165)  loss_giou_aux_2: 0.8353 (0.8998)  loss_giou_aux_3: 0.8191 (0.8918)  loss_giou_aux_4: 0.8244 (0.8881)  loss_giou_aux_5: 0.9960 (1.0755)  loss_giou_dn_0: 1.0327 (1.1194)  loss_giou_dn_1: 0.9266 (1.0466)  loss_giou_dn_2: 0.8953 (1.0163)  loss_giou_dn_3: 0.8841 (1.0057)  loss_giou_dn_4: 0.8820 (1.0039)  loss_giou_dn_5: 0.8819 (1.0048)  loss_vfl: 0.9895 (1.1587)  loss_vfl_aux_0: 0.9594 (1.0595)  loss_vfl_aux_1: 1.0004 (1.1056)  loss_vfl_aux_2: 0.9516 (1.1237)  loss_vfl_aux_3: 0.9975 (1.1324)  loss_vfl_aux_4: 1.0053 (1.1418)  loss_vfl_aux_5: 0.8978 (0.9956)  loss_vfl_dn_0: 0.4753 (0.4883)  loss_vfl_dn_1: 0.5026 (0.5144)  loss_vfl_dn_2: 0.5050 (0.5393)  loss_vfl_dn_3: 0.5182 (0.5623)  loss_vfl_dn_4: 0.5374 (0.5766)  loss_vfl_dn_5: 0.5503 (0.5835)  time: 0.2594  data: 0.0090  max mem: 6603
Epoch: [0]  [ 4300/14786]  eta: 0:46:06  lr: 0.000010  loss: 26.5582 (30.5770)  loss_bbox: 0.2780 (0.4106)  loss_bbox_aux_0: 0.3046 (0.4693)  loss_bbox_aux_1: 0.2827 (0.4343)  loss_bbox_aux_2: 0.2881 (0.4213)  loss_bbox_aux_3: 0.2859 (0.4159)  loss_bbox_aux_4: 0.2815 (0.4134)  loss_bbox_aux_5: 0.4152 (0.5717)  loss_bbox_dn_0: 0.5294 (0.6867)  loss_bbox_dn_1: 0.4501 (0.6353)  loss_bbox_dn_2: 0.4087 (0.6193)  loss_bbox_dn_3: 0.4043 (0.6152)  loss_bbox_dn_4: 0.4070 (0.6144)  loss_bbox_dn_5: 0.4073 (0.6144)  loss_giou: 0.7144 (0.8815)  loss_giou_aux_0: 0.7675 (0.9556)  loss_giou_aux_1: 0.7410 (0.9129)  loss_giou_aux_2: 0.7150 (0.8962)  loss_giou_aux_3: 0.7148 (0.8883)  loss_giou_aux_4: 0.7128 (0.8845)  loss_giou_aux_5: 0.9034 (1.0722)  loss_giou_dn_0: 1.0121 (1.1171)  loss_giou_dn_1: 0.9017 (1.0437)  loss_giou_dn_2: 0.8619 (1.0132)  loss_giou_dn_3: 0.8486 (1.0025)  loss_giou_dn_4: 0.8460 (1.0006)  loss_giou_dn_5: 0.8474 (1.0015)  loss_vfl: 1.0648 (1.1585)  loss_vfl_aux_0: 1.0425 (1.0611)  loss_vfl_aux_1: 1.0597 (1.1068)  loss_vfl_aux_2: 1.0703 (1.1247)  loss_vfl_aux_3: 1.0793 (1.1328)  loss_vfl_aux_4: 1.0425 (1.1418)  loss_vfl_aux_5: 1.0526 (0.9973)  loss_vfl_dn_0: 0.4897 (0.4884)  loss_vfl_dn_1: 0.5012 (0.5142)  loss_vfl_dn_2: 0.5230 (0.5388)  loss_vfl_dn_3: 0.5270 (0.5617)  loss_vfl_dn_4: 0.5524 (0.5761)  loss_vfl_dn_5: 0.5680 (0.5832)  time: 0.2665  data: 0.0086  max mem: 6603
Epoch: [0]  [ 4400/14786]  eta: 0:45:40  lr: 0.000010  loss: 27.2322 (30.5047)  loss_bbox: 0.3038 (0.4082)  loss_bbox_aux_0: 0.3375 (0.4665)  loss_bbox_aux_1: 0.3193 (0.4316)  loss_bbox_aux_2: 0.3259 (0.4189)  loss_bbox_aux_3: 0.2976 (0.4135)  loss_bbox_aux_4: 0.2929 (0.4110)  loss_bbox_aux_5: 0.4392 (0.5689)  loss_bbox_dn_0: 0.6079 (0.6848)  loss_bbox_dn_1: 0.5706 (0.6330)  loss_bbox_dn_2: 0.5473 (0.6170)  loss_bbox_dn_3: 0.5471 (0.6129)  loss_bbox_dn_4: 0.5480 (0.6120)  loss_bbox_dn_5: 0.5481 (0.6121)  loss_giou: 0.7123 (0.8786)  loss_giou_aux_0: 0.7711 (0.9523)  loss_giou_aux_1: 0.7321 (0.9097)  loss_giou_aux_2: 0.7245 (0.8932)  loss_giou_aux_3: 0.7212 (0.8853)  loss_giou_aux_4: 0.7193 (0.8816)  loss_giou_aux_5: 0.8944 (1.0692)  loss_giou_dn_0: 1.0009 (1.1150)  loss_giou_dn_1: 0.8960 (1.0410)  loss_giou_dn_2: 0.8538 (1.0103)  loss_giou_dn_3: 0.8379 (0.9995)  loss_giou_dn_4: 0.8325 (0.9976)  loss_giou_dn_5: 0.8325 (0.9985)  loss_vfl: 1.1069 (1.1574)  loss_vfl_aux_0: 1.0442 (1.0621)  loss_vfl_aux_1: 1.0282 (1.1074)  loss_vfl_aux_2: 1.0444 (1.1244)  loss_vfl_aux_3: 1.0488 (1.1322)  loss_vfl_aux_4: 1.0825 (1.1409)  loss_vfl_aux_5: 1.0015 (0.9981)  loss_vfl_dn_0: 0.4929 (0.4885)  loss_vfl_dn_1: 0.5116 (0.5140)  loss_vfl_dn_2: 0.5238 (0.5383)  loss_vfl_dn_3: 0.5265 (0.5610)  loss_vfl_dn_4: 0.5402 (0.5755)  loss_vfl_dn_5: 0.5526 (0.5828)  time: 0.2680  data: 0.0082  max mem: 6603
Epoch: [0]  [ 4500/14786]  eta: 0:45:14  lr: 0.000010  loss: 28.0677 (30.4436)  loss_bbox: 0.3310 (0.4063)  loss_bbox_aux_0: 0.3679 (0.4643)  loss_bbox_aux_1: 0.3242 (0.4295)  loss_bbox_aux_2: 0.3066 (0.4169)  loss_bbox_aux_3: 0.3339 (0.4116)  loss_bbox_aux_4: 0.3321 (0.4092)  loss_bbox_aux_5: 0.4613 (0.5666)  loss_bbox_dn_0: 0.6409 (0.6836)  loss_bbox_dn_1: 0.5781 (0.6314)  loss_bbox_dn_2: 0.5490 (0.6152)  loss_bbox_dn_3: 0.5496 (0.6110)  loss_bbox_dn_4: 0.5518 (0.6101)  loss_bbox_dn_5: 0.5525 (0.6102)  loss_giou: 0.6187 (0.8753)  loss_giou_aux_0: 0.7152 (0.9488)  loss_giou_aux_1: 0.6677 (0.9063)  loss_giou_aux_2: 0.6385 (0.8899)  loss_giou_aux_3: 0.6212 (0.8820)  loss_giou_aux_4: 0.6210 (0.8783)  loss_giou_aux_5: 0.8973 (1.0661)  loss_giou_dn_0: 0.9753 (1.1127)  loss_giou_dn_1: 0.8634 (1.0380)  loss_giou_dn_2: 0.8328 (1.0071)  loss_giou_dn_3: 0.8158 (0.9963)  loss_giou_dn_4: 0.8144 (0.9943)  loss_giou_dn_5: 0.8146 (0.9952)  loss_vfl: 1.2566 (1.1575)  loss_vfl_aux_0: 1.1708 (1.0636)  loss_vfl_aux_1: 1.2278 (1.1086)  loss_vfl_aux_2: 1.2143 (1.1251)  loss_vfl_aux_3: 1.2659 (1.1325)  loss_vfl_aux_4: 1.2510 (1.1410)  loss_vfl_aux_5: 1.1687 (0.9998)  loss_vfl_dn_0: 0.5154 (0.4887)  loss_vfl_dn_1: 0.5359 (0.5140)  loss_vfl_dn_2: 0.5436 (0.5380)  loss_vfl_dn_3: 0.5687 (0.5607)  loss_vfl_dn_4: 0.5834 (0.5752)  loss_vfl_dn_5: 0.6046 (0.5827)  time: 0.2709  data: 0.0094  max mem: 6603
Epoch: [0]  [ 4600/14786]  eta: 0:44:48  lr: 0.000010  loss: 26.4791 (30.3710)  loss_bbox: 0.2647 (0.4038)  loss_bbox_aux_0: 0.3195 (0.4614)  loss_bbox_aux_1: 0.2773 (0.4267)  loss_bbox_aux_2: 0.2821 (0.4141)  loss_bbox_aux_3: 0.2651 (0.4090)  loss_bbox_aux_4: 0.2723 (0.4066)  loss_bbox_aux_5: 0.3928 (0.5636)  loss_bbox_dn_0: 0.5664 (0.6819)  loss_bbox_dn_1: 0.5054 (0.6292)  loss_bbox_dn_2: 0.4736 (0.6129)  loss_bbox_dn_3: 0.4614 (0.6086)  loss_bbox_dn_4: 0.4617 (0.6077)  loss_bbox_dn_5: 0.4612 (0.6078)  loss_giou: 0.7133 (0.8724)  loss_giou_aux_0: 0.7812 (0.9457)  loss_giou_aux_1: 0.7306 (0.9033)  loss_giou_aux_2: 0.7261 (0.8869)  loss_giou_aux_3: 0.7145 (0.8791)  loss_giou_aux_4: 0.7107 (0.8755)  loss_giou_aux_5: 0.8804 (1.0631)  loss_giou_dn_0: 1.0159 (1.1107)  loss_giou_dn_1: 0.8917 (1.0354)  loss_giou_dn_2: 0.8433 (1.0044)  loss_giou_dn_3: 0.8318 (0.9934)  loss_giou_dn_4: 0.8319 (0.9914)  loss_giou_dn_5: 0.8322 (0.9923)  loss_vfl: 1.0250 (1.1565)  loss_vfl_aux_0: 1.0567 (1.0644)  loss_vfl_aux_1: 1.1007 (1.1089)  loss_vfl_aux_2: 1.0249 (1.1248)  loss_vfl_aux_3: 1.0101 (1.1318)  loss_vfl_aux_4: 1.0360 (1.1401)  loss_vfl_aux_5: 0.9940 (1.0010)  loss_vfl_dn_0: 0.4893 (0.4887)  loss_vfl_dn_1: 0.5178 (0.5138)  loss_vfl_dn_2: 0.5107 (0.5375)  loss_vfl_dn_3: 0.5173 (0.5600)  loss_vfl_dn_4: 0.5457 (0.5745)  loss_vfl_dn_5: 0.5578 (0.5822)  time: 0.2609  data: 0.0088  max mem: 6603
Epoch: [0]  [ 4700/14786]  eta: 0:44:22  lr: 0.000010  loss: 27.0657 (30.3100)  loss_bbox: 0.2992 (0.4019)  loss_bbox_aux_0: 0.3610 (0.4593)  loss_bbox_aux_1: 0.3197 (0.4247)  loss_bbox_aux_2: 0.3122 (0.4122)  loss_bbox_aux_3: 0.3086 (0.4071)  loss_bbox_aux_4: 0.2943 (0.4047)  loss_bbox_aux_5: 0.4278 (0.5615)  loss_bbox_dn_0: 0.5914 (0.6804)  loss_bbox_dn_1: 0.4996 (0.6274)  loss_bbox_dn_2: 0.4739 (0.6111)  loss_bbox_dn_3: 0.4665 (0.6068)  loss_bbox_dn_4: 0.4645 (0.6059)  loss_bbox_dn_5: 0.4649 (0.6059)  loss_giou: 0.7365 (0.8694)  loss_giou_aux_0: 0.8072 (0.9426)  loss_giou_aux_1: 0.7749 (0.9002)  loss_giou_aux_2: 0.7332 (0.8838)  loss_giou_aux_3: 0.7206 (0.8760)  loss_giou_aux_4: 0.7345 (0.8724)  loss_giou_aux_5: 0.9149 (1.0604)  loss_giou_dn_0: 1.0139 (1.1086)  loss_giou_dn_1: 0.9147 (1.0327)  loss_giou_dn_2: 0.8708 (1.0015)  loss_giou_dn_3: 0.8557 (0.9905)  loss_giou_dn_4: 0.8548 (0.9884)  loss_giou_dn_5: 0.8552 (0.9893)  loss_vfl: 1.0385 (1.1562)  loss_vfl_aux_0: 1.0802 (1.0655)  loss_vfl_aux_1: 1.0711 (1.1099)  loss_vfl_aux_2: 1.0593 (1.1252)  loss_vfl_aux_3: 1.0481 (1.1321)  loss_vfl_aux_4: 1.0815 (1.1400)  loss_vfl_aux_5: 0.9566 (1.0019)  loss_vfl_dn_0: 0.4794 (0.4888)  loss_vfl_dn_1: 0.4922 (0.5136)  loss_vfl_dn_2: 0.5013 (0.5370)  loss_vfl_dn_3: 0.4914 (0.5593)  loss_vfl_dn_4: 0.5069 (0.5739)  loss_vfl_dn_5: 0.5162 (0.5818)  time: 0.2680  data: 0.0094  max mem: 6603
Epoch: [0]  [ 4800/14786]  eta: 0:43:56  lr: 0.000010  loss: 28.0365 (30.2453)  loss_bbox: 0.2827 (0.3999)  loss_bbox_aux_0: 0.3278 (0.4569)  loss_bbox_aux_1: 0.2848 (0.4224)  loss_bbox_aux_2: 0.2942 (0.4101)  loss_bbox_aux_3: 0.2873 (0.4050)  loss_bbox_aux_4: 0.2941 (0.4026)  loss_bbox_aux_5: 0.4442 (0.5590)  loss_bbox_dn_0: 0.6383 (0.6782)  loss_bbox_dn_1: 0.5528 (0.6250)  loss_bbox_dn_2: 0.5253 (0.6086)  loss_bbox_dn_3: 0.5232 (0.6043)  loss_bbox_dn_4: 0.5216 (0.6034)  loss_bbox_dn_5: 0.5217 (0.6034)  loss_giou: 0.7451 (0.8675)  loss_giou_aux_0: 0.7946 (0.9406)  loss_giou_aux_1: 0.7408 (0.8983)  loss_giou_aux_2: 0.7461 (0.8819)  loss_giou_aux_3: 0.7451 (0.8741)  loss_giou_aux_4: 0.7458 (0.8705)  loss_giou_aux_5: 0.9158 (1.0585)  loss_giou_dn_0: 1.0148 (1.1068)  loss_giou_dn_1: 0.8925 (1.0305)  loss_giou_dn_2: 0.8565 (0.9992)  loss_giou_dn_3: 0.8541 (0.9881)  loss_giou_dn_4: 0.8521 (0.9861)  loss_giou_dn_5: 0.8521 (0.9869)  loss_vfl: 1.2062 (1.1550)  loss_vfl_aux_0: 1.1329 (1.0651)  loss_vfl_aux_1: 1.2097 (1.1093)  loss_vfl_aux_2: 1.1626 (1.1245)  loss_vfl_aux_3: 1.2164 (1.1311)  loss_vfl_aux_4: 1.1970 (1.1390)  loss_vfl_aux_5: 1.1042 (1.0017)  loss_vfl_dn_0: 0.4936 (0.4887)  loss_vfl_dn_1: 0.5063 (0.5134)  loss_vfl_dn_2: 0.5265 (0.5364)  loss_vfl_dn_3: 0.5316 (0.5586)  loss_vfl_dn_4: 0.5506 (0.5733)  loss_vfl_dn_5: 0.5693 (0.5813)  time: 0.2650  data: 0.0091  max mem: 6603
Epoch: [0]  [ 4900/14786]  eta: 0:43:30  lr: 0.000010  loss: 26.7632 (30.1811)  loss_bbox: 0.2930 (0.3980)  loss_bbox_aux_0: 0.3362 (0.4547)  loss_bbox_aux_1: 0.3059 (0.4202)  loss_bbox_aux_2: 0.2938 (0.4080)  loss_bbox_aux_3: 0.2942 (0.4030)  loss_bbox_aux_4: 0.2997 (0.4007)  loss_bbox_aux_5: 0.4301 (0.5566)  loss_bbox_dn_0: 0.4754 (0.6762)  loss_bbox_dn_1: 0.4327 (0.6229)  loss_bbox_dn_2: 0.4319 (0.6065)  loss_bbox_dn_3: 0.4339 (0.6022)  loss_bbox_dn_4: 0.4377 (0.6012)  loss_bbox_dn_5: 0.4377 (0.6013)  loss_giou: 0.8585 (0.8652)  loss_giou_aux_0: 0.8821 (0.9382)  loss_giou_aux_1: 0.8623 (0.8959)  loss_giou_aux_2: 0.8575 (0.8796)  loss_giou_aux_3: 0.8560 (0.8718)  loss_giou_aux_4: 0.8598 (0.8682)  loss_giou_aux_5: 1.0240 (1.0562)  loss_giou_dn_0: 1.0213 (1.1050)  loss_giou_dn_1: 0.9363 (1.0282)  loss_giou_dn_2: 0.8997 (0.9968)  loss_giou_dn_3: 0.8882 (0.9856)  loss_giou_dn_4: 0.8873 (0.9836)  loss_giou_dn_5: 0.8870 (0.9844)  loss_vfl: 0.9929 (1.1539)  loss_vfl_aux_0: 0.9716 (1.0651)  loss_vfl_aux_1: 0.9924 (1.1091)  loss_vfl_aux_2: 0.9623 (1.1239)  loss_vfl_aux_3: 0.9971 (1.1303)  loss_vfl_aux_4: 0.9735 (1.1379)  loss_vfl_aux_5: 0.9101 (1.0018)  loss_vfl_dn_0: 0.4720 (0.4887)  loss_vfl_dn_1: 0.4850 (0.5131)  loss_vfl_dn_2: 0.4894 (0.5358)  loss_vfl_dn_3: 0.5004 (0.5579)  loss_vfl_dn_4: 0.5118 (0.5726)  loss_vfl_dn_5: 0.5369 (0.5808)  time: 0.2553  data: 0.0091  max mem: 6603
Epoch: [0]  [ 5000/14786]  eta: 0:43:02  lr: 0.000010  loss: 27.1450 (30.1186)  loss_bbox: 0.2725 (0.3959)  loss_bbox_aux_0: 0.3139 (0.4522)  loss_bbox_aux_1: 0.2849 (0.4179)  loss_bbox_aux_2: 0.2774 (0.4058)  loss_bbox_aux_3: 0.2715 (0.4008)  loss_bbox_aux_4: 0.2750 (0.3986)  loss_bbox_aux_5: 0.3795 (0.5541)  loss_bbox_dn_0: 0.5499 (0.6744)  loss_bbox_dn_1: 0.4848 (0.6208)  loss_bbox_dn_2: 0.4795 (0.6044)  loss_bbox_dn_3: 0.4727 (0.6001)  loss_bbox_dn_4: 0.4746 (0.5991)  loss_bbox_dn_5: 0.4744 (0.5992)  loss_giou: 0.6693 (0.8629)  loss_giou_aux_0: 0.7397 (0.9357)  loss_giou_aux_1: 0.6870 (0.8934)  loss_giou_aux_2: 0.6789 (0.8771)  loss_giou_aux_3: 0.6807 (0.8694)  loss_giou_aux_4: 0.6765 (0.8659)  loss_giou_aux_5: 0.8655 (1.0538)  loss_giou_dn_0: 0.9875 (1.1032)  loss_giou_dn_1: 0.8899 (1.0260)  loss_giou_dn_2: 0.8515 (0.9945)  loss_giou_dn_3: 0.8392 (0.9833)  loss_giou_dn_4: 0.8390 (0.9812)  loss_giou_dn_5: 0.8399 (0.9820)  loss_vfl: 1.1079 (1.1529)  loss_vfl_aux_0: 1.1319 (1.0654)  loss_vfl_aux_1: 1.1835 (1.1093)  loss_vfl_aux_2: 1.2149 (1.1238)  loss_vfl_aux_3: 1.1475 (1.1296)  loss_vfl_aux_4: 1.0925 (1.1371)  loss_vfl_aux_5: 1.1474 (1.0025)  loss_vfl_dn_0: 0.5067 (0.4887)  loss_vfl_dn_1: 0.5173 (0.5129)  loss_vfl_dn_2: 0.5326 (0.5353)  loss_vfl_dn_3: 0.5507 (0.5572)  loss_vfl_dn_4: 0.5697 (0.5719)  loss_vfl_dn_5: 0.5823 (0.5803)  time: 0.2545  data: 0.0090  max mem: 6603
Epoch: [0]  [ 5100/14786]  eta: 0:42:35  lr: 0.000010  loss: 28.0978 (30.0636)  loss_bbox: 0.3412 (0.3943)  loss_bbox_aux_0: 0.4042 (0.4504)  loss_bbox_aux_1: 0.3860 (0.4161)  loss_bbox_aux_2: 0.3625 (0.4040)  loss_bbox_aux_3: 0.3441 (0.3991)  loss_bbox_aux_4: 0.3458 (0.3969)  loss_bbox_aux_5: 0.5114 (0.5523)  loss_bbox_dn_0: 0.6428 (0.6736)  loss_bbox_dn_1: 0.5642 (0.6196)  loss_bbox_dn_2: 0.5472 (0.6030)  loss_bbox_dn_3: 0.5398 (0.5987)  loss_bbox_dn_4: 0.5365 (0.5977)  loss_bbox_dn_5: 0.5366 (0.5978)  loss_giou: 0.6872 (0.8602)  loss_giou_aux_0: 0.7812 (0.9330)  loss_giou_aux_1: 0.7233 (0.8907)  loss_giou_aux_2: 0.7032 (0.8744)  loss_giou_aux_3: 0.6952 (0.8668)  loss_giou_aux_4: 0.6934 (0.8633)  loss_giou_aux_5: 0.9100 (1.0512)  loss_giou_dn_0: 0.9956 (1.1013)  loss_giou_dn_1: 0.9057 (1.0235)  loss_giou_dn_2: 0.8647 (0.9919)  loss_giou_dn_3: 0.8460 (0.9806)  loss_giou_dn_4: 0.8396 (0.9786)  loss_giou_dn_5: 0.8398 (0.9794)  loss_vfl: 1.0780 (1.1520)  loss_vfl_aux_0: 1.1062 (1.0662)  loss_vfl_aux_1: 1.1494 (1.1099)  loss_vfl_aux_2: 1.1424 (1.1239)  loss_vfl_aux_3: 1.1386 (1.1293)  loss_vfl_aux_4: 1.1233 (1.1365)  loss_vfl_aux_5: 1.0498 (1.0035)  loss_vfl_dn_0: 0.4922 (0.4888)  loss_vfl_dn_1: 0.5007 (0.5127)  loss_vfl_dn_2: 0.5095 (0.5348)  loss_vfl_dn_3: 0.5283 (0.5565)  loss_vfl_dn_4: 0.5470 (0.5713)  loss_vfl_dn_5: 0.5602 (0.5798)  time: 0.2602  data: 0.0093  max mem: 6603
Epoch: [0]  [ 5200/14786]  eta: 0:42:09  lr: 0.000010  loss: 26.4030 (30.0053)  loss_bbox: 0.2608 (0.3926)  loss_bbox_aux_0: 0.2938 (0.4483)  loss_bbox_aux_1: 0.2581 (0.4142)  loss_bbox_aux_2: 0.2675 (0.4022)  loss_bbox_aux_3: 0.2606 (0.3973)  loss_bbox_aux_4: 0.2582 (0.3952)  loss_bbox_aux_5: 0.4094 (0.5500)  loss_bbox_dn_0: 0.5254 (0.6720)  loss_bbox_dn_1: 0.4777 (0.6177)  loss_bbox_dn_2: 0.4603 (0.6011)  loss_bbox_dn_3: 0.4546 (0.5967)  loss_bbox_dn_4: 0.4514 (0.5958)  loss_bbox_dn_5: 0.4515 (0.5958)  loss_giou: 0.7740 (0.8580)  loss_giou_aux_0: 0.8178 (0.9305)  loss_giou_aux_1: 0.7856 (0.8883)  loss_giou_aux_2: 0.7780 (0.8722)  loss_giou_aux_3: 0.7799 (0.8645)  loss_giou_aux_4: 0.7661 (0.8610)  loss_giou_aux_5: 0.9181 (1.0486)  loss_giou_dn_0: 1.0243 (1.0996)  loss_giou_dn_1: 0.9178 (1.0214)  loss_giou_dn_2: 0.8924 (0.9897)  loss_giou_dn_3: 0.8703 (0.9783)  loss_giou_dn_4: 0.8656 (0.9763)  loss_giou_dn_5: 0.8651 (0.9771)  loss_vfl: 1.0609 (1.1510)  loss_vfl_aux_0: 1.0216 (1.0667)  loss_vfl_aux_1: 1.0462 (1.1098)  loss_vfl_aux_2: 1.0399 (1.1233)  loss_vfl_aux_3: 1.0520 (1.1285)  loss_vfl_aux_4: 1.0673 (1.1355)  loss_vfl_aux_5: 0.9554 (1.0044)  loss_vfl_dn_0: 0.4879 (0.4888)  loss_vfl_dn_1: 0.4992 (0.5126)  loss_vfl_dn_2: 0.4991 (0.5344)  loss_vfl_dn_3: 0.5209 (0.5559)  loss_vfl_dn_4: 0.5294 (0.5707)  loss_vfl_dn_5: 0.5482 (0.5793)  time: 0.2695  data: 0.0082  max mem: 6603
Epoch: [0]  [ 5300/14786]  eta: 0:41:42  lr: 0.000010  loss: 26.8353 (29.9465)  loss_bbox: 0.2756 (0.3906)  loss_bbox_aux_0: 0.3099 (0.4460)  loss_bbox_aux_1: 0.2879 (0.4120)  loss_bbox_aux_2: 0.2900 (0.4001)  loss_bbox_aux_3: 0.2853 (0.3952)  loss_bbox_aux_4: 0.2771 (0.3931)  loss_bbox_aux_5: 0.4095 (0.5478)  loss_bbox_dn_0: 0.6496 (0.6705)  loss_bbox_dn_1: 0.5801 (0.6160)  loss_bbox_dn_2: 0.5668 (0.5993)  loss_bbox_dn_3: 0.5578 (0.5949)  loss_bbox_dn_4: 0.5526 (0.5940)  loss_bbox_dn_5: 0.5523 (0.5940)  loss_giou: 0.7300 (0.8555)  loss_giou_aux_0: 0.7625 (0.9278)  loss_giou_aux_1: 0.7266 (0.8858)  loss_giou_aux_2: 0.7230 (0.8697)  loss_giou_aux_3: 0.7466 (0.8620)  loss_giou_aux_4: 0.7383 (0.8586)  loss_giou_aux_5: 0.8896 (1.0462)  loss_giou_dn_0: 1.0126 (1.0978)  loss_giou_dn_1: 0.9127 (1.0191)  loss_giou_dn_2: 0.8696 (0.9873)  loss_giou_dn_3: 0.8564 (0.9759)  loss_giou_dn_4: 0.8474 (0.9738)  loss_giou_dn_5: 0.8470 (0.9746)  loss_vfl: 1.0333 (1.1503)  loss_vfl_aux_0: 1.0324 (1.0673)  loss_vfl_aux_1: 1.0545 (1.1102)  loss_vfl_aux_2: 1.0333 (1.1231)  loss_vfl_aux_3: 1.0419 (1.1281)  loss_vfl_aux_4: 1.0518 (1.1349)  loss_vfl_aux_5: 1.0395 (1.0051)  loss_vfl_dn_0: 0.4770 (0.4889)  loss_vfl_dn_1: 0.4773 (0.5124)  loss_vfl_dn_2: 0.4850 (0.5339)  loss_vfl_dn_3: 0.5004 (0.5554)  loss_vfl_dn_4: 0.5155 (0.5701)  loss_vfl_dn_5: 0.5317 (0.5789)  time: 0.2564  data: 0.0094  max mem: 6603
Epoch: [0]  [ 5400/14786]  eta: 0:41:16  lr: 0.000010  loss: 26.6364 (29.8933)  loss_bbox: 0.2766 (0.3891)  loss_bbox_aux_0: 0.3005 (0.4442)  loss_bbox_aux_1: 0.2766 (0.4103)  loss_bbox_aux_2: 0.2735 (0.3985)  loss_bbox_aux_3: 0.2801 (0.3937)  loss_bbox_aux_4: 0.2770 (0.3916)  loss_bbox_aux_5: 0.4049 (0.5459)  loss_bbox_dn_0: 0.5718 (0.6691)  loss_bbox_dn_1: 0.5130 (0.6143)  loss_bbox_dn_2: 0.4979 (0.5976)  loss_bbox_dn_3: 0.4931 (0.5931)  loss_bbox_dn_4: 0.4936 (0.5922)  loss_bbox_dn_5: 0.4935 (0.5923)  loss_giou: 0.7033 (0.8533)  loss_giou_aux_0: 0.7724 (0.9253)  loss_giou_aux_1: 0.7308 (0.8833)  loss_giou_aux_2: 0.7431 (0.8673)  loss_giou_aux_3: 0.7313 (0.8597)  loss_giou_aux_4: 0.7133 (0.8563)  loss_giou_aux_5: 0.9122 (1.0439)  loss_giou_dn_0: 1.0030 (1.0959)  loss_giou_dn_1: 0.8956 (1.0167)  loss_giou_dn_2: 0.8567 (0.9848)  loss_giou_dn_3: 0.8423 (0.9734)  loss_giou_dn_4: 0.8403 (0.9713)  loss_giou_dn_5: 0.8427 (0.9721)  loss_vfl: 1.0672 (1.1497)  loss_vfl_aux_0: 1.0754 (1.0681)  loss_vfl_aux_1: 1.1335 (1.1108)  loss_vfl_aux_2: 1.0757 (1.1232)  loss_vfl_aux_3: 1.0628 (1.1279)  loss_vfl_aux_4: 1.0550 (1.1345)  loss_vfl_aux_5: 1.0133 (1.0058)  loss_vfl_dn_0: 0.4903 (0.4891)  loss_vfl_dn_1: 0.5072 (0.5123)  loss_vfl_dn_2: 0.5152 (0.5336)  loss_vfl_dn_3: 0.5202 (0.5549)  loss_vfl_dn_4: 0.5337 (0.5697)  loss_vfl_dn_5: 0.5399 (0.5786)  time: 0.2609  data: 0.0088  max mem: 6603
Epoch: [0]  [ 5500/14786]  eta: 0:40:54  lr: 0.000010  loss: 27.3494 (29.8375)  loss_bbox: 0.3179 (0.3877)  loss_bbox_aux_0: 0.3646 (0.4424)  loss_bbox_aux_1: 0.3361 (0.4086)  loss_bbox_aux_2: 0.3209 (0.3970)  loss_bbox_aux_3: 0.3184 (0.3922)  loss_bbox_aux_4: 0.3200 (0.3902)  loss_bbox_aux_5: 0.4522 (0.5441)  loss_bbox_dn_0: 0.6009 (0.6675)  loss_bbox_dn_1: 0.5444 (0.6125)  loss_bbox_dn_2: 0.5267 (0.5957)  loss_bbox_dn_3: 0.5242 (0.5912)  loss_bbox_dn_4: 0.5267 (0.5903)  loss_bbox_dn_5: 0.5266 (0.5903)  loss_giou: 0.7106 (0.8517)  loss_giou_aux_0: 0.7749 (0.9234)  loss_giou_aux_1: 0.7276 (0.8816)  loss_giou_aux_2: 0.6993 (0.8656)  loss_giou_aux_3: 0.6986 (0.8580)  loss_giou_aux_4: 0.7106 (0.8547)  loss_giou_aux_5: 0.8795 (1.0422)  loss_giou_dn_0: 0.9952 (1.0941)  loss_giou_dn_1: 0.8982 (1.0146)  loss_giou_dn_2: 0.8562 (0.9826)  loss_giou_dn_3: 0.8441 (0.9712)  loss_giou_dn_4: 0.8438 (0.9691)  loss_giou_dn_5: 0.8439 (0.9699)  loss_vfl: 1.1177 (1.1481)  loss_vfl_aux_0: 1.1106 (1.0680)  loss_vfl_aux_1: 1.1115 (1.1102)  loss_vfl_aux_2: 1.1352 (1.1221)  loss_vfl_aux_3: 1.0589 (1.1267)  loss_vfl_aux_4: 1.0811 (1.1330)  loss_vfl_aux_5: 0.9930 (1.0055)  loss_vfl_dn_0: 0.4989 (0.4891)  loss_vfl_dn_1: 0.5057 (0.5121)  loss_vfl_dn_2: 0.5144 (0.5331)  loss_vfl_dn_3: 0.5295 (0.5542)  loss_vfl_dn_4: 0.5333 (0.5690)  loss_vfl_dn_5: 0.5386 (0.5780)  time: 0.2647  data: 0.0096  max mem: 6603
Epoch: [0]  [ 5600/14786]  eta: 0:40:27  lr: 0.000010  loss: 27.1047 (29.7865)  loss_bbox: 0.3286 (0.3862)  loss_bbox_aux_0: 0.3352 (0.4406)  loss_bbox_aux_1: 0.3420 (0.4070)  loss_bbox_aux_2: 0.3251 (0.3954)  loss_bbox_aux_3: 0.3183 (0.3907)  loss_bbox_aux_4: 0.3161 (0.3887)  loss_bbox_aux_5: 0.4353 (0.5423)  loss_bbox_dn_0: 0.5615 (0.6663)  loss_bbox_dn_1: 0.5197 (0.6110)  loss_bbox_dn_2: 0.5037 (0.5941)  loss_bbox_dn_3: 0.5082 (0.5896)  loss_bbox_dn_4: 0.5082 (0.5886)  loss_bbox_dn_5: 0.5082 (0.5887)  loss_giou: 0.7235 (0.8493)  loss_giou_aux_0: 0.7928 (0.9208)  loss_giou_aux_1: 0.7621 (0.8791)  loss_giou_aux_2: 0.7600 (0.8632)  loss_giou_aux_3: 0.7439 (0.8557)  loss_giou_aux_4: 0.7340 (0.8524)  loss_giou_aux_5: 0.9626 (1.0397)  loss_giou_dn_0: 1.0135 (1.0922)  loss_giou_dn_1: 0.9035 (1.0123)  loss_giou_dn_2: 0.8681 (0.9802)  loss_giou_dn_3: 0.8557 (0.9687)  loss_giou_dn_4: 0.8517 (0.9666)  loss_giou_dn_5: 0.8516 (0.9674)  loss_vfl: 0.9747 (1.1478)  loss_vfl_aux_0: 1.0186 (1.0690)  loss_vfl_aux_1: 1.0059 (1.1107)  loss_vfl_aux_2: 0.9992 (1.1223)  loss_vfl_aux_3: 0.9993 (1.1267)  loss_vfl_aux_4: 0.9910 (1.1328)  loss_vfl_aux_5: 0.9883 (1.0065)  loss_vfl_dn_0: 0.4835 (0.4893)  loss_vfl_dn_1: 0.5059 (0.5121)  loss_vfl_dn_2: 0.5057 (0.5327)  loss_vfl_dn_3: 0.5168 (0.5538)  loss_vfl_dn_4: 0.5263 (0.5685)  loss_vfl_dn_5: 0.5409 (0.5776)  time: 0.2583  data: 0.0106  max mem: 6603
Epoch: [0]  [ 5700/14786]  eta: 0:40:00  lr: 0.000010  loss: 26.7917 (29.7399)  loss_bbox: 0.3039 (0.3850)  loss_bbox_aux_0: 0.3306 (0.4391)  loss_bbox_aux_1: 0.3019 (0.4056)  loss_bbox_aux_2: 0.3051 (0.3941)  loss_bbox_aux_3: 0.3114 (0.3894)  loss_bbox_aux_4: 0.3076 (0.3875)  loss_bbox_aux_5: 0.4329 (0.5408)  loss_bbox_dn_0: 0.5220 (0.6654)  loss_bbox_dn_1: 0.4690 (0.6099)  loss_bbox_dn_2: 0.4277 (0.5929)  loss_bbox_dn_3: 0.4224 (0.5884)  loss_bbox_dn_4: 0.4222 (0.5874)  loss_bbox_dn_5: 0.4222 (0.5875)  loss_giou: 0.7747 (0.8475)  loss_giou_aux_0: 0.8260 (0.9188)  loss_giou_aux_1: 0.8164 (0.8771)  loss_giou_aux_2: 0.7871 (0.8613)  loss_giou_aux_3: 0.7629 (0.8538)  loss_giou_aux_4: 0.7791 (0.8505)  loss_giou_aux_5: 0.9460 (1.0378)  loss_giou_dn_0: 1.0368 (1.0906)  loss_giou_dn_1: 0.9407 (1.0102)  loss_giou_dn_2: 0.9077 (0.9780)  loss_giou_dn_3: 0.8956 (0.9665)  loss_giou_dn_4: 0.8894 (0.9644)  loss_giou_dn_5: 0.8892 (0.9652)  loss_vfl: 0.9668 (1.1467)  loss_vfl_aux_0: 0.9880 (1.0693)  loss_vfl_aux_1: 1.0038 (1.1108)  loss_vfl_aux_2: 0.9808 (1.1219)  loss_vfl_aux_3: 0.9917 (1.1260)  loss_vfl_aux_4: 0.9715 (1.1318)  loss_vfl_aux_5: 0.9176 (1.0068)  loss_vfl_dn_0: 0.4658 (0.4893)  loss_vfl_dn_1: 0.4747 (0.5119)  loss_vfl_dn_2: 0.4894 (0.5324)  loss_vfl_dn_3: 0.4889 (0.5533)  loss_vfl_dn_4: 0.5010 (0.5680)  loss_vfl_dn_5: 0.5225 (0.5772)  time: 0.2616  data: 0.0093  max mem: 6603
Epoch: [0]  [ 5800/14786]  eta: 0:39:33  lr: 0.000010  loss: 25.6796 (29.6817)  loss_bbox: 0.2589 (0.3832)  loss_bbox_aux_0: 0.2817 (0.4369)  loss_bbox_aux_1: 0.2738 (0.4036)  loss_bbox_aux_2: 0.2533 (0.3922)  loss_bbox_aux_3: 0.2595 (0.3875)  loss_bbox_aux_4: 0.2589 (0.3857)  loss_bbox_aux_5: 0.3805 (0.5384)  loss_bbox_dn_0: 0.4899 (0.6632)  loss_bbox_dn_1: 0.4162 (0.6076)  loss_bbox_dn_2: 0.3955 (0.5907)  loss_bbox_dn_3: 0.3962 (0.5861)  loss_bbox_dn_4: 0.3959 (0.5852)  loss_bbox_dn_5: 0.3959 (0.5852)  loss_giou: 0.7138 (0.8461)  loss_giou_aux_0: 0.7731 (0.9171)  loss_giou_aux_1: 0.7330 (0.8757)  loss_giou_aux_2: 0.7187 (0.8599)  loss_giou_aux_3: 0.7154 (0.8524)  loss_giou_aux_4: 0.7029 (0.8491)  loss_giou_aux_5: 0.8487 (1.0361)  loss_giou_dn_0: 1.0141 (1.0891)  loss_giou_dn_1: 0.9050 (1.0085)  loss_giou_dn_2: 0.8718 (0.9763)  loss_giou_dn_3: 0.8589 (0.9647)  loss_giou_dn_4: 0.8544 (0.9626)  loss_giou_dn_5: 0.8548 (0.9633)  loss_vfl: 1.0744 (1.1449)  loss_vfl_aux_0: 1.0712 (1.0689)  loss_vfl_aux_1: 1.0829 (1.1098)  loss_vfl_aux_2: 1.0956 (1.1208)  loss_vfl_aux_3: 1.0577 (1.1246)  loss_vfl_aux_4: 1.0554 (1.1302)  loss_vfl_aux_5: 1.0130 (1.0065)  loss_vfl_dn_0: 0.4871 (0.4894)  loss_vfl_dn_1: 0.4946 (0.5117)  loss_vfl_dn_2: 0.4969 (0.5319)  loss_vfl_dn_3: 0.5075 (0.5526)  loss_vfl_dn_4: 0.5232 (0.5674)  loss_vfl_dn_5: 0.5352 (0.5767)  time: 0.2592  data: 0.0099  max mem: 6603
Epoch: [0]  [ 5900/14786]  eta: 0:39:05  lr: 0.000010  loss: 25.9049 (29.6301)  loss_bbox: 0.2452 (0.3817)  loss_bbox_aux_0: 0.2937 (0.4352)  loss_bbox_aux_1: 0.2672 (0.4019)  loss_bbox_aux_2: 0.2593 (0.3906)  loss_bbox_aux_3: 0.2418 (0.3860)  loss_bbox_aux_4: 0.2441 (0.3841)  loss_bbox_aux_5: 0.3738 (0.5366)  loss_bbox_dn_0: 0.5029 (0.6618)  loss_bbox_dn_1: 0.4479 (0.6060)  loss_bbox_dn_2: 0.4341 (0.5890)  loss_bbox_dn_3: 0.4249 (0.5844)  loss_bbox_dn_4: 0.4260 (0.5835)  loss_bbox_dn_5: 0.4258 (0.5835)  loss_giou: 0.7445 (0.8443)  loss_giou_aux_0: 0.8250 (0.9151)  loss_giou_aux_1: 0.7785 (0.8738)  loss_giou_aux_2: 0.7512 (0.8580)  loss_giou_aux_3: 0.7427 (0.8506)  loss_giou_aux_4: 0.7457 (0.8473)  loss_giou_aux_5: 0.9505 (1.0342)  loss_giou_dn_0: 1.0040 (1.0875)  loss_giou_dn_1: 0.8973 (1.0065)  loss_giou_dn_2: 0.8480 (0.9742)  loss_giou_dn_3: 0.8360 (0.9627)  loss_giou_dn_4: 0.8343 (0.9605)  loss_giou_dn_5: 0.8345 (0.9613)  loss_vfl: 1.0713 (1.1438)  loss_vfl_aux_0: 1.0335 (1.0689)  loss_vfl_aux_1: 1.0337 (1.1095)  loss_vfl_aux_2: 1.0892 (1.1203)  loss_vfl_aux_3: 1.0750 (1.1238)  loss_vfl_aux_4: 1.0720 (1.1292)  loss_vfl_aux_5: 0.9508 (1.0066)  loss_vfl_dn_0: 0.4825 (0.4895)  loss_vfl_dn_1: 0.4875 (0.5117)  loss_vfl_dn_2: 0.4961 (0.5315)  loss_vfl_dn_3: 0.5024 (0.5520)  loss_vfl_dn_4: 0.5122 (0.5668)  loss_vfl_dn_5: 0.5184 (0.5762)  time: 0.2455  data: 0.0078  max mem: 6603
Epoch: [0]  [ 6000/14786]  eta: 0:38:38  lr: 0.000010  loss: 25.6183 (29.5792)  loss_bbox: 0.2672 (0.3802)  loss_bbox_aux_0: 0.3019 (0.4334)  loss_bbox_aux_1: 0.2896 (0.4002)  loss_bbox_aux_2: 0.2704 (0.3890)  loss_bbox_aux_3: 0.2864 (0.3844)  loss_bbox_aux_4: 0.2746 (0.3826)  loss_bbox_aux_5: 0.3751 (0.5346)  loss_bbox_dn_0: 0.4974 (0.6603)  loss_bbox_dn_1: 0.4352 (0.6043)  loss_bbox_dn_2: 0.4108 (0.5873)  loss_bbox_dn_3: 0.3949 (0.5828)  loss_bbox_dn_4: 0.3871 (0.5818)  loss_bbox_dn_5: 0.3872 (0.5819)  loss_giou: 0.7442 (0.8423)  loss_giou_aux_0: 0.7843 (0.9129)  loss_giou_aux_1: 0.7539 (0.8718)  loss_giou_aux_2: 0.7567 (0.8560)  loss_giou_aux_3: 0.7097 (0.8485)  loss_giou_aux_4: 0.7440 (0.8453)  loss_giou_aux_5: 0.8982 (1.0322)  loss_giou_dn_0: 0.9925 (1.0859)  loss_giou_dn_1: 0.8844 (1.0045)  loss_giou_dn_2: 0.8501 (0.9722)  loss_giou_dn_3: 0.8361 (0.9606)  loss_giou_dn_4: 0.8340 (0.9584)  loss_giou_dn_5: 0.8340 (0.9592)  loss_vfl: 1.0401 (1.1430)  loss_vfl_aux_0: 0.9949 (1.0693)  loss_vfl_aux_1: 1.0528 (1.1095)  loss_vfl_aux_2: 1.0273 (1.1200)  loss_vfl_aux_3: 1.0377 (1.1233)  loss_vfl_aux_4: 1.0169 (1.1285)  loss_vfl_aux_5: 0.9723 (1.0073)  loss_vfl_dn_0: 0.4808 (0.4896)  loss_vfl_dn_1: 0.4959 (0.5116)  loss_vfl_dn_2: 0.4956 (0.5311)  loss_vfl_dn_3: 0.5197 (0.5515)  loss_vfl_dn_4: 0.5389 (0.5662)  loss_vfl_dn_5: 0.5558 (0.5758)  time: 0.2639  data: 0.0100  max mem: 6603
Epoch: [0]  [ 6100/14786]  eta: 0:38:12  lr: 0.000010  loss: 26.1200 (29.5277)  loss_bbox: 0.2943 (0.3789)  loss_bbox_aux_0: 0.3433 (0.4318)  loss_bbox_aux_1: 0.3078 (0.3988)  loss_bbox_aux_2: 0.2928 (0.3876)  loss_bbox_aux_3: 0.2894 (0.3831)  loss_bbox_aux_4: 0.2891 (0.3813)  loss_bbox_aux_5: 0.4364 (0.5331)  loss_bbox_dn_0: 0.6139 (0.6591)  loss_bbox_dn_1: 0.5071 (0.6028)  loss_bbox_dn_2: 0.4819 (0.5857)  loss_bbox_dn_3: 0.4832 (0.5811)  loss_bbox_dn_4: 0.4852 (0.5802)  loss_bbox_dn_5: 0.4853 (0.5802)  loss_giou: 0.6508 (0.8400)  loss_giou_aux_0: 0.7321 (0.9104)  loss_giou_aux_1: 0.6527 (0.8693)  loss_giou_aux_2: 0.6561 (0.8536)  loss_giou_aux_3: 0.6536 (0.8462)  loss_giou_aux_4: 0.6581 (0.8429)  loss_giou_aux_5: 0.8888 (1.0298)  loss_giou_dn_0: 0.9564 (1.0839)  loss_giou_dn_1: 0.8303 (1.0021)  loss_giou_dn_2: 0.7736 (0.9697)  loss_giou_dn_3: 0.7599 (0.9580)  loss_giou_dn_4: 0.7502 (0.9559)  loss_giou_dn_5: 0.7505 (0.9566)  loss_vfl: 1.0059 (1.1423)  loss_vfl_aux_0: 1.0340 (1.0703)  loss_vfl_aux_1: 1.1256 (1.1098)  loss_vfl_aux_2: 1.0498 (1.1200)  loss_vfl_aux_3: 1.0095 (1.1229)  loss_vfl_aux_4: 1.0004 (1.1279)  loss_vfl_aux_5: 1.0384 (1.0081)  loss_vfl_dn_0: 0.5004 (0.4898)  loss_vfl_dn_1: 0.5106 (0.5116)  loss_vfl_dn_2: 0.5164 (0.5309)  loss_vfl_dn_3: 0.5195 (0.5511)  loss_vfl_dn_4: 0.5258 (0.5658)  loss_vfl_dn_5: 0.5483 (0.5754)  time: 0.2659  data: 0.0095  max mem: 6603
Epoch: [0]  [ 6200/14786]  eta: 0:37:45  lr: 0.000010  loss: 26.1637 (29.4832)  loss_bbox: 0.2534 (0.3777)  loss_bbox_aux_0: 0.2940 (0.4303)  loss_bbox_aux_1: 0.2577 (0.3974)  loss_bbox_aux_2: 0.2609 (0.3862)  loss_bbox_aux_3: 0.2717 (0.3818)  loss_bbox_aux_4: 0.2641 (0.3801)  loss_bbox_aux_5: 0.3862 (0.5316)  loss_bbox_dn_0: 0.6081 (0.6584)  loss_bbox_dn_1: 0.5086 (0.6018)  loss_bbox_dn_2: 0.4713 (0.5847)  loss_bbox_dn_3: 0.4712 (0.5800)  loss_bbox_dn_4: 0.4716 (0.5791)  loss_bbox_dn_5: 0.4717 (0.5791)  loss_giou: 0.6782 (0.8377)  loss_giou_aux_0: 0.7343 (0.9080)  loss_giou_aux_1: 0.6920 (0.8670)  loss_giou_aux_2: 0.6729 (0.8512)  loss_giou_aux_3: 0.6725 (0.8439)  loss_giou_aux_4: 0.6791 (0.8407)  loss_giou_aux_5: 0.8750 (1.0276)  loss_giou_dn_0: 0.9521 (1.0822)  loss_giou_dn_1: 0.8193 (0.9998)  loss_giou_dn_2: 0.7758 (0.9673)  loss_giou_dn_3: 0.7587 (0.9557)  loss_giou_dn_4: 0.7529 (0.9535)  loss_giou_dn_5: 0.7524 (0.9542)  loss_vfl: 1.0793 (1.1419)  loss_vfl_aux_0: 1.1015 (1.0709)  loss_vfl_aux_1: 1.1408 (1.1103)  loss_vfl_aux_2: 1.1686 (1.1203)  loss_vfl_aux_3: 1.0845 (1.1228)  loss_vfl_aux_4: 1.0583 (1.1277)  loss_vfl_aux_5: 1.0078 (1.0091)  loss_vfl_dn_0: 0.5030 (0.4900)  loss_vfl_dn_1: 0.5149 (0.5116)  loss_vfl_dn_2: 0.5217 (0.5306)  loss_vfl_dn_3: 0.5400 (0.5507)  loss_vfl_dn_4: 0.5547 (0.5653)  loss_vfl_dn_5: 0.5654 (0.5751)  time: 0.2523  data: 0.0087  max mem: 6603
Epoch: [0]  [ 6300/14786]  eta: 0:37:18  lr: 0.000010  loss: 25.6585 (29.4365)  loss_bbox: 0.2601 (0.3761)  loss_bbox_aux_0: 0.2850 (0.4285)  loss_bbox_aux_1: 0.2650 (0.3957)  loss_bbox_aux_2: 0.2579 (0.3846)  loss_bbox_aux_3: 0.2588 (0.3802)  loss_bbox_aux_4: 0.2579 (0.3785)  loss_bbox_aux_5: 0.3696 (0.5297)  loss_bbox_dn_0: 0.4854 (0.6567)  loss_bbox_dn_1: 0.4142 (0.6000)  loss_bbox_dn_2: 0.3747 (0.5828)  loss_bbox_dn_3: 0.3590 (0.5782)  loss_bbox_dn_4: 0.3550 (0.5772)  loss_bbox_dn_5: 0.3551 (0.5773)  loss_giou: 0.7038 (0.8360)  loss_giou_aux_0: 0.7375 (0.9061)  loss_giou_aux_1: 0.7195 (0.8652)  loss_giou_aux_2: 0.7200 (0.8495)  loss_giou_aux_3: 0.7143 (0.8421)  loss_giou_aux_4: 0.7052 (0.8390)  loss_giou_aux_5: 0.8760 (1.0258)  loss_giou_dn_0: 0.9750 (1.0806)  loss_giou_dn_1: 0.8929 (0.9979)  loss_giou_dn_2: 0.8570 (0.9654)  loss_giou_dn_3: 0.8358 (0.9537)  loss_giou_dn_4: 0.8359 (0.9515)  loss_giou_dn_5: 0.8357 (0.9522)  loss_vfl: 0.9988 (1.1415)  loss_vfl_aux_0: 1.0060 (1.0713)  loss_vfl_aux_1: 1.0142 (1.1106)  loss_vfl_aux_2: 1.0106 (1.1205)  loss_vfl_aux_3: 1.0036 (1.1228)  loss_vfl_aux_4: 0.9595 (1.1274)  loss_vfl_aux_5: 0.9538 (1.0095)  loss_vfl_dn_0: 0.4888 (0.4902)  loss_vfl_dn_1: 0.4967 (0.5116)  loss_vfl_dn_2: 0.5113 (0.5304)  loss_vfl_dn_3: 0.5242 (0.5504)  loss_vfl_dn_4: 0.5453 (0.5650)  loss_vfl_dn_5: 0.5614 (0.5750)  time: 0.2517  data: 0.0084  max mem: 6603
Epoch: [0]  [ 6400/14786]  eta: 0:36:51  lr: 0.000010  loss: 25.6486 (29.3890)  loss_bbox: 0.2528 (0.3747)  loss_bbox_aux_0: 0.3111 (0.4268)  loss_bbox_aux_1: 0.2805 (0.3942)  loss_bbox_aux_2: 0.2677 (0.3831)  loss_bbox_aux_3: 0.2617 (0.3788)  loss_bbox_aux_4: 0.2529 (0.3771)  loss_bbox_aux_5: 0.3882 (0.5278)  loss_bbox_dn_0: 0.5220 (0.6552)  loss_bbox_dn_1: 0.4572 (0.5983)  loss_bbox_dn_2: 0.4305 (0.5811)  loss_bbox_dn_3: 0.4233 (0.5764)  loss_bbox_dn_4: 0.4233 (0.5755)  loss_bbox_dn_5: 0.4232 (0.5755)  loss_giou: 0.6828 (0.8346)  loss_giou_aux_0: 0.7408 (0.9045)  loss_giou_aux_1: 0.7081 (0.8637)  loss_giou_aux_2: 0.6983 (0.8480)  loss_giou_aux_3: 0.6912 (0.8407)  loss_giou_aux_4: 0.6839 (0.8376)  loss_giou_aux_5: 0.9006 (1.0242)  loss_giou_dn_0: 0.9611 (1.0791)  loss_giou_dn_1: 0.8473 (0.9962)  loss_giou_dn_2: 0.8070 (0.9636)  loss_giou_dn_3: 0.7881 (0.9519)  loss_giou_dn_4: 0.7862 (0.9497)  loss_giou_dn_5: 0.7862 (0.9504)  loss_vfl: 0.9755 (1.1403)  loss_vfl_aux_0: 1.0167 (1.0713)  loss_vfl_aux_1: 0.9983 (1.1102)  loss_vfl_aux_2: 1.0204 (1.1198)  loss_vfl_aux_3: 0.9827 (1.1219)  loss_vfl_aux_4: 0.9916 (1.1264)  loss_vfl_aux_5: 0.9888 (1.0095)  loss_vfl_dn_0: 0.4930 (0.4903)  loss_vfl_dn_1: 0.4967 (0.5115)  loss_vfl_dn_2: 0.5053 (0.5300)  loss_vfl_dn_3: 0.5083 (0.5499)  loss_vfl_dn_4: 0.5242 (0.5645)  loss_vfl_dn_5: 0.5328 (0.5745)  time: 0.2704  data: 0.0096  max mem: 6603
Epoch: [0]  [ 6500/14786]  eta: 0:36:24  lr: 0.000010  loss: 26.1866 (29.3399)  loss_bbox: 0.2795 (0.3734)  loss_bbox_aux_0: 0.3141 (0.4252)  loss_bbox_aux_1: 0.2973 (0.3927)  loss_bbox_aux_2: 0.2801 (0.3817)  loss_bbox_aux_3: 0.2801 (0.3774)  loss_bbox_aux_4: 0.2785 (0.3757)  loss_bbox_aux_5: 0.4376 (0.5260)  loss_bbox_dn_0: 0.5758 (0.6537)  loss_bbox_dn_1: 0.5114 (0.5967)  loss_bbox_dn_2: 0.4996 (0.5795)  loss_bbox_dn_3: 0.5020 (0.5748)  loss_bbox_dn_4: 0.5028 (0.5738)  loss_bbox_dn_5: 0.5027 (0.5739)  loss_giou: 0.6920 (0.8330)  loss_giou_aux_0: 0.7428 (0.9028)  loss_giou_aux_1: 0.7146 (0.8620)  loss_giou_aux_2: 0.6900 (0.8464)  loss_giou_aux_3: 0.6973 (0.8391)  loss_giou_aux_4: 0.6966 (0.8360)  loss_giou_aux_5: 0.8594 (1.0225)  loss_giou_dn_0: 0.9762 (1.0776)  loss_giou_dn_1: 0.8491 (0.9944)  loss_giou_dn_2: 0.8207 (0.9617)  loss_giou_dn_3: 0.8070 (0.9500)  loss_giou_dn_4: 0.8039 (0.9478)  loss_giou_dn_5: 0.8041 (0.9485)  loss_vfl: 1.0458 (1.1392)  loss_vfl_aux_0: 1.0590 (1.0713)  loss_vfl_aux_1: 1.0803 (1.1098)  loss_vfl_aux_2: 1.0857 (1.1191)  loss_vfl_aux_3: 1.0898 (1.1210)  loss_vfl_aux_4: 1.0491 (1.1254)  loss_vfl_aux_5: 1.0085 (1.0094)  loss_vfl_dn_0: 0.4909 (0.4904)  loss_vfl_dn_1: 0.4952 (0.5113)  loss_vfl_dn_2: 0.4981 (0.5297)  loss_vfl_dn_3: 0.5110 (0.5493)  loss_vfl_dn_4: 0.5297 (0.5639)  loss_vfl_dn_5: 0.5401 (0.5740)  time: 0.2553  data: 0.0097  max mem: 6603
Epoch: [0]  [ 6600/14786]  eta: 0:35:57  lr: 0.000010  loss: 25.7791 (29.2974)  loss_bbox: 0.2588 (0.3723)  loss_bbox_aux_0: 0.2940 (0.4237)  loss_bbox_aux_1: 0.2735 (0.3914)  loss_bbox_aux_2: 0.2525 (0.3805)  loss_bbox_aux_3: 0.2530 (0.3762)  loss_bbox_aux_4: 0.2617 (0.3746)  loss_bbox_aux_5: 0.3976 (0.5246)  loss_bbox_dn_0: 0.5857 (0.6527)  loss_bbox_dn_1: 0.4962 (0.5955)  loss_bbox_dn_2: 0.4643 (0.5782)  loss_bbox_dn_3: 0.4706 (0.5735)  loss_bbox_dn_4: 0.4741 (0.5726)  loss_bbox_dn_5: 0.4747 (0.5726)  loss_giou: 0.6262 (0.8312)  loss_giou_aux_0: 0.6753 (0.9008)  loss_giou_aux_1: 0.6386 (0.8602)  loss_giou_aux_2: 0.6284 (0.8445)  loss_giou_aux_3: 0.6137 (0.8372)  loss_giou_aux_4: 0.6260 (0.8341)  loss_giou_aux_5: 0.8260 (1.0207)  loss_giou_dn_0: 0.9438 (1.0761)  loss_giou_dn_1: 0.8231 (0.9926)  loss_giou_dn_2: 0.7951 (0.9599)  loss_giou_dn_3: 0.7845 (0.9482)  loss_giou_dn_4: 0.7866 (0.9459)  loss_giou_dn_5: 0.7908 (0.9466)  loss_vfl: 1.0744 (1.1384)  loss_vfl_aux_0: 1.1081 (1.0717)  loss_vfl_aux_1: 1.0801 (1.1098)  loss_vfl_aux_2: 1.0661 (1.1190)  loss_vfl_aux_3: 1.0719 (1.1206)  loss_vfl_aux_4: 1.0782 (1.1248)  loss_vfl_aux_5: 1.0189 (1.0098)  loss_vfl_dn_0: 0.5017 (0.4904)  loss_vfl_dn_1: 0.5151 (0.5112)  loss_vfl_dn_2: 0.5151 (0.5293)  loss_vfl_dn_3: 0.5193 (0.5488)  loss_vfl_dn_4: 0.5406 (0.5634)  loss_vfl_dn_5: 0.5549 (0.5736)  time: 0.2600  data: 0.0090  max mem: 6603
Epoch: [0]  [ 6700/14786]  eta: 0:35:31  lr: 0.000010  loss: 25.2727 (29.2517)  loss_bbox: 0.2657 (0.3713)  loss_bbox_aux_0: 0.2866 (0.4223)  loss_bbox_aux_1: 0.2688 (0.3901)  loss_bbox_aux_2: 0.2637 (0.3793)  loss_bbox_aux_3: 0.2638 (0.3750)  loss_bbox_aux_4: 0.2605 (0.3735)  loss_bbox_aux_5: 0.3871 (0.5231)  loss_bbox_dn_0: 0.5158 (0.6516)  loss_bbox_dn_1: 0.4413 (0.5942)  loss_bbox_dn_2: 0.4230 (0.5768)  loss_bbox_dn_3: 0.4193 (0.5721)  loss_bbox_dn_4: 0.4198 (0.5712)  loss_bbox_dn_5: 0.4197 (0.5712)  loss_giou: 0.6628 (0.8296)  loss_giou_aux_0: 0.7237 (0.8989)  loss_giou_aux_1: 0.6962 (0.8584)  loss_giou_aux_2: 0.6672 (0.8428)  loss_giou_aux_3: 0.6792 (0.8355)  loss_giou_aux_4: 0.6770 (0.8325)  loss_giou_aux_5: 0.8543 (1.0188)  loss_giou_dn_0: 0.9431 (1.0745)  loss_giou_dn_1: 0.8334 (0.9906)  loss_giou_dn_2: 0.7994 (0.9579)  loss_giou_dn_3: 0.7952 (0.9461)  loss_giou_dn_4: 0.7954 (0.9439)  loss_giou_dn_5: 0.7967 (0.9446)  loss_vfl: 1.0161 (1.1373)  loss_vfl_aux_0: 1.0572 (1.0720)  loss_vfl_aux_1: 1.1062 (1.1098)  loss_vfl_aux_2: 1.0107 (1.1185)  loss_vfl_aux_3: 1.0171 (1.1199)  loss_vfl_aux_4: 1.0036 (1.1238)  loss_vfl_aux_5: 1.0283 (1.0102)  loss_vfl_dn_0: 0.4991 (0.4906)  loss_vfl_dn_1: 0.4991 (0.5111)  loss_vfl_dn_2: 0.4967 (0.5290)  loss_vfl_dn_3: 0.4932 (0.5483)  loss_vfl_dn_4: 0.4940 (0.5628)  loss_vfl_dn_5: 0.5069 (0.5730)  time: 0.2523  data: 0.0087  max mem: 6603
