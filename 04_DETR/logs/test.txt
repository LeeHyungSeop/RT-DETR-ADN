/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
| distributed init (rank 0): env://
git:
  sha: a4bbde1b3c3623bfcdd0f212279dd4523277c9d7, status: has uncommited changes, branch: main

Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=4, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='coco', coco_path='/media/data/coco', coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
/home/hslee/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/hslee/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
number of params: 41302368
module.transformer.encoder.layers.0.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.0.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.0.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.0.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.0.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.0.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.0.linear2.bias torch.Size([256])
module.transformer.encoder.layers.0.norm1.weight torch.Size([256])
module.transformer.encoder.layers.0.norm1.bias torch.Size([256])
module.transformer.encoder.layers.0.norm2.weight torch.Size([256])
module.transformer.encoder.layers.0.norm2.bias torch.Size([256])
module.transformer.encoder.layers.1.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.1.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.1.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.1.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.1.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.1.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.1.linear2.bias torch.Size([256])
module.transformer.encoder.layers.1.norm1.weight torch.Size([256])
module.transformer.encoder.layers.1.norm1.bias torch.Size([256])
module.transformer.encoder.layers.1.norm2.weight torch.Size([256])
module.transformer.encoder.layers.1.norm2.bias torch.Size([256])
module.transformer.encoder.layers.2.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.2.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.2.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.2.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.2.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.2.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.2.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.2.linear2.bias torch.Size([256])
module.transformer.encoder.layers.2.norm1.weight torch.Size([256])
module.transformer.encoder.layers.2.norm1.bias torch.Size([256])
module.transformer.encoder.layers.2.norm2.weight torch.Size([256])
module.transformer.encoder.layers.2.norm2.bias torch.Size([256])
module.transformer.encoder.layers.3.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.3.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.3.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.3.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.3.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.3.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.3.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.3.linear2.bias torch.Size([256])
module.transformer.encoder.layers.3.norm1.weight torch.Size([256])
module.transformer.encoder.layers.3.norm1.bias torch.Size([256])
module.transformer.encoder.layers.3.norm2.weight torch.Size([256])
module.transformer.encoder.layers.3.norm2.bias torch.Size([256])
module.transformer.encoder.layers.4.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.4.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.4.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.4.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.4.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.4.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.4.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.4.linear2.bias torch.Size([256])
module.transformer.encoder.layers.4.norm1.weight torch.Size([256])
module.transformer.encoder.layers.4.norm1.bias torch.Size([256])
module.transformer.encoder.layers.4.norm2.weight torch.Size([256])
module.transformer.encoder.layers.4.norm2.bias torch.Size([256])
module.transformer.encoder.layers.5.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.encoder.layers.5.self_attn.in_proj_bias torch.Size([768])
module.transformer.encoder.layers.5.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.encoder.layers.5.self_attn.out_proj.bias torch.Size([256])
module.transformer.encoder.layers.5.linear1.weight torch.Size([2048, 256])
module.transformer.encoder.layers.5.linear1.bias torch.Size([2048])
module.transformer.encoder.layers.5.linear2.weight torch.Size([256, 2048])
module.transformer.encoder.layers.5.linear2.bias torch.Size([256])
module.transformer.encoder.layers.5.norm1.weight torch.Size([256])
module.transformer.encoder.layers.5.norm1.bias torch.Size([256])
module.transformer.encoder.layers.5.norm2.weight torch.Size([256])
module.transformer.encoder.layers.5.norm2.bias torch.Size([256])
module.transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.0.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.0.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.0.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.0.linear2.bias torch.Size([256])
module.transformer.decoder.layers.0.norm1.weight torch.Size([256])
module.transformer.decoder.layers.0.norm1.bias torch.Size([256])
module.transformer.decoder.layers.0.norm2.weight torch.Size([256])
module.transformer.decoder.layers.0.norm2.bias torch.Size([256])
module.transformer.decoder.layers.0.norm3.weight torch.Size([256])
module.transformer.decoder.layers.0.norm3.bias torch.Size([256])
module.transformer.decoder.layers.1.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.1.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.1.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.1.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.1.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.1.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.1.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.1.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.1.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.1.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.1.linear2.bias torch.Size([256])
module.transformer.decoder.layers.1.norm1.weight torch.Size([256])
module.transformer.decoder.layers.1.norm1.bias torch.Size([256])
module.transformer.decoder.layers.1.norm2.weight torch.Size([256])
module.transformer.decoder.layers.1.norm2.bias torch.Size([256])
module.transformer.decoder.layers.1.norm3.weight torch.Size([256])
module.transformer.decoder.layers.1.norm3.bias torch.Size([256])
module.transformer.decoder.layers.2.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.2.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.2.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.2.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.2.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.2.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.2.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.2.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.2.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.2.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.2.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.2.linear2.bias torch.Size([256])
module.transformer.decoder.layers.2.norm1.weight torch.Size([256])
module.transformer.decoder.layers.2.norm1.bias torch.Size([256])
module.transformer.decoder.layers.2.norm2.weight torch.Size([256])
module.transformer.decoder.layers.2.norm2.bias torch.Size([256])
module.transformer.decoder.layers.2.norm3.weight torch.Size([256])
module.transformer.decoder.layers.2.norm3.bias torch.Size([256])
module.transformer.decoder.layers.3.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.3.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.3.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.3.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.3.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.3.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.3.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.3.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.3.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.3.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.3.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.3.linear2.bias torch.Size([256])
module.transformer.decoder.layers.3.norm1.weight torch.Size([256])
module.transformer.decoder.layers.3.norm1.bias torch.Size([256])
module.transformer.decoder.layers.3.norm2.weight torch.Size([256])
module.transformer.decoder.layers.3.norm2.bias torch.Size([256])
module.transformer.decoder.layers.3.norm3.weight torch.Size([256])
module.transformer.decoder.layers.3.norm3.bias torch.Size([256])
module.transformer.decoder.layers.4.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.4.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.4.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.4.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.4.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.4.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.4.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.4.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.4.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.4.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.4.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.4.linear2.bias torch.Size([256])
module.transformer.decoder.layers.4.norm1.weight torch.Size([256])
module.transformer.decoder.layers.4.norm1.bias torch.Size([256])
module.transformer.decoder.layers.4.norm2.weight torch.Size([256])
module.transformer.decoder.layers.4.norm2.bias torch.Size([256])
module.transformer.decoder.layers.4.norm3.weight torch.Size([256])
module.transformer.decoder.layers.4.norm3.bias torch.Size([256])
module.transformer.decoder.layers.5.self_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.5.self_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.5.self_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.5.self_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.5.multihead_attn.in_proj_weight torch.Size([768, 256])
module.transformer.decoder.layers.5.multihead_attn.in_proj_bias torch.Size([768])
module.transformer.decoder.layers.5.multihead_attn.out_proj.weight torch.Size([256, 256])
module.transformer.decoder.layers.5.multihead_attn.out_proj.bias torch.Size([256])
module.transformer.decoder.layers.5.linear1.weight torch.Size([2048, 256])
module.transformer.decoder.layers.5.linear1.bias torch.Size([2048])
module.transformer.decoder.layers.5.linear2.weight torch.Size([256, 2048])
module.transformer.decoder.layers.5.linear2.bias torch.Size([256])
module.transformer.decoder.layers.5.norm1.weight torch.Size([256])
module.transformer.decoder.layers.5.norm1.bias torch.Size([256])
module.transformer.decoder.layers.5.norm2.weight torch.Size([256])
module.transformer.decoder.layers.5.norm2.bias torch.Size([256])
module.transformer.decoder.layers.5.norm3.weight torch.Size([256])
module.transformer.decoder.layers.5.norm3.bias torch.Size([256])
module.transformer.decoder.norm.weight torch.Size([256])
module.transformer.decoder.norm.bias torch.Size([256])
module.class_embed.weight torch.Size([92, 256])
module.class_embed.bias torch.Size([92])
module.bbox_embed.layers.0.weight torch.Size([256, 256])
module.bbox_embed.layers.0.bias torch.Size([256])
module.bbox_embed.layers.1.weight torch.Size([256, 256])
module.bbox_embed.layers.1.bias torch.Size([256])
module.bbox_embed.layers.2.weight torch.Size([4, 256])
module.bbox_embed.layers.2.bias torch.Size([4])
module.query_embed.weight torch.Size([100, 256])
module.input_proj.weight torch.Size([256, 2048, 1, 1])
module.input_proj.bias torch.Size([256])
module.backbone.0.body.conv1.weight torch.Size([64, 3, 7, 7]) not require grad
module.backbone.0.body.layer1.0.conv1.weight torch.Size([64, 64, 1, 1]) not require grad
module.backbone.0.body.layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) not require grad
module.backbone.0.body.layer1.0.conv3.weight torch.Size([256, 64, 1, 1]) not require grad
module.backbone.0.body.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1]) not require grad
module.backbone.0.body.layer1.1.conv1.weight torch.Size([64, 256, 1, 1]) not require grad
module.backbone.0.body.layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) not require grad
module.backbone.0.body.layer1.1.conv3.weight torch.Size([256, 64, 1, 1]) not require grad
module.backbone.0.body.layer1.2.conv1.weight torch.Size([64, 256, 1, 1]) not require grad
module.backbone.0.body.layer1.2.conv2.weight torch.Size([64, 64, 3, 3]) not require grad
module.backbone.0.body.layer1.2.conv3.weight torch.Size([256, 64, 1, 1]) not require grad
module.backbone.0.body.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])
module.backbone.0.body.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])
module.backbone.0.body.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])
module.backbone.0.body.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])
module.backbone.0.body.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])
module.backbone.0.body.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])
module.backbone.0.body.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])
module.backbone.0.body.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])
module.backbone.0.body.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])
module.backbone.0.body.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])
module.backbone.0.body.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])
module.backbone.0.body.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])
module.backbone.0.body.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])
module.backbone.0.body.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])
module.backbone.0.body.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])
module.backbone.0.body.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])
module.backbone.0.body.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])
module.backbone.0.body.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])
module.backbone.0.body.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])
module.backbone.0.body.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])
module.backbone.0.body.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])
module.backbone.0.body.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])
module.backbone.0.body.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])
module.backbone.0.body.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])
module.backbone.0.body.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])
module.backbone.0.body.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])
module.backbone.0.body.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])
module.backbone.0.body.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])
module.backbone.0.body.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])
module.backbone.0.body.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])
module.backbone.0.body.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])
module.backbone.0.body.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])
loading annotations into memory...
Done (t=8.18s)
creating index...
index created!
loading annotations into memory...
Done (t=0.24s)
creating index...
index created!
Start training
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([896, 4, 256]), pos_embed.shape: torch.Size([896, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[ 5.7738e-01, -5.5328e-01,  8.6964e-01,  ...,  7.2809e-01,
           2.9489e-01,  5.4491e-01],
         [-2.8408e-01,  1.4592e+00,  1.0452e+00,  ...,  2.4944e-01,
           9.0636e-01,  1.2617e-01],
         [-4.5135e-01, -3.7179e-01,  2.9445e-01,  ...,  4.7529e-02,
          -7.5350e-01,  1.1739e-02],
         [-5.3823e-01,  1.5676e+00,  1.2349e+00,  ..., -4.1852e-01,
           5.1508e-01, -2.6843e-01]],

        [[ 7.4108e-01,  4.4052e-01,  4.7134e-01,  ...,  1.0236e+00,
           2.2860e-01,  6.0137e-01],
         [-1.0163e-01,  1.4449e+00,  6.2080e-01,  ..., -7.6271e-01,
           8.1941e-01,  9.5606e-01],
         [-5.3501e-01, -9.6988e-02,  1.9109e-01,  ..., -7.0315e-01,
           5.5574e-02,  5.2151e-01],
         [-8.3860e-01,  1.3130e+00,  1.6018e+00,  ..., -6.1025e-01,
           4.1352e-01,  3.2172e-01]],

        [[ 5.9898e-01,  1.9740e-01,  7.2828e-01,  ...,  8.9277e-01,
           3.7787e-01,  4.3398e-01],
         [ 1.4077e-01,  1.1624e+00,  8.8960e-01,  ..., -8.6215e-01,
           4.5797e-01,  5.2016e-01],
         [-3.5189e-01, -3.0340e-01,  2.2820e-02,  ..., -2.9229e-01,
          -1.6027e-01,  1.5044e-01],
         [-7.7729e-01,  1.4181e+00,  6.4647e-01,  ...,  1.3354e-02,
           4.5089e-01,  2.3916e-04]],

        ...,

        [[ 3.7298e-01,  1.0481e-01,  7.1025e-01,  ...,  8.1817e-01,
           2.2510e-01,  8.7111e-01],
         [ 1.2664e-01,  1.6627e+00,  8.6713e-01,  ..., -5.1602e-01,
           5.6734e-01,  1.0229e+00],
         [-1.9113e-01,  2.8030e-01,  6.9205e-02,  ..., -3.9474e-01,
          -1.8509e-01,  6.5418e-01],
         [-1.2459e-01,  1.4184e+00,  1.3905e+00,  ..., -4.5205e-01,
           2.1350e-01,  2.2264e-01]],

        [[ 1.7400e-01,  3.6048e-01,  8.2137e-01,  ..., -3.5828e-01,
           3.6058e-02,  4.5757e-01],
         [ 2.0412e-02,  1.4338e+00,  6.7435e-01,  ..., -6.9939e-01,
           6.6816e-01,  6.3514e-01],
         [-4.1487e-01, -2.6507e-01,  1.9892e-01,  ..., -2.5977e-01,
          -5.5802e-01,  2.4560e-01],
         [-4.8009e-01,  3.8435e-02,  1.1505e+00,  ..., -3.3517e-01,
           5.0354e-01,  5.8178e-01]],

        [[ 7.1936e-01,  1.2872e-01,  1.1655e+00,  ...,  5.6354e-01,
           1.9405e-01,  6.2627e-01],
         [-3.0087e-01,  1.0340e+00,  9.1175e-01,  ..., -7.4817e-01,
           8.9557e-01,  4.4491e-01],
         [-6.3481e-01, -1.1202e-01,  4.5058e-01,  ..., -6.2095e-01,
           1.5859e-02,  6.2578e-01],
         [-2.9236e-01,  1.4618e+00,  1.3117e+00,  ..., -3.0640e-01,
           2.2922e-01, -8.6510e-02]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-4.2422e-02,  1.4309e+00,  1.6529e+00,  ...,  4.8891e-01,
           6.6318e-01,  6.8857e-02],
         [-4.8178e-01,  2.5770e+00,  6.3825e-01,  ...,  1.8039e-01,
           1.1998e+00, -4.2486e-01],
         [-1.0350e+00,  9.5924e-01,  1.3108e+00,  ..., -1.3460e-01,
          -4.8026e-03, -1.6929e+00],
         [-1.0741e+00,  3.7955e+00,  8.7077e-01,  ..., -2.4168e-02,
           4.7581e-01, -7.1922e-01]],

        [[-1.2058e-01,  1.8629e+00,  1.5173e+00,  ...,  3.8348e-01,
           2.0914e-01, -2.6661e-01],
         [-1.9341e-01,  2.6827e+00,  5.8488e-01,  ..., -5.7149e-01,
           9.4352e-01,  9.4295e-01],
         [-9.2054e-01,  8.0677e-01,  7.6001e-01,  ...,  2.3089e-01,
           3.7600e-01, -1.3633e+00],
         [-6.1648e-01,  3.1318e+00,  1.6328e+00,  ..., -4.0053e-01,
           6.5669e-01, -7.7174e-01]],

        [[ 6.4130e-01,  1.8684e+00,  1.9250e+00,  ..., -4.2724e-01,
           5.2908e-01,  5.7330e-03],
         [-3.4070e-01,  2.8894e+00,  8.9903e-01,  ..., -8.7169e-01,
           7.0276e-01,  7.8615e-02],
         [-1.3166e+00,  1.9303e+00,  1.5258e+00,  ..., -1.0427e-01,
           5.6486e-01, -1.3342e+00],
         [-3.4954e-01,  3.3095e+00,  5.5747e-01,  ..., -8.8542e-02,
           6.7319e-01, -7.0619e-01]],

        ...,

        [[-1.4448e-01,  1.5401e+00,  1.6079e+00,  ..., -1.3459e-01,
           6.8021e-01,  2.3491e-01],
         [-2.4981e-01,  2.4745e+00,  4.3614e-01,  ..., -3.8554e-01,
           4.8121e-01,  6.5069e-01],
         [-1.1020e+00,  1.7326e+00,  6.8406e-01,  ..., -9.0591e-01,
           6.9746e-02, -1.1886e+00],
         [-1.1229e+00,  3.3809e+00,  9.9112e-01,  ..., -4.2508e-01,
          -1.5073e-01, -2.8892e-01]],

        [[ 1.3155e-01,  1.6988e+00,  1.4695e+00,  ..., -2.6786e-01,
           3.2670e-01, -2.8595e-01],
         [-6.2924e-01,  2.2744e+00,  8.3256e-01,  ..., -5.2757e-01,
           6.2548e-01,  8.4485e-01],
         [-7.7928e-01,  1.1324e+00,  9.9825e-01,  ..., -2.9382e-02,
           6.6695e-01, -1.2308e+00],
         [-1.0955e+00,  2.5664e+00,  1.3470e+00,  ..., -3.1476e-01,
           3.0787e-01, -5.3756e-01]],

        [[ 1.2194e-01,  1.8805e+00,  2.0630e+00,  ...,  4.5050e-02,
           9.4328e-01, -4.3165e-01],
         [-9.0848e-01,  3.1963e+00,  6.2561e-01,  ...,  3.6921e-03,
           1.1658e+00,  8.2265e-01],
         [-1.2452e+00,  1.8697e+00,  1.3285e+00,  ..., -1.1881e-01,
           5.7125e-01, -7.1532e-01],
         [-1.0483e+00,  2.6069e+00,  1.5614e+00,  ..., -2.3495e-01,
           9.3524e-02, -5.0111e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.4310,  0.3961,  0.4929,  ...,  0.1319,  0.5499,  0.0724],
         [ 0.2309,  1.0871,  0.1056,  ...,  0.3394, -0.0241, -1.3085],
         [-0.0044,  0.2651,  0.9861,  ...,  0.0689,  0.1232, -0.5689],
         [-0.4435,  2.2512,  0.9375,  ..., -0.7139,  0.0706, -1.1915]],

        [[-0.7661,  0.2462,  0.2589,  ..., -0.1763,  0.4429, -0.6123],
         [ 0.7160,  0.8372, -0.2234,  ..., -0.1651,  0.3898, -0.4605],
         [ 0.2935,  0.1780,  0.8908,  ...,  0.7609, -0.5503, -0.6448],
         [-0.1582,  1.7508,  1.1171,  ...,  0.0664,  0.8919, -1.2018]],

        [[ 0.7038,  0.6803,  0.4833,  ..., -0.3164, -0.7260, -0.7107],
         [ 0.7139,  0.7709,  0.7238,  ..., -0.9599,  0.1307, -1.2131],
         [-0.2183,  1.1338,  1.7209,  ...,  0.0712,  0.2967, -0.5622],
         [ 0.3418,  1.6322,  0.3935,  ..., -0.1606, -0.2804, -1.1925]],

        ...,

        [[-0.5519, -0.4605,  0.7717,  ..., -0.4750,  0.3749, -0.2153],
         [ 0.5483,  0.4920, -0.0668,  ...,  0.0834,  0.1667, -0.7661],
         [-0.7017,  1.3063,  0.6899,  ..., -0.4338,  0.1759, -0.2507],
         [-0.6588,  1.7844,  0.3791,  ..., -0.1358, -0.9010, -0.7083]],

        [[-0.0300,  0.3704, -0.0117,  ..., -0.0660, -0.2558, -1.2143],
         [-0.9368,  0.9750,  0.3470,  ...,  0.2242,  0.2093, -0.7586],
         [ 0.2175,  0.7444,  1.2160,  ...,  0.1150,  0.8782, -0.4738],
         [-0.1441,  0.9385,  1.2067,  ..., -0.1947, -0.3246, -0.6834]],

        [[-0.1471,  0.7389,  0.9177,  ..., -0.2291,  0.6179, -1.3414],
         [-0.8245,  0.7263,  0.2496,  ...,  0.3668,  0.3954, -0.4458],
         [-0.2265,  1.3289,  1.0164,  ...,  0.1529,  0.1336, -0.1594],
         [-0.1373,  1.4022,  0.9738,  ..., -0.2485,  0.1390, -1.1381]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6604, -2.0269, -0.6134,  ..., -0.7481,  0.2990,  1.2162],
         [-0.7722, -0.7675, -0.3465,  ..., -0.3922,  1.0288, -0.3479],
         [-1.0901, -0.7961,  0.3618,  ..., -0.7463, -0.6033,  0.8305],
         [-1.1529,  1.2757,  0.7208,  ..., -1.0047, -0.2054, -0.2820]],

        [[-1.3823, -2.1723, -0.2679,  ..., -0.8258,  1.0963,  0.2796],
         [ 0.5057,  1.0367, -0.9345,  ..., -0.1790,  0.5296, -0.5634],
         [-0.5860, -0.4584,  0.5169,  ..., -0.0272, -0.8272,  0.6454],
         [-1.1295, -0.4335,  0.7925,  ..., -0.3741,  0.9456, -0.6125]],

        [[ 0.3782, -0.2863, -0.5303,  ..., -1.4713,  0.2304,  0.2216],
         [-0.7482, -0.8304, -0.1192,  ..., -1.4480,  0.0745, -0.2787],
         [-0.6594, -0.0711,  0.9063,  ..., -0.3710, -0.1423,  0.7240],
         [-0.4883, -0.7216,  0.0272,  ..., -0.8637, -0.0306, -0.3125]],

        ...,

        [[-0.9585, -2.5761, -0.4232,  ..., -1.6775, -0.5593, -0.3515],
         [-0.4265, -0.4792, -0.7074,  ..., -0.8645, -0.0600, -0.3732],
         [-1.5515,  0.0112, -0.0746,  ..., -0.7543, -0.3126,  0.9708],
         [-1.5939, -0.4155,  0.2594,  ..., -0.2721,  0.4407, -0.3264]],

        [[-0.7564, -2.0584, -0.7860,  ..., -0.9793, -0.1114, -0.3257],
         [-1.3217, -0.2872, -0.4036,  ..., -0.7494,  1.1900, -0.2510],
         [-0.8573,  0.4574,  0.8791,  ..., -0.1331, -0.0243,  0.6254],
         [-0.9877, -0.7516,  0.5629,  ..., -0.8118, -0.6873, -0.2371]],

        [[-0.7383, -1.8352, -0.0271,  ..., -0.9868, -0.0433, -0.1728],
         [-1.0128, -0.5811, -0.4187,  ..., -0.0379,  0.2420,  0.1914],
         [-0.9417, -0.1682,  0.2405,  ..., -0.7449,  0.5920,  0.8445],
         [-1.6474,  0.7524,  0.3695,  ..., -0.7714,  0.1910, -0.3380]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1486e+00, -4.3857e-01, -1.3752e+00,  ..., -1.6215e+00,
          -3.3676e-01,  9.0806e-01],
         [-1.3734e+00,  1.9184e-02, -1.2107e+00,  ..., -6.6844e-01,
           1.2963e-01,  4.6426e-01],
         [-3.6509e-01, -4.1741e-01, -9.6738e-02,  ..., -1.6841e+00,
          -9.0431e-01,  5.0704e-01],
         [-1.8890e+00,  1.1765e+00, -4.2938e-01,  ..., -7.6101e-01,
          -1.6080e+00, -1.1746e-01]],

        [[-2.6870e+00, -3.2398e-01, -1.1071e+00,  ..., -7.8833e-01,
          -2.0099e-01,  8.2861e-01],
         [-9.0986e-01,  1.6223e+00, -1.0641e+00,  ..., -4.7151e-01,
          -6.5950e-01,  3.1095e-02],
         [-4.6754e-01, -6.1686e-01, -1.2198e-02,  ..., -1.0233e+00,
          -1.3951e+00, -9.6070e-02],
         [-1.8135e+00,  1.8260e-02, -4.9520e-01,  ..., -3.4835e-01,
          -8.2142e-01, -2.7024e-01]],

        [[-1.0504e+00,  2.0814e-01, -1.0471e+00,  ..., -1.1348e+00,
           8.1383e-01,  1.4099e-01],
         [-1.9367e+00,  2.6733e-01, -8.6699e-01,  ..., -1.0255e+00,
          -5.1892e-02,  4.5362e-01],
         [-5.1593e-01,  4.7789e-02,  5.4576e-02,  ..., -1.4776e+00,
          -5.0242e-01, -1.2680e-02],
         [-1.6739e+00,  9.3377e-02, -9.3449e-01,  ..., -4.8724e-01,
          -1.3213e+00, -1.9949e-01]],

        ...,

        [[-1.7985e+00, -1.2220e+00, -1.1920e+00,  ..., -1.5653e+00,
          -7.8066e-01,  1.3014e-01],
         [-1.0286e+00,  5.4243e-01, -1.2439e+00,  ..., -9.9395e-01,
          -1.1043e+00, -1.5306e-01],
         [-1.3974e+00,  1.8244e-02, -7.0678e-01,  ..., -1.6458e+00,
          -8.3623e-01,  1.0567e-01],
         [-2.1468e+00, -8.8303e-03,  3.1168e-01,  ..., -1.4092e-01,
          -8.4526e-02,  1.7305e-02]],

        [[-2.2902e+00, -3.2409e-01, -5.1933e-01,  ..., -1.6839e+00,
          -1.0340e+00,  9.8725e-02],
         [-1.3567e+00,  4.4107e-01, -7.4582e-01,  ..., -4.9233e-01,
           4.0404e-02,  3.1741e-01],
         [-9.9283e-01,  3.6839e-01,  4.1049e-01,  ..., -1.1939e+00,
          -6.5638e-01,  1.8727e-01],
         [-1.9040e+00, -3.4012e-01, -3.6591e-01,  ..., -5.4301e-01,
          -4.9109e-01, -4.9051e-03]],

        [[-1.9981e+00, -4.5888e-01, -8.2830e-01,  ..., -9.5918e-01,
          -4.2947e-01,  5.8556e-01],
         [-1.4652e+00,  3.3976e-01, -1.3046e+00,  ..., -2.3519e-01,
          -7.0349e-01,  7.1530e-01],
         [-5.8841e-01,  1.6921e-03, -5.9717e-01,  ..., -1.3129e+00,
           1.4653e-01, -1.9108e-01],
         [-1.2949e+00,  8.1331e-01, -1.4114e-01,  ..., -3.0153e-01,
          -1.4549e+00,  1.3079e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
Epoch: [0]  [    0/29571]  eta: 10:37:16  lr: 0.000100  class_error: 100.00  loss: 63.4273 (63.4273)  loss_ce: 4.4362 (4.4362)  loss_bbox: 4.1006 (4.1006)  loss_giou: 1.7867 (1.7867)  loss_ce_0: 4.6170 (4.6170)  loss_bbox_0: 4.0462 (4.0462)  loss_giou_0: 1.8064 (1.8064)  loss_ce_1: 4.5336 (4.5336)  loss_bbox_1: 4.0748 (4.0748)  loss_giou_1: 1.7959 (1.7959)  loss_ce_2: 4.8770 (4.8770)  loss_bbox_2: 4.1079 (4.1079)  loss_giou_2: 1.7818 (1.7818)  loss_ce_3: 4.8290 (4.8290)  loss_bbox_3: 4.1143 (4.1143)  loss_giou_3: 1.7796 (1.7796)  loss_ce_4: 4.7930 (4.7930)  loss_bbox_4: 4.1661 (4.1661)  loss_giou_4: 1.7814 (1.7814)  loss_ce_unscaled: 4.4362 (4.4362)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.8201 (0.8201)  loss_giou_unscaled: 0.8933 (0.8933)  cardinality_error_unscaled: 94.2500 (94.2500)  loss_ce_0_unscaled: 4.6170 (4.6170)  loss_bbox_0_unscaled: 0.8092 (0.8092)  loss_giou_0_unscaled: 0.9032 (0.9032)  cardinality_error_0_unscaled: 95.0000 (95.0000)  loss_ce_1_unscaled: 4.5336 (4.5336)  loss_bbox_1_unscaled: 0.8150 (0.8150)  loss_giou_1_unscaled: 0.8979 (0.8979)  cardinality_error_1_unscaled: 95.0000 (95.0000)  loss_ce_2_unscaled: 4.8770 (4.8770)  loss_bbox_2_unscaled: 0.8216 (0.8216)  loss_giou_2_unscaled: 0.8909 (0.8909)  cardinality_error_2_unscaled: 95.0000 (95.0000)  loss_ce_3_unscaled: 4.8290 (4.8290)  loss_bbox_3_unscaled: 0.8229 (0.8229)  loss_giou_3_unscaled: 0.8898 (0.8898)  cardinality_error_3_unscaled: 95.0000 (95.0000)  loss_ce_4_unscaled: 4.7930 (4.7930)  loss_bbox_4_unscaled: 0.8332 (0.8332)  loss_giou_4_unscaled: 0.8907 (0.8907)  cardinality_error_4_unscaled: 95.0000 (95.0000)  time: 1.2930  data: 0.2821  max mem: 4285
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([910, 4, 256]), pos_embed.shape: torch.Size([910, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
encoder start :
	 src.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([910, 4, 256]), k.shape: torch.Size([910, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([910, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([910, 4, 256])
	 (after FFN) src2.shape: torch.Size([910, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([910, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.1232, -0.3070,  0.1810,  ..., -0.5672,  2.4976, -0.5871],
         [-1.7954,  0.4070,  0.6869,  ...,  0.3531,  1.5997, -1.0920],
         [-1.4691, -0.6523,  0.1364,  ..., -0.2364,  1.5917,  0.2791],
         [-0.2851,  0.4913,  1.7502,  ..., -0.1473,  2.0197,  0.1405]],

        [[-0.4289,  0.7114,  0.3376,  ..., -0.0223,  2.3610, -0.5017],
         [-1.5691,  0.4838, -0.2165,  ...,  0.2028,  1.7096, -0.6346],
         [-1.0337,  0.2234,  0.3669,  ..., -0.1468,  1.6561,  0.0420],
         [-1.7444,  0.4816,  1.4431,  ..., -0.4841,  1.7786,  0.7371]],

        [[-0.5375,  0.6366,  0.8135,  ...,  0.0062,  0.3520, -0.3477],
         [-2.0243,  0.5432,  1.2329,  ...,  0.3250,  1.4060, -0.4074],
         [-1.2177, -0.4883,  0.5136,  ..., -0.4227,  1.3707,  0.0413],
         [-1.6032,  0.4854,  0.9550,  ..., -0.1719,  0.1750,  0.3961]],

        ...,

        [[-1.3071,  1.2088, -0.2162,  ..., -0.2710,  2.7489, -0.5566],
         [-0.4490,  0.4884,  1.2000,  ...,  0.3972,  1.8542, -0.9077],
         [-0.3118, -0.5621,  0.5508,  ..., -0.5464,  1.5418, -0.1020],
         [-2.3681,  0.5731, -0.1991,  ..., -0.2021,  1.9776,  0.1306]],

        [[-1.3271,  0.6392,  0.1979,  ...,  0.0887,  2.8152, -0.3489],
         [-2.0860,  0.4754,  1.2754,  ...,  0.2955,  0.2303, -0.6817],
         [-1.2738, -0.2848,  0.2797,  ..., -0.3334,  1.9732, -0.1180],
         [-2.0926,  0.4184,  1.1731,  ..., -0.4685,  1.8304,  0.2571]],

        [[-0.7642,  0.7251,  1.0268,  ..., -0.4777,  2.8226, -0.3113],
         [-0.2304, -0.5852,  1.1760,  ...,  0.5614,  1.8622,  0.1518],
         [-0.9496, -0.6739, -0.1770,  ..., -0.2514,  1.5452,  0.0379],
         [-2.2226,  0.6405,  1.3444,  ..., -0.1863,  2.2301,  0.1464]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.8477,  0.5596,  1.2434,  ...,  0.5722,  1.4735, -0.5566],
         [-1.8206,  1.4459,  1.3094,  ...,  0.1651,  1.0213, -0.0607],
         [-1.9179,  1.1325,  1.5978,  ..., -0.2146,  0.8888,  0.4093],
         [-0.5437,  1.2790,  1.8223,  ..., -0.3058,  0.9601,  0.6132]],

        [[-0.1960,  0.7764,  0.9569,  ...,  0.0903,  1.1994, -0.3563],
         [-0.9948,  0.7900,  1.1825,  ..., -0.3439,  0.8398,  0.1341],
         [-1.6769,  1.3738,  1.8956,  ...,  0.6595,  1.3571,  0.2083],
         [-1.4428,  0.8352,  1.5546,  ..., -0.0313,  0.5490,  0.6281]],

        [[-0.1649,  1.1221,  1.5822,  ...,  0.1455,  0.3576, -0.2791],
         [-1.1653,  0.7710,  1.4348,  ..., -0.3469,  0.8938, -0.0283],
         [-1.5178,  0.7453,  1.6380,  ..., -0.5181,  0.8298,  0.0042],
         [-1.5517,  1.8191,  1.5047,  ..., -0.1015,  0.1545,  0.6202]],

        ...,

        [[-0.8818,  1.7750,  1.0745,  ..., -0.2451,  1.5484, -0.4090],
         [-0.8430,  0.7190,  1.3281,  ...,  0.2701,  0.8955,  0.2042],
         [-1.2874,  0.8343,  0.5281,  ..., -0.2702,  1.3348,  0.0512],
         [-1.4039,  1.4071,  0.8123,  ..., -0.2500,  0.6934,  0.7787]],

        [[-0.9547,  1.5309,  1.3935,  ...,  0.0922,  1.7724,  0.1042],
         [-1.6383,  1.2016,  1.7122,  ..., -0.1496,  0.1044, -0.0867],
         [-1.6017,  1.4311,  1.4979,  ...,  0.2453,  0.8071, -0.2997],
         [-2.1992,  1.1599,  1.8849,  ..., -0.4881,  0.6295,  0.5840]],

        [[-0.7411,  1.3574,  1.6355,  ...,  0.0169,  1.1239, -0.4585],
         [-0.6120,  0.4760,  1.4479,  ...,  0.3284,  1.2057,  0.1857],
         [-1.4013,  0.6795,  1.3787,  ...,  0.0024,  0.9645,  0.1906],
         [-1.5665,  1.3047,  1.7100,  ..., -0.4529,  1.2304,  0.6678]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-7.1329e-01, -4.4727e-01,  6.8616e-01,  ...,  1.8639e-01,
           7.8613e-01, -2.2998e+00],
         [-1.2660e+00, -4.2169e-03,  4.5036e-01,  ..., -6.9699e-01,
           1.5116e+00, -3.1753e+00],
         [-1.0219e+00,  4.7648e-01,  1.3261e+00,  ...,  3.0945e-01,
           9.4351e-01, -1.2711e+00],
         [-5.3783e-01, -2.0180e-01,  1.0674e+00,  ..., -6.8380e-01,
           1.5470e+00, -1.7314e+00]],

        [[-4.9876e-01, -4.7812e-01,  3.9754e-01,  ...,  1.8396e-01,
           1.7214e+00, -2.8215e+00],
         [-4.1174e-01, -2.8899e-01,  4.5984e-01,  ..., -6.3996e-01,
           1.3005e+00, -2.7471e+00],
         [-7.8313e-01,  9.2985e-04,  1.6116e+00,  ...,  6.0090e-01,
           1.1492e+00, -1.3937e+00],
         [-1.1891e+00, -4.0193e-01,  1.0503e+00,  ..., -1.0237e-01,
           1.0628e+00, -2.6207e+00]],

        [[-2.8977e-01, -4.6587e-01,  4.8703e-01,  ...,  4.4160e-01,
           4.9654e-01, -2.8814e+00],
         [-1.1399e+00, -2.8268e-01,  7.8317e-01,  ..., -9.3545e-01,
           1.6577e+00, -2.8742e+00],
         [-7.2918e-01,  1.1782e-01,  1.3164e+00,  ..., -4.6735e-02,
           7.8030e-01, -2.5037e+00],
         [-8.5692e-01, -4.6282e-02,  6.6963e-01,  ..., -2.6301e-01,
           9.7524e-01, -2.8704e+00]],

        ...,

        [[-3.8393e-01, -9.8545e-02,  2.4085e-01,  ..., -4.8543e-01,
           7.6445e-01, -3.1271e+00],
         [-8.8785e-01, -1.8017e-01,  7.6082e-01,  ..., -7.9902e-01,
           1.1933e+00, -2.6862e+00],
         [-7.3388e-01,  9.0156e-02,  9.9060e-01,  ...,  2.6392e-01,
           1.5290e+00, -1.5775e+00],
         [-5.8416e-01, -5.8572e-01,  5.1782e-01,  ..., -8.2796e-01,
           1.0647e+00, -2.3822e+00]],

        [[-1.0305e+00, -1.7112e-01,  3.4476e-01,  ...,  9.9329e-02,
           1.7238e+00, -3.1533e+00],
         [-1.1118e+00,  4.5533e-01,  8.4309e-01,  ..., -4.3194e-01,
           1.2036e+00, -2.7495e+00],
         [-1.0230e+00,  7.6753e-01,  1.7041e+00,  ...,  9.3759e-02,
          -3.2105e-02, -2.3997e+00],
         [-1.3293e+00, -4.8170e-01,  1.0316e+00,  ..., -6.7680e-01,
           1.2346e+00, -2.6887e+00]],

        [[-7.6109e-01, -6.5938e-02,  5.5370e-01,  ..., -8.3958e-02,
           6.2990e-01, -3.1709e+00],
         [-6.5429e-01, -1.7052e-01,  7.1347e-01,  ..., -4.2726e-01,
           1.6475e+00, -2.7142e+00],
         [-9.5742e-01,  1.3489e-01,  1.3839e+00,  ...,  7.1099e-01,
           1.2447e+00, -1.5867e+00],
         [-9.9414e-01, -5.8060e-01,  9.5535e-01,  ..., -5.3745e-01,
           1.9217e+00, -2.8816e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9534, -1.2963, -0.1738,  ..., -1.3051,  1.5936, -0.8184],
         [-1.8727, -0.6244, -0.7291,  ..., -1.2998,  2.2290, -0.8330],
         [-1.0618, -1.2310,  0.1316,  ..., -0.7681,  1.9348,  0.3766],
         [-1.4028, -2.3733,  1.0161,  ..., -1.5040,  2.0025, -0.5932]],

        [[-0.2000, -2.3476, -0.0221,  ..., -0.9170,  2.5032, -1.5462],
         [-1.1303, -1.8609, -0.0675,  ..., -0.9762,  1.9522, -0.1030],
         [-1.7077, -0.2550,  0.5863,  ..., -0.0510,  2.5627,  0.1446],
         [-1.8101, -2.3086,  0.5436,  ..., -0.3928,  1.8680, -0.9668]],

        [[-0.9685, -2.2809, -0.0075,  ..., -1.0444,  1.9696, -1.5686],
         [-1.2060, -1.8591, -0.0296,  ..., -2.1436,  3.2604, -0.7984],
         [-1.1543, -1.0857,  0.8088,  ..., -0.8648,  2.4051,  0.0352],
         [-1.6534, -2.5788,  0.5082,  ..., -1.1624,  0.1545, -1.3195]],

        ...,

        [[-0.7051, -2.1345,  0.2324,  ..., -1.6365,  1.9753, -1.6409],
         [-1.5551, -1.9128,  0.3531,  ..., -1.6739,  2.4070, -0.3985],
         [-1.6739, -1.5079,  0.4607,  ..., -0.5946,  1.2622,  0.5213],
         [-1.6474, -2.3421,  0.0957,  ..., -1.4679,  1.8864, -1.2415]],

        [[-0.9443, -1.7300,  0.0218,  ..., -1.2884,  2.4526, -1.9070],
         [-1.7068, -1.6389,  0.0961,  ..., -1.2173,  2.1396, -0.2431],
         [-1.3726, -0.8827,  1.0794,  ..., -0.6128,  1.7369, -0.0859],
         [-1.9956, -2.3388,  0.4605,  ..., -1.8250,  1.7136, -1.1198]],

        [[-0.7528, -1.0802,  0.1212,  ..., -1.0784,  2.0216, -1.2174],
         [-0.6443, -1.4494,  0.1657,  ..., -1.3890,  2.3664, -0.7525],
         [-1.3279, -0.5284,  0.3802,  ..., -0.4859,  2.4987, -0.6367],
         [-1.5142, -2.7785,  0.6185,  ..., -1.4799,  2.4877, -1.1812]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-9.7990e-01, -7.7238e-01, -7.6046e-01,  ..., -9.4724e-01,
          -5.0406e-01,  5.9836e-01],
         [-2.1340e+00, -7.1508e-01, -1.0370e+00,  ..., -1.7458e+00,
           1.2969e-01,  7.7330e-01],
         [-8.6302e-01, -8.2820e-01, -2.9379e-01,  ..., -8.5029e-01,
          -4.8543e-01,  1.1199e+00],
         [-1.8958e+00, -1.4865e+00, -5.3556e-01,  ..., -1.3449e+00,
           7.3327e-02,  5.1004e-01]],

        [[-6.1783e-01, -1.7714e+00, -6.4218e-01,  ..., -8.3437e-01,
           1.6197e-01,  1.0836e-01],
         [-1.5070e+00, -1.4679e+00, -8.5518e-01,  ..., -1.5389e+00,
          -2.3715e-01,  9.8207e-01],
         [-1.0890e+00, -4.5323e-01, -7.2062e-02,  ..., -6.9402e-01,
           1.8395e-02,  1.2788e-01],
         [-1.7638e+00, -1.7843e+00, -2.2855e-01,  ..., -8.0550e-01,
           3.9028e-02,  6.5347e-01]],

        [[-1.3273e+00, -1.8398e+00, -1.2366e+00,  ..., -9.8472e-01,
          -9.8334e-02, -1.1662e+00],
         [-1.4004e+00, -1.4763e+00,  2.4461e-01,  ..., -2.0194e+00,
           1.0354e+00,  5.2878e-01],
         [-1.0790e+00, -6.4350e-01, -5.2595e-01,  ..., -1.3834e+00,
           1.0875e-01,  5.8969e-01],
         [-1.7274e+00, -1.5652e+00, -1.1320e+00,  ..., -1.0264e+00,
          -7.5842e-01,  2.7824e-01]],

        ...,

        [[-9.2722e-01, -1.7123e+00, -7.5758e-01,  ..., -1.6820e+00,
          -3.9570e-02,  1.5496e-03],
         [-1.7760e+00, -1.1104e+00, -1.0857e+00,  ..., -1.5432e+00,
           5.7102e-01,  7.3950e-01],
         [-1.0008e+00, -9.4791e-01, -5.9371e-01,  ..., -1.2118e+00,
          -1.8575e-01,  6.0853e-01],
         [-1.3074e+00, -1.4972e+00, -8.6769e-01,  ..., -1.6932e+00,
           1.9727e-01,  7.8758e-02]],

        [[-1.0822e+00, -1.4981e+00, -9.4573e-01,  ..., -1.3320e+00,
          -2.9123e-02, -1.4567e+00],
         [-1.9791e+00, -1.1393e+00, -8.1573e-01,  ..., -1.1949e+00,
           8.6962e-02,  1.2150e+00],
         [-1.1969e+00, -4.1464e-01,  1.6386e-01,  ..., -1.2753e+00,
           2.7048e-01,  4.7896e-01],
         [-1.6711e+00, -1.3089e+00, -7.7361e-01,  ..., -1.3815e+00,
          -3.6981e-01,  3.4260e-01]],

        [[-1.0666e+00, -1.1404e+00, -7.5975e-01,  ..., -1.2851e+00,
           2.5150e-01,  4.6724e-01],
         [-1.0908e+00, -1.3195e+00, -7.9869e-01,  ..., -1.6506e+00,
           2.9090e-01,  1.1999e+00],
         [-8.5395e-01, -5.4532e-01, -6.7700e-01,  ..., -1.5519e+00,
           4.1407e-01,  1.4789e-01],
         [-2.0149e+00, -1.7197e+00,  4.6259e-01,  ..., -1.0648e+00,
           5.9360e-01,  1.1082e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([910, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([999, 4, 256]), pos_embed.shape: torch.Size([999, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
encoder start :
	 src.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([999, 4, 256]), k.shape: torch.Size([999, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([999, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([999, 4, 256])
	 (after FFN) src2.shape: torch.Size([999, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([999, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.6632,  0.4358,  1.3708,  ..., -0.9994,  1.7970, -1.1327],
         [-1.6284,  0.6236,  1.4662,  ..., -1.2396,  1.5796, -0.7795],
         [-1.5029, -0.1556,  0.4755,  ..., -1.7774,  2.3908, -1.1709],
         [-0.9461,  0.1931,  1.0547,  ..., -0.6587,  1.9092, -0.5398]],

        [[-1.5982, -0.1400,  1.0648,  ..., -1.1445,  2.3529, -0.8105],
         [-1.9388, -0.3525, -0.4872,  ..., -1.7343,  2.0996, -0.5962],
         [-1.2790,  0.8148,  0.3474,  ..., -0.6525,  2.2316, -0.4500],
         [-0.6169,  0.0863,  1.1661,  ..., -0.3875,  1.0968, -0.7421]],

        [[-1.6639,  1.1187,  1.5304,  ..., -1.1773,  2.1289, -0.9120],
         [-1.9211, -0.0838,  0.8366,  ..., -1.3757,  2.0807, -0.6196],
         [-1.1103,  0.1625,  1.0913,  ..., -1.4949,  2.4447, -0.6920],
         [-0.8175,  0.6975, -0.5812,  ..., -1.2262,  1.9487, -0.8354]],

        ...,

        [[-1.7495,  0.4226,  1.4861,  ..., -1.2814,  2.1011, -1.0411],
         [-2.0125,  0.0680,  1.3878,  ..., -1.5491,  1.2032, -0.5621],
         [-0.9258,  0.7938, -0.4702,  ..., -1.8989,  2.4164, -0.4337],
         [-1.1184,  0.4740,  1.1920,  ..., -0.3807,  1.9070, -0.8125]],

        [[-1.5706,  1.1229,  0.9262,  ..., -0.9704,  2.0206, -0.9861],
         [-0.2632,  0.1042,  0.7698,  ..., -1.6588,  1.9468, -0.8734],
         [-1.2413,  0.3644,  0.5661,  ..., -1.8674,  2.5652, -0.5178],
         [-1.2627, -0.3227,  0.8930,  ..., -1.0783,  1.9789, -0.5104]],

        [[-1.7467,  0.5879,  1.3956,  ..., -1.3746,  2.1354, -0.4808],
         [-1.8665,  0.4618,  0.9259,  ..., -0.3423,  1.5713, -0.6873],
         [-1.1771,  0.4567,  0.9078,  ..., -1.4103,  2.4355, -0.0045],
         [-0.8715,  0.7935,  0.8590,  ..., -0.9116,  2.0385, -0.7629]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6875,  0.6750,  0.8908,  ..., -1.4771,  1.2948, -0.4933],
         [-0.6314,  1.1174,  0.7875,  ..., -1.5532,  0.7064, -0.0253],
         [-1.0585,  0.1867,  0.1315,  ..., -1.4027,  1.0082, -0.2953],
         [-0.5175,  0.3846,  1.1393,  ..., -1.3004,  1.2114, -0.1018]],

        [[-0.9753,  0.5635,  0.9306,  ..., -1.9561,  0.7649,  0.0235],
         [-0.8840,  0.6180,  0.0065,  ..., -1.5732,  0.8993, -0.0100],
         [-0.6136,  0.9408, -0.1499,  ..., -0.6611,  1.1526,  0.0543],
         [-0.5848,  0.4402,  0.9746,  ..., -1.0877,  0.5701, -0.2412]],

        [[-0.6717,  1.0066,  1.1332,  ..., -1.6331,  1.0186, -0.0073],
         [-0.7301,  0.6698,  0.2213,  ..., -1.7861,  0.7495,  0.2127],
         [-0.5428,  0.7368,  0.3318,  ..., -0.5136,  1.2035,  0.0244],
         [-0.0635,  0.8649,  0.0763,  ..., -1.2692,  1.0574, -0.5015]],

        ...,

        [[-0.9611,  0.8207,  1.0961,  ..., -1.0141,  0.7273, -0.4752],
         [-1.0549,  0.6881,  1.1046,  ..., -1.2269,  0.4322, -0.2276],
         [-0.2139,  1.0411, -0.3243,  ..., -1.9018,  1.1958, -0.0168],
         [-0.4134,  0.8444,  0.8450,  ..., -0.6490,  1.1198, -0.2970]],

        [[-0.8545,  1.2089,  0.6375,  ..., -1.8645,  0.9660, -0.0714],
         [-0.2402,  0.5144,  0.3074,  ..., -1.8350,  0.8510,  0.0105],
         [-0.5658,  0.5925, -0.1244,  ..., -1.5224,  1.4454, -0.0981],
         [-0.7307,  0.1486,  0.0953,  ..., -1.6223,  1.1647, -0.3569]],

        [[-0.8637,  0.5954,  0.2173,  ..., -1.4725,  0.9562,  0.0531],
         [-1.1744,  0.3514,  1.0794,  ..., -1.3165,  0.7678, -0.2566],
         [-0.9953,  0.4877,  0.5257,  ..., -1.6114,  1.1683, -0.0086],
         [-0.4060,  0.4673,  0.8754,  ..., -0.5786,  1.0408, -0.2373]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.1803, -0.5027,  0.6163,  ..., -0.6709,  1.4913, -3.1151],
         [-0.0710, -0.4762,  0.5043,  ..., -1.1532,  0.2596, -3.2070],
         [-0.6358, -0.0692,  0.3040,  ..., -0.2376,  0.5805, -1.9802],
         [-0.4634, -0.8442,  0.4199,  ..., -0.8400,  1.3839, -3.5132]],

        [[-0.7395, -0.3701,  0.2782,  ..., -1.5978,  1.1862, -3.6366],
         [-0.5906,  0.1009, -0.1736,  ..., -1.6234,  1.0931, -2.2605],
         [-0.8320, -0.6200,  0.3582,  ..., -0.0726,  1.2175, -3.1620],
         [-0.3632, -0.7604, -0.1027,  ..., -0.3415,  0.8617, -3.5681]],

        [[-0.3288, -0.3065,  0.4971,  ..., -1.2035,  1.2826, -3.2763],
         [-1.1769, -0.6790,  0.0704,  ..., -0.9367,  1.0029, -2.9709],
         [-0.2693, -0.1238, -0.1487,  ..., -0.1869,  0.2353, -3.3409],
         [-0.2933, -0.1913, -0.0619,  ..., -0.7169,  1.3465, -3.2239]],

        ...,

        [[-0.2365,  0.4083, -0.4167,  ..., -1.1067,  1.2606, -2.4780],
         [-0.7029, -0.8982,  0.5370,  ..., -0.9984, -0.0299, -3.3601],
         [ 0.0511, -0.4787, -0.4757,  ..., -0.6553,  0.4857, -3.2200],
         [ 0.0064, -0.1068,  0.7852,  ..., -0.4314,  1.7791, -1.1516]],

        [[-0.3768, -0.0592,  0.1151,  ..., -1.6404,  1.5248, -1.0172],
         [-0.2119, -1.1044,  0.3583,  ..., -1.0434,  1.2710, -2.8103],
         [-0.3892, -0.3619, -0.1047,  ..., -0.7392,  1.3695, -3.3359],
         [-0.5035, -0.6366, -0.0837,  ..., -0.9474,  0.7654, -3.5737]],

        [[-0.2716, -0.3056,  0.7077,  ..., -1.2640,  1.2776, -3.5796],
         [-1.0805, -0.8847, -0.2425,  ..., -1.0478,  0.4355, -2.3157],
         [-0.6940, -0.9795,  0.3471,  ..., -1.1824, -0.0975, -2.5293],
         [ 0.1444, -0.6465,  0.4403,  ..., -0.4319,  1.1868, -3.1738]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.8944, -1.8267, -0.0074,  ..., -1.8702,  1.4300, -0.9199],
         [-1.2767, -2.1126,  0.2462,  ..., -2.2760,  1.2131, -1.3714],
         [-1.4557, -2.1352,  0.3400,  ..., -1.1454,  1.0517, -0.5741],
         [-1.0994, -3.1538,  0.3411,  ..., -1.1086,  1.2065, -1.6044]],

        [[-0.9642, -2.4619,  0.1953,  ..., -2.4562,  1.3634, -1.6060],
         [-1.1252, -2.0810,  0.1470,  ..., -2.5374,  1.9010, -1.1203],
         [-1.1847, -1.8249,  0.1544,  ..., -1.1957,  1.8771, -1.6226],
         [-1.0427, -2.3505,  0.2020,  ..., -1.0222,  1.5864, -1.6804]],

        [[-1.2166, -3.1239,  0.3737,  ..., -2.3454,  0.9112, -1.6824],
         [-1.5302, -2.4711,  0.1773,  ..., -2.2095,  1.4888, -1.4080],
         [-0.8844, -2.1638,  0.0492,  ..., -1.7579,  1.4058, -1.5915],
         [-0.9712, -1.5373,  0.0692,  ..., -1.7254,  0.2429, -1.5577]],

        ...,

        [[-1.1506, -1.7635, -0.7256,  ..., -2.4754,  1.7389, -0.7691],
         [-1.0635, -2.3856,  0.4856,  ..., -1.8036,  0.6127, -1.7602],
         [-0.1791, -1.8360, -0.4418,  ..., -0.8698,  1.3685, -1.7023],
         [-0.8709, -2.3111,  0.1900,  ..., -0.8576,  2.0362, -0.0786]],

        [[-0.8528, -2.2615, -0.6046,  ..., -2.4467,  1.7870, -0.2361],
         [-0.6039, -3.3095,  0.2881,  ..., -2.2747,  1.7016, -1.5530],
         [-1.2163, -1.7274, -0.1092,  ..., -1.6081,  1.3210, -1.3686],
         [-1.0699, -2.8062,  0.1777,  ..., -1.3479,  1.4819, -1.8017]],

        [[-0.2167, -2.5140,  0.2024,  ..., -2.4768,  1.4057, -1.2769],
         [-1.7092, -1.9680, -0.0487,  ..., -1.4780, -0.5604, -1.1947],
         [-1.6884, -1.6817,  0.2435,  ..., -1.7221, -0.1048, -0.7983],
         [-0.9575, -2.4862,  0.3975,  ..., -1.1702,  0.5283, -1.2972]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7124, -0.6755, -1.3668,  ..., -1.5789,  0.2544, -1.2491],
         [-1.5431, -0.7357,  0.1929,  ..., -1.7782, -0.5010, -0.8811],
         [-1.7009, -1.5151, -0.4604,  ..., -1.1546,  0.3514,  0.3171],
         [-1.5076, -2.2288, -0.7312,  ..., -1.1929, -0.2174, -0.1082]],

        [[-1.5701, -1.0819, -0.8949,  ..., -2.3168,  0.1625, -0.2189],
         [-1.6408, -1.5089, -0.3952,  ..., -1.5115,  0.2257,  0.2442],
         [-1.2908, -1.3386, -0.3201,  ..., -1.0890, -0.3595, -0.0303],
         [-0.8963, -1.2487, -0.5575,  ..., -0.5582,  0.5132, -1.1320]],

        [[-1.1265, -1.8293, -0.6179,  ..., -1.5455, -0.3607,  0.0691],
         [-1.7003, -1.2817, -0.3531,  ..., -1.4630, -0.1407,  0.2174],
         [-1.9056, -0.9548, -0.4670,  ..., -1.1312, -0.5276, -0.6273],
         [-1.6001, -1.1610, -0.7934,  ..., -1.1040, -0.3747,  0.1578]],

        ...,

        [[-2.2731, -0.6729, -0.9191,  ..., -1.8592,  0.6179,  0.1540],
         [-1.0471, -1.4293, -0.4536,  ..., -1.3084, -0.2669, -1.1590],
         [-1.5031, -1.0733, -1.0173,  ..., -0.7899, -0.0545, -0.2431],
         [-1.7859, -1.5010, -0.5316,  ..., -0.9664,  0.1617, -0.1691]],

        [[-1.5178, -1.1948, -1.3091,  ..., -1.0631,  0.7002,  0.7653],
         [-1.1808, -1.9642, -0.5179,  ..., -1.4979, -0.1834,  0.4004],
         [-1.6035, -1.0420, -0.8941,  ..., -1.2664, -0.4010, -0.2191],
         [-1.7127, -2.0092, -0.8643,  ..., -1.1259,  0.2492,  0.3535]],

        [[-1.8064, -1.2689, -0.9296,  ..., -2.1679,  0.4515,  0.4214],
         [-2.4810, -0.8160, -0.4756,  ..., -0.8036, -1.1895,  0.3626],
         [-2.3210, -1.1633, -0.4835,  ..., -1.4442, -1.3154,  0.0431],
         [-1.5161, -1.7321, -0.7187,  ..., -1.1230, -0.5331,  0.2988]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([999, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([1258, 4, 256]), pos_embed.shape: torch.Size([1258, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
encoder start :
	 src.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([1258, 4, 256]), k.shape: torch.Size([1258, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1258, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1258, 4, 256])
	 (after FFN) src2.shape: torch.Size([1258, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1258, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.2863,  0.7226,  0.5300,  ..., -1.8479,  1.9478, -0.6281],
         [-2.0806,  0.4604,  0.7452,  ..., -1.5024,  2.4143, -0.9376],
         [-2.2729,  0.8179,  0.5195,  ..., -1.4774,  2.3183, -0.6164],
         [-1.7700,  0.4778,  0.4839,  ..., -1.5600,  1.9852, -0.8106]],

        [[-1.8062,  0.9175,  0.2560,  ..., -1.6544,  2.1928, -0.2831],
         [-2.4227,  0.4903,  1.3410,  ..., -1.6029,  1.6900, -0.8829],
         [-2.0732,  0.4412,  0.5008,  ..., -1.4514,  2.3275, -0.6427],
         [-2.1687,  0.0061,  0.7187,  ..., -1.8053,  1.3973, -0.6603]],

        [[-2.2042,  0.7482,  0.6412,  ..., -0.1432,  2.1795, -0.2524],
         [-1.7677,  0.4246,  0.6543,  ..., -0.3982,  1.7780, -0.5658],
         [-2.0137,  0.4754,  0.2743,  ..., -1.9187,  0.7708, -0.9143],
         [-1.7024,  0.5795,  0.6678,  ..., -1.4547,  2.0654, -0.8140]],

        ...,

        [[-1.9382,  1.1717,  0.3284,  ..., -2.4108,  2.3775, -0.1880],
         [-1.6442,  0.8774,  1.0308,  ..., -1.7563,  2.3938, -0.7693],
         [-1.9247,  0.5889,  0.5679,  ..., -1.4399,  2.1574, -0.0423],
         [-0.1199,  0.0905,  0.4508,  ..., -1.9538,  1.5417, -0.6216]],

        [[-2.3354,  0.7722, -0.3702,  ..., -2.1459,  2.4494, -0.3032],
         [-1.9727,  0.7254,  1.1057,  ..., -1.5840,  2.4714, -0.4904],
         [-2.6001,  0.4370,  0.5015,  ..., -1.7298,  2.3395, -0.9000],
         [-1.7073,  0.8480,  0.4513,  ..., -1.6895,  2.2164, -0.6704]],

        [[-1.7990,  0.0750,  0.9919,  ..., -2.1507,  1.8738, -0.4802],
         [-2.0516,  0.2614,  0.8436,  ..., -1.4366,  2.3853, -0.5392],
         [-2.5238,  0.8889,  0.5778,  ..., -1.6424,  0.6329, -0.9373],
         [-1.5769,  0.7905,  0.6522,  ..., -1.8911,  1.9216, -0.9266]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.0863,  0.5813,  0.1360,  ..., -2.0302,  0.7139,  0.3377],
         [-1.0053,  0.9153,  0.2460,  ..., -1.6285,  0.8965,  0.1573],
         [-1.1633,  0.9366, -0.1076,  ..., -1.4984,  0.8009,  0.3697],
         [-0.7262,  0.4609,  0.0415,  ..., -1.6581,  0.7104, -0.1772]],

        [[-1.0054,  0.8692, -0.0286,  ..., -1.8871,  0.8259,  0.2681],
         [-1.4435,  0.5148,  0.9851,  ..., -1.6413,  0.7014,  0.0891],
         [-0.9076,  0.9714, -0.2082,  ..., -1.7236,  0.6684,  0.5322],
         [-1.0027,  0.5609,  0.6533,  ..., -1.2232,  0.8191,  0.1607]],

        [[-1.0120,  0.8631,  0.0360,  ..., -0.4174,  0.7674,  0.2328],
         [-0.8894,  0.5169, -0.0215,  ..., -1.3114,  0.1640,  0.0992],
         [-0.8607,  0.4960,  0.0155,  ..., -1.8486,  0.2425,  0.0962],
         [-0.9002,  0.4558,  0.8703,  ..., -1.0456,  0.5595, -0.0253]],

        ...,

        [[-1.1108,  0.9068,  0.0558,  ..., -2.1947,  1.1304,  0.3165],
         [-0.7349,  0.5990, -0.0643,  ..., -1.5263,  1.3511, -0.0321],
         [-0.7899,  0.9760, -0.0578,  ..., -1.5640,  0.5616,  0.7647],
         [ 0.0666,  0.4528,  0.0271,  ..., -1.6079,  0.1195,  0.3632]],

        [[-0.9620,  0.6466, -0.3260,  ..., -2.3449,  0.9825,  0.0058],
         [-1.1867,  0.5942,  0.3725,  ..., -1.9771,  1.0324,  0.5361],
         [-1.0542,  0.5336, -0.3354,  ..., -1.2368,  0.9492,  0.4412],
         [-0.9705,  0.6624,  0.0267,  ..., -1.6538,  0.5626, -0.1745]],

        [[-0.7652,  0.3278,  0.3748,  ..., -1.9592,  1.1512,  0.4625],
         [-1.3332,  0.2949,  0.6834,  ..., -1.3401,  0.7557,  0.6176],
         [-1.4343,  1.4208,  0.4520,  ..., -1.6292, -0.0600,  0.1964],
         [-0.6035,  0.9419,  0.1522,  ..., -1.7510,  0.5172,  0.2012]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.3102, -0.4707,  0.2862,  ..., -1.1359,  0.8953, -1.4742],
         [-0.3730, -0.7295, -0.8885,  ..., -1.4070,  1.1019, -1.7190],
         [-0.7541, -0.7947, -0.2964,  ..., -1.4744,  1.1390, -3.0454],
         [-0.4486, -1.0494, -0.0913,  ..., -0.8260,  0.8820, -3.1299]],

        [[-0.0838, -1.6682, -0.0590,  ..., -1.0587,  0.7765, -2.7331],
         [-0.7848, -0.7854,  0.4848,  ..., -1.7033,  0.7893, -2.9577],
         [-0.1260, -1.0291, -0.8174,  ..., -1.5931,  0.6387, -2.6592],
         [-0.5287, -0.7439,  1.0373,  ..., -1.0787,  0.9867, -3.0066]],

        [[-0.4935,  0.3843, -0.4884,  ..., -0.1201,  0.8668, -2.5196],
         [-0.4894, -1.4692, -0.1790,  ..., -1.1747,  0.8649, -2.4295],
         [-0.4917, -1.0483, -0.8537,  ..., -1.7857,  0.7379, -1.9969],
         [-0.6619, -1.1163,  0.4946,  ..., -1.4436,  0.7046, -3.1352]],

        ...,

        [[-0.1375, -1.2850,  0.2809,  ..., -1.6796,  1.3402, -2.8253],
         [-0.4961, -0.8770,  0.1253,  ..., -1.5908,  1.2015, -3.1239],
         [-0.3133, -0.6676, -0.1530,  ..., -1.4718,  0.9868, -2.3999],
         [-0.2583, -1.3968,  0.0101,  ..., -1.2624,  0.0362, -2.4886]],

        [[-0.3502, -1.4881, -0.3447,  ..., -1.5371,  0.6929, -1.9190],
         [-0.5920, -0.6791, -0.5916,  ..., -1.9198,  1.1289, -1.7488],
         [-0.7420, -1.2164, -0.4254,  ..., -1.3036,  1.0073, -2.7536],
         [-0.3987, -0.5542, -0.6059,  ..., -1.5766,  0.8313, -1.7375]],

        [[-0.7794, -1.2233,  0.4565,  ..., -0.8300,  1.1552, -2.5499],
         [-0.5982, -1.2757, -0.1198,  ..., -1.2582,  0.9133, -2.7874],
         [-0.6147,  0.3995,  0.1876,  ..., -1.3887,  0.5543, -2.7218],
         [-0.4768, -0.8742,  0.0301,  ..., -1.5962,  0.8482, -2.8195]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-8.6564e-01, -2.0983e+00, -1.3719e-03,  ..., -1.8516e+00,
           1.3596e+00, -7.2241e-01],
         [-1.1817e+00, -2.3328e+00, -6.4073e-01,  ..., -2.0875e+00,
           1.2924e+00, -7.5909e-01],
         [-1.1463e+00, -2.3646e+00, -3.6071e-01,  ..., -2.4408e+00,
           1.2767e+00, -1.3609e+00],
         [-1.0536e+00, -2.6088e+00, -2.7337e-01,  ..., -1.5151e+00,
          -1.8705e-01, -1.3312e+00]],

        [[-8.2737e-01, -2.3571e+00, -1.9284e-01,  ..., -1.7463e+00,
           1.2850e+00, -1.3056e+00],
         [-1.2301e+00, -2.3255e+00,  1.3627e-01,  ..., -2.3278e+00,
           1.1758e+00, -1.3111e+00],
         [-8.2322e-01, -2.3984e+00, -5.6908e-01,  ..., -2.0916e+00,
           1.0205e+00, -9.6288e-01],
         [-7.7672e-01, -1.7650e+00,  1.4033e-01,  ..., -1.8131e+00,
           9.8961e-01, -1.2239e+00]],

        [[-1.0427e+00, -1.7031e+00, -3.7811e-01,  ...,  2.3957e-02,
           1.6952e+00, -9.4555e-01],
         [-5.4224e-01, -2.5298e+00, -6.0430e-02,  ..., -1.9751e+00,
           9.9208e-01, -1.4529e+00],
         [-1.0604e+00, -2.4014e+00, -3.7797e-01,  ..., -2.2237e+00,
           1.0790e+00, -6.5582e-01],
         [-7.5074e-01, -2.2924e+00, -8.9692e-03,  ..., -1.7686e+00,
           7.8072e-01, -1.6769e+00]],

        ...,

        [[-9.1041e-01, -2.4661e+00, -3.4435e-01,  ..., -1.8662e+00,
           1.5166e+00, -1.5233e+00],
         [-9.7495e-01, -1.6858e+00, -3.6530e-02,  ..., -1.1450e+00,
           9.3634e-01, -1.2766e+00],
         [-9.9495e-01, -2.1659e+00,  4.0856e-02,  ..., -2.2718e+00,
           1.1009e+00, -1.1297e+00],
         [-1.1266e+00, -1.9142e+00, -1.1654e-01,  ..., -1.3602e+00,
           7.4291e-01, -7.4867e-01]],

        [[-1.0379e+00, -2.2878e+00, -3.1479e-01,  ..., -1.7807e+00,
           6.3370e-01, -5.9308e-01],
         [-1.2548e+00, -2.1584e+00, -4.6905e-01,  ..., -1.8111e+00,
           1.0217e+00, -2.4331e-01],
         [-1.2553e+00, -2.6627e+00, -7.8642e-01,  ..., -1.7755e+00,
           1.1599e+00, -1.2322e+00],
         [-9.0509e-01, -1.9897e+00, -8.0192e-01,  ..., -2.2705e+00,
           1.1029e+00, -2.2749e-01]],

        [[-1.4028e+00, -2.5405e+00,  1.7726e-01,  ..., -1.5590e+00,
           1.1444e+00, -1.3479e+00],
         [-1.1532e+00, -2.5333e+00, -1.6265e-01,  ..., -1.9418e+00,
           1.0769e+00, -9.1624e-01],
         [-7.8296e-01, -1.8955e+00, -6.6627e-01,  ..., -1.9790e+00,
           1.1100e+00, -1.1034e+00],
         [-9.0661e-01, -2.6438e+00, -2.8350e-01,  ..., -2.0838e+00,
           2.4067e-01, -1.1771e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4443, -0.9588, -0.3202,  ..., -0.8651, -0.6032,  0.3765],
         [-1.8659, -1.1359, -0.5043,  ..., -1.3417, -0.6838,  0.5110],
         [-1.9400, -1.4689, -0.3602,  ..., -1.5642, -0.3404,  0.0258],
         [-1.9280, -1.3307, -0.4759,  ..., -1.3429, -0.8353,  0.0205]],

        [[-1.3887, -1.4042, -0.7900,  ..., -1.4942, -0.2940,  0.0568],
         [-1.3154, -1.2742, -0.6910,  ..., -1.8522, -0.3747, -0.3115],
         [-1.7368, -1.3415, -0.5581,  ..., -1.3671, -0.3931,  0.1751],
         [-1.6219, -1.1722,  0.4440,  ..., -0.7515, -0.0782,  0.1390]],

        [[-0.8187, -1.0048, -0.9780,  ..., -0.6520, -0.5630, -0.3295],
         [-2.0679, -1.2457, -0.6241,  ..., -1.5869, -0.3043, -0.1336],
         [-1.7578, -1.4850, -0.0372,  ..., -1.2891, -0.7962,  0.4157],
         [-1.6009, -1.0827, -0.3465,  ..., -1.3046, -0.7251, -0.2645]],

        ...,

        [[-1.6772, -1.4379, -0.7792,  ..., -1.5040, -0.5959, -0.3997],
         [-1.8008, -1.2258, -0.6085,  ..., -0.9216, -0.0108, -0.1488],
         [-1.6963, -1.2929,  0.0035,  ..., -1.6941, -0.5135,  0.0243],
         [-2.1097, -1.1682,  0.2254,  ..., -0.9019, -0.7665,  0.1530]],

        [[-1.5846, -1.4888, -0.9063,  ..., -0.5712, -0.9871, -0.0744],
         [-1.7899, -1.0714, -0.7412,  ..., -1.3315, -0.4808,  0.4299],
         [-1.9883, -1.4840, -0.6835,  ..., -1.3846, -0.8737, -0.0456],
         [-1.0180, -1.3162, -0.7608,  ..., -1.7723, -0.9039,  0.3767]],

        [[-2.1262, -1.5275, -0.4531,  ..., -1.4809, -0.5834, -1.0325],
         [-2.0235, -1.1919, -0.0192,  ..., -1.3316, -0.5272, -0.1217],
         [-1.9142, -1.2369, -0.4600,  ..., -1.1217, -0.0684,  0.1343],
         [-1.8272, -1.4572, -0.4262,  ..., -1.3307, -1.0677, -0.1433]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1258, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([624, 4, 256]), pos_embed.shape: torch.Size([624, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
encoder start :
	 src.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([624, 4, 256]), k.shape: torch.Size([624, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([624, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([624, 4, 256])
	 (after FFN) src2.shape: torch.Size([624, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([624, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3259,  0.3566,  0.3192,  ..., -1.9529,  1.8895, -0.6269],
         [-2.1793,  0.9935,  0.5894,  ..., -1.5211,  1.9850, -0.4032],
         [-2.2848,  0.6110,  0.5323,  ..., -1.5832,  1.6848, -0.7318],
         [-1.8813,  0.5117, -0.2775,  ..., -1.5267,  1.8290, -0.6316]],

        [[-2.0121,  0.4207,  0.2448,  ..., -2.1243,  2.0780, -0.3762],
         [-2.0172,  0.9074,  0.0732,  ..., -0.5487,  1.5999, -0.5299],
         [-2.0376,  0.7361,  0.3232,  ..., -1.8595,  1.4974, -0.5588],
         [-0.5355,  0.2585,  0.4251,  ..., -1.9927,  2.0136, -0.6575]],

        [[-2.4451,  0.2269, -0.4467,  ..., -1.9917,  1.8750, -0.7252],
         [-1.9444,  0.4114,  0.4034,  ..., -1.6962,  1.7427, -0.5803],
         [-0.4486,  0.6437,  0.0899,  ..., -1.7079,  1.9490, -0.6906],
         [-2.1035,  0.5447,  0.3017,  ..., -1.4590,  2.0388, -0.7922]],

        ...,

        [[-2.2881,  0.3855,  0.1310,  ..., -2.1742,  1.9070, -0.4805],
         [-2.2440,  0.8983,  0.3508,  ..., -2.0055,  0.7097, -0.6654],
         [-2.2523,  0.5445,  0.6116,  ..., -2.1761,  1.4203, -0.5260],
         [-0.2094,  0.1821,  0.5144,  ..., -1.7197,  0.6539, -0.6886]],

        [[-2.5127, -0.2301,  0.2116,  ..., -1.8799,  1.4173, -1.0346],
         [-2.0440,  0.4335,  0.5435,  ..., -1.9454,  1.7810, -0.4135],
         [-1.9643,  0.6879,  0.3797,  ..., -1.6039,  1.8472, -0.6932],
         [-2.1545,  0.1757, -0.6176,  ..., -1.5283,  2.0689, -0.9960]],

        [[-0.6191,  0.6858,  0.2262,  ...,  0.1291,  1.5188, -0.7626],
         [-1.6293,  0.8052,  1.0351,  ..., -1.7132,  2.1743, -0.8822],
         [-2.0322,  0.7714,  0.4570,  ..., -1.8699,  1.8817, -0.6994],
         [-2.5950,  0.3854,  0.3369,  ..., -1.1767,  1.4288, -0.8046]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.8239,  0.8109, -0.0420,  ..., -1.3672,  0.7731,  0.9379],
         [-0.6211,  0.7981, -0.3780,  ..., -1.9094,  0.3245,  0.9591],
         [-0.8079,  0.6837, -0.2260,  ..., -1.6684,  0.4749,  1.0765],
         [-0.7476,  0.6118, -0.6627,  ..., -1.9776,  1.0357,  0.6580]],

        [[-1.1728,  0.5392, -0.7417,  ..., -1.7540,  0.7179,  0.5193],
         [-0.8267,  0.8606, -0.8971,  ..., -1.2297,  0.6871,  0.9670],
         [-0.6668,  0.7989, -0.5189,  ..., -1.5923,  0.6696,  0.8747],
         [ 0.1464,  0.7544, -0.7073,  ..., -1.5418,  0.8170,  1.0438]],

        [[-1.0091,  0.9563, -1.0061,  ..., -1.6814,  0.9676,  0.6464],
         [-0.8070,  0.9050, -0.8817,  ..., -1.6735,  0.9116,  0.7560],
         [-0.2882,  1.0810, -0.7678,  ..., -2.0082,  0.4736,  0.7314],
         [-0.8667,  0.5285, -0.7516,  ..., -1.2911,  0.7221,  0.8140]],

        ...,

        [[-0.9617,  0.8621, -0.4647,  ..., -1.9837,  0.6854,  1.2486],
         [-0.9981,  0.9855, -0.6018,  ..., -1.9239,  0.3094,  0.9901],
         [-1.0915,  1.0906, -0.5958,  ..., -1.3511,  0.4359,  0.7647],
         [-0.0591,  0.2782, -0.6392,  ..., -1.8574,  0.1209,  0.9586]],

        [[-1.2123,  0.3947, -0.8528,  ..., -1.9142,  0.3266,  0.8738],
         [-0.7748,  1.0256, -0.6503,  ..., -2.0558,  0.4759,  1.2266],
         [-1.2920,  0.7099, -0.7543,  ..., -2.1255,  0.6014,  0.7091],
         [-0.6881,  0.5462, -1.2152,  ..., -1.6476,  0.5401,  0.5710]],

        [[-0.4639,  1.3323, -0.8288,  ..., -0.6511, -0.0343,  0.3122],
         [-0.5323,  0.9118, -0.4431,  ..., -1.7977,  0.6286,  0.7189],
         [-0.9110,  0.9883, -0.6744,  ..., -2.1751,  0.1028,  0.7118],
         [-1.4265,  0.7572, -0.4203,  ..., -1.5405,  0.4974,  0.3846]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.4305, -0.9555,  0.2649,  ..., -1.4722,  0.7468, -1.8708],
         [-0.4221, -0.8971, -0.4624,  ..., -1.7121, -0.0544, -1.8996],
         [-0.6939, -0.8561, -0.2783,  ..., -1.6670,  0.0106, -1.7217],
         [-0.4899, -0.4821, -0.5119,  ..., -1.0614,  1.0305, -2.5258]],

        [[-0.4855, -1.6277, -0.1805,  ..., -1.5025,  0.4686, -2.4633],
         [-0.7595, -1.2098,  0.2552,  ..., -0.6644,  0.3513, -1.7494],
         [-0.3451, -1.5760, -0.1958,  ..., -1.5913,  0.6839, -1.8513],
         [ 0.1822, -0.6279, -0.2950,  ..., -1.5550,  0.7432, -1.1587]],

        [[-0.4107, -0.9619, -0.5656,  ..., -1.3168,  0.4537, -2.0082],
         [-0.3243, -1.1198, -0.6035,  ..., -1.5890,  0.4389, -2.0441],
         [-0.2044, -0.6403, -0.1171,  ..., -1.3215,  0.5432, -2.2893],
         [-0.3739, -1.4600, -0.3064,  ..., -1.5348,  0.7763, -2.4052]],

        ...,

        [[-0.6124, -1.1123, -0.2714,  ..., -1.7178,  0.6630, -1.0779],
         [-0.8471, -1.6053, -0.3195,  ..., -2.0849,  0.3904, -1.0170],
         [-0.7146, -0.7900, -0.2720,  ..., -1.5144,  0.5699, -1.8814],
         [-0.0860, -0.6831, -1.5260,  ..., -1.7570,  0.1529, -1.1374]],

        [[-0.9775, -1.3586, -0.4651,  ..., -1.8874,  0.7289, -1.8318],
         [-0.3303, -1.1033, -0.4987,  ..., -1.1022,  0.6088, -1.0388],
         [-0.9578, -1.4694, -0.2257,  ..., -1.6633,  0.5358, -2.1571],
         [-0.6236, -1.1429, -0.6484,  ..., -1.6064, -0.0472, -2.2228]],

        [[-0.5462, -0.1863,  0.4543,  ..., -1.2942,  0.3126, -2.1142],
         [-0.3027, -1.3623, -1.0136,  ..., -1.7934,  0.4701, -1.7164],
         [-0.2521, -1.4698, -0.4333,  ..., -1.0452,  0.3508, -1.7598],
         [-0.7373, -1.3499,  0.0165,  ..., -1.2262,  0.6851, -2.0248]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9671, -2.1465, -0.4101,  ..., -1.7585,  1.0021, -0.8123],
         [-0.9953, -1.7315, -0.7241,  ..., -2.1280,  0.6495, -1.0630],
         [-0.5145, -1.2800, -0.4729,  ..., -1.9512, -0.3043, -0.8295],
         [-0.8804, -2.2849, -0.5179,  ..., -1.6474,  0.4331, -0.9058]],

        [[-0.9274, -2.8243, -0.5949,  ..., -1.4264,  0.1459, -0.9687],
         [-0.7625, -2.6349, -0.0299,  ..., -1.2829,  0.3853, -0.7867],
         [-0.7843, -2.7179, -0.6406,  ..., -1.8007,  0.8003, -0.5495],
         [-0.2116, -1.8459, -0.6804,  ..., -1.7915,  0.3946, -0.3838]],

        [[-0.9359, -2.4539, -1.0955,  ..., -1.4811,  0.1311, -0.7510],
         [-0.6683, -2.1911, -0.6080,  ..., -1.5557,  0.9039, -0.8458],
         [-0.7983, -1.2052, -0.2646,  ..., -2.1255,  0.5557, -1.2801],
         [-0.6862, -2.6214, -0.8764,  ..., -1.7467,  0.7868, -0.9616]],

        ...,

        [[-1.0190, -2.2979, -0.6046,  ..., -1.8354,  0.0623, -0.2526],
         [-0.4133, -2.6353, -0.8576,  ..., -2.0390,  0.4240, -0.5880],
         [-0.6778, -2.2801, -0.8446,  ..., -2.0059,  0.2179, -1.0414],
         [-0.7032, -2.4362, -0.8297,  ..., -1.7379,  0.1771, -0.1116]],

        [[-0.9342, -2.4053, -0.8421,  ..., -1.8937,  0.2736, -0.7314],
         [-0.6921, -2.5964, -0.9407,  ..., -1.7591,  0.4082, -0.2452],
         [-1.0479, -2.8485, -0.2146,  ..., -1.8544,  0.6440, -0.6144],
         [-1.1540, -2.7409, -0.7060,  ..., -0.9161, -0.0102, -0.5804]],

        [[-0.8469, -0.8893, -0.1591,  ..., -0.6197,  0.2621, -0.5087],
         [-0.9860, -2.4945, -1.1168,  ..., -2.2486,  0.2458, -0.8325],
         [-0.5705, -2.6118, -0.4554,  ..., -1.4567,  0.2422, -0.6918],
         [-1.0068, -2.4784, -0.1471,  ..., -1.8961,  0.8089, -0.8332]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4992, -0.6931, -0.0790,  ..., -1.2165, -0.9226, -0.0711],
         [-0.9629, -0.5768, -0.5505,  ..., -1.5815, -0.1565, -0.2215],
         [-1.3311, -0.5972,  0.1257,  ..., -1.4683, -1.3172, -0.0929],
         [-1.6638, -0.9429, -0.2495,  ..., -1.2571, -1.1639, -0.2460]],

        [[-1.9552, -1.4460, -0.7853,  ..., -1.4491, -0.3062, -0.4138],
         [-1.2964, -1.0187, -0.0989,  ..., -0.9122, -0.8521, -0.1047],
         [-1.3740, -0.9277,  0.1125,  ..., -1.2911, -1.1258,  0.2321],
         [-1.3597, -0.5126, -0.3460,  ..., -1.5316, -0.8356,  0.4812]],

        [[-1.7415, -1.4011, -0.7903,  ..., -1.0962, -1.4036, -0.2800],
         [-1.5290, -0.7731, -0.4178,  ..., -1.2549, -0.4217,  0.3033],
         [-1.2497, -0.3019, -0.0787,  ..., -1.6754, -0.8374, -0.2470],
         [-1.3615, -0.8093, -0.5760,  ..., -1.3106, -0.2569, -0.1215]],

        ...,

        [[-1.4798, -0.6368, -0.5412,  ..., -1.3374, -0.3482,  0.1810],
         [-0.9892, -0.7374, -0.1703,  ..., -1.4784, -0.5669,  0.2143],
         [-0.8936, -1.2027, -0.6018,  ..., -1.3253, -1.0067, -0.2512],
         [-1.3240, -1.0218, -0.4555,  ..., -0.9008, -1.2298,  0.0033]],

        [[-1.2592, -1.1329, -0.4475,  ..., -1.3272, -0.9940,  0.2771],
         [-0.9964, -1.1722, -0.8944,  ..., -1.3084, -0.3283,  0.3144],
         [-1.1990, -0.6332, -0.2962,  ..., -1.4653, -0.9016, -0.1521],
         [-1.5760, -1.2539, -0.7296,  ..., -1.1942, -0.9463,  0.2327]],

        [[-1.5314, -0.4775, -0.3020,  ..., -0.6608, -0.9677, -0.0231],
         [-1.7499, -0.9468, -0.5920,  ..., -1.4497, -0.7167,  0.0152],
         [-1.1932, -1.1450, -0.3031,  ..., -1.2806, -0.7313, -0.0535],
         [-1.5684, -0.8536,  0.0429,  ..., -1.3689, -0.6547, -0.3239]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([624, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([696, 4, 256]), pos_embed.shape: torch.Size([696, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
encoder start :
	 src.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([696, 4, 256]), k.shape: torch.Size([696, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([696, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([696, 4, 256])
	 (after FFN) src2.shape: torch.Size([696, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([696, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1499,  0.1533,  0.0169,  ..., -1.8698,  2.1007, -0.8273],
         [-1.8305,  0.1863, -0.3637,  ..., -1.1809,  1.5212, -0.2550],
         [-0.1932,  0.3288,  0.1599,  ..., -1.3829,  1.2162,  0.0103],
         [-1.4049,  0.6386,  0.0147,  ..., -1.5990,  1.6438,  0.0957]],

        [[-1.9877,  0.4290,  0.4146,  ..., -2.0951,  1.7314, -1.1815],
         [-1.3615,  0.0876,  0.2255,  ..., -1.4254,  1.7244, -0.0445],
         [-1.8839,  0.2652, -0.4023,  ..., -2.0740,  1.5645, -0.5292],
         [-0.3066,  0.4940,  0.0452,  ..., -1.5028,  1.6466, -0.6739]],

        [[-2.0284,  0.1482,  0.3117,  ..., -1.3480,  1.5594, -0.6657],
         [-1.5206,  0.6127, -0.1246,  ..., -1.1230,  1.7182, -0.6097],
         [-2.2331,  0.3336, -0.1436,  ..., -1.7245,  1.9109, -0.7163],
         [-1.8090,  0.6746,  0.2519,  ..., -2.0216,  1.8276, -0.6582]],

        ...,

        [[-2.2188,  0.4160,  0.5315,  ..., -1.6246,  1.5192, -0.8576],
         [-1.6702,  0.5804,  0.1356,  ..., -1.5268,  1.2770, -0.6966],
         [-1.8345,  0.3084,  0.9525,  ..., -1.6388,  1.6221,  0.3610],
         [-1.8434,  0.3139,  0.2702,  ..., -1.7922,  1.3435, -0.8303]],

        [[-2.0258,  0.3850,  0.5193,  ..., -1.5545,  1.4975, -0.8007],
         [-1.6744,  0.3149,  0.1497,  ..., -1.4796,  1.9023, -0.7022],
         [-2.0397,  0.2095, -0.2128,  ..., -1.5228,  1.7452, -0.0541],
         [ 0.3070,  0.3036, -1.0024,  ..., -1.6360,  1.6291, -0.7325]],

        [[-1.9498,  0.4670,  0.1568,  ..., -0.4657,  1.9099, -1.3981],
         [-1.7439,  0.2882,  0.2747,  ..., -1.2982,  1.4163, -0.7245],
         [-1.6003,  0.3873,  0.5121,  ..., -1.9603,  1.8175, -0.7264],
         [-1.9834,  0.6824,  0.1508,  ..., -1.7775,  1.3048, -0.9038]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.8772,  0.5381, -0.5798,  ..., -1.6341,  0.7079,  0.5825],
         [-0.4777,  0.3730, -1.0522,  ..., -1.1348,  0.5250,  1.3479],
         [-0.5712,  0.5150, -0.5870,  ..., -0.9724,  0.0580,  0.6709],
         [ 0.1453,  0.7304, -0.9105,  ..., -1.5060,  0.2483,  1.5171]],

        [[-0.7933,  0.6718, -0.4610,  ..., -1.6632,  0.6457,  0.0206],
         [-0.8027,  0.3831,  0.3155,  ..., -0.8693,  0.5689,  1.1483],
         [-0.7823,  0.9586, -0.9918,  ..., -1.7494,  0.7968,  0.9697],
         [ 0.3610,  0.6508, -1.0806,  ..., -1.2729,  0.2792,  0.9310]],

        [[-1.0579,  0.3343, -0.4828,  ..., -1.1381,  0.6692,  1.0904],
         [-0.4638,  1.1023, -1.0226,  ..., -0.8023,  0.6140,  1.4892],
         [-1.2531,  0.8141, -0.7693,  ..., -0.8368,  0.5745,  0.8917],
         [-1.1554,  0.7794, -0.7764,  ..., -1.5436,  0.6392,  0.5008]],

        ...,

        [[-0.9641,  0.9477, -0.2442,  ..., -1.2498,  0.5268,  1.0825],
         [-0.6826,  0.8228, -0.8546,  ..., -1.3075,  0.4484,  0.6635],
         [-0.9895,  1.1052, -0.0766,  ..., -1.5597,  0.6553,  1.5385],
         [-0.6968,  0.9693, -0.5831,  ..., -0.8326,  0.8512,  1.0286]],

        [[-0.9326,  0.9179, -0.2832,  ..., -1.2356,  0.5436,  0.9995],
         [-0.5807,  0.7854, -0.5805,  ..., -1.0172,  1.2173,  0.6776],
         [-0.9441,  0.7217, -0.5849,  ..., -1.6815,  0.4761,  1.1472],
         [ 0.5479,  0.2600, -0.7785,  ..., -1.4338,  0.4469,  0.4928]],

        [[-1.1031,  0.5317, -0.6298,  ..., -0.6344,  1.1522,  0.1806],
         [-0.6570,  1.0777, -0.4986,  ..., -0.9917,  0.4851,  0.8778],
         [-0.5112,  0.5111, -0.3190,  ..., -1.3464,  0.4481,  0.5874],
         [-1.5099,  0.7852, -0.8316,  ..., -1.4425,  0.0589,  0.7640]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.4324, -1.4929, -0.3785,  ..., -1.1928,  0.2900, -1.8979],
         [-0.3953, -1.6127, -0.5548,  ..., -1.0680,  0.3339, -1.7817],
         [-0.2781, -1.3402, -0.2292,  ..., -1.1380,  0.2512, -2.0177],
         [ 0.2057, -1.1964, -0.7282,  ..., -1.3507,  0.1742, -0.7753]],

        [[-0.4038, -0.6207, -0.1161,  ..., -1.5016,  0.4692, -1.2683],
         [-0.7244, -0.9999, -0.0506,  ..., -0.5727,  0.2899, -1.2370],
         [-0.1160, -1.2844,  0.2888,  ..., -1.5604,  0.2652, -1.4059],
         [ 0.4153, -1.5769, -0.8440,  ..., -1.2617,  0.1369, -1.0838]],

        [[-0.7498, -1.5189, -0.3613,  ..., -1.2489,  0.3289, -1.7201],
         [ 0.0197, -1.1042, -0.7100,  ..., -1.1431,  0.2385, -1.4029],
         [-0.9152, -1.6101,  0.2524,  ..., -1.1254,  0.6713, -1.2891],
         [-0.4268, -0.6859, -0.4345,  ..., -1.6633,  0.6750, -0.7512]],

        ...,

        [[-0.5205, -1.3143, -0.3103,  ..., -1.4262,  0.3315, -1.9373],
         [-0.6452, -0.7275, -0.6958,  ..., -0.8570,  0.2518, -0.3254],
         [-0.7267, -1.4870, -0.0362,  ..., -0.8596,  0.2650, -1.0255],
         [-0.3425, -0.6068, -0.2262,  ..., -1.2507,  0.7176, -1.3288]],

        [[-0.6381, -1.7701,  0.1717,  ..., -1.4711, -0.0154, -1.7135],
         [-0.4867, -1.6344,  0.3721,  ..., -1.2813,  0.9398, -1.9541],
         [-0.4482, -1.0470,  0.4886,  ..., -1.0329,  0.5841, -1.3257],
         [ 0.0752, -1.1831, -0.4871,  ..., -1.5636,  0.1001, -1.5577]],

        [[-0.4198, -1.1200, -0.3363,  ..., -1.1695,  0.7929, -1.4599],
         [-0.3298, -0.6457, -0.7220,  ..., -1.2967, -0.0755, -1.6509],
         [-0.5084, -1.6088, -0.2779,  ..., -1.3463,  0.1951, -1.8081],
         [-0.6390, -1.0200, -0.8667,  ..., -1.3839,  0.1827, -1.7961]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.5347, -2.3518, -0.5641,  ..., -1.3689,  0.0649, -0.7334],
         [-0.8989, -2.8876, -0.5872,  ..., -1.1279,  0.3177, -0.7110],
         [ 0.1893, -1.5342, -0.6312,  ..., -1.2101,  0.2877, -0.7522],
         [-0.4075, -2.0230, -0.7162,  ..., -1.3449,  0.2809, -0.1040]],

        [[-0.7919, -1.1286, -0.1299,  ..., -1.4900,  0.8598, -0.3839],
         [-0.7766, -2.4150, -0.5023,  ..., -1.0098,  0.4133, -0.1439],
         [-0.7716, -1.4380, -0.4404,  ..., -0.5555,  0.5491, -0.0935],
         [-0.2989, -2.7378, -1.1346,  ..., -1.3011, -0.0311, -0.1694]],

        [[-0.9774, -1.9082, -0.3058,  ..., -1.4796,  0.2095, -0.7947],
         [-0.5113, -1.4806, -0.6181,  ..., -1.2395,  0.0551, -0.5602],
         [-1.2481, -1.6267, -0.2871,  ..., -1.5114,  0.6176, -0.4025],
         [-0.9244, -2.2340, -0.7306,  ..., -1.4806,  0.3652, -0.1966]],

        ...,

        [[-1.1726, -2.5640, -0.3077,  ..., -1.3862,  0.3572, -0.7362],
         [-0.5099, -1.7350, -0.6048,  ..., -0.1404,  0.3215,  0.3548],
         [-1.3477, -2.6738, -0.1888,  ..., -1.2680, -0.0327, -0.0936],
         [-1.0834, -0.9152, -0.4975,  ..., -1.2197,  0.1015, -0.5253]],

        [[-1.1687, -1.5264, -0.4165,  ..., -0.6319, -0.0629, -0.6878],
         [-1.0613, -2.7046,  0.0292,  ..., -1.2438,  0.0328, -0.8261],
         [-0.5992, -2.4439, -0.0356,  ..., -1.3780,  0.3712, -0.6603],
         [-0.5498, -2.9491, -0.7495,  ..., -1.5500,  0.3691, -0.9289]],

        [[ 0.0369, -1.9719, -0.4408,  ..., -1.3876,  0.4423, -0.7562],
         [-0.7644, -2.1996, -0.4014,  ..., -0.9954,  0.6911, -0.6758],
         [-1.0214, -2.5385, -0.6690,  ..., -1.4774,  0.0980, -0.4929],
         [-0.8472, -2.4620, -0.9162,  ..., -1.5402,  0.2122, -0.6985]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3645, -0.6956, -0.6190,  ..., -1.0929, -0.6851, -0.1655],
         [-1.4174, -1.3530,  0.2232,  ..., -0.9571, -0.9106, -0.3191],
         [-0.9298, -0.6236, -0.5545,  ..., -0.8596, -0.7652, -0.8071],
         [-1.2034, -0.6779, -0.2939,  ..., -0.4930, -0.9334,  0.1379]],

        [[-0.9337, -0.1316, -0.0309,  ..., -1.1014, -0.4961,  0.2575],
         [-1.6478, -1.0236, -0.4414,  ..., -0.9111, -0.1475, -0.1229],
         [-1.3395, -0.5261, -0.1150,  ..., -0.7190,  0.0534,  0.3263],
         [-1.2278, -1.1368, -0.3918,  ..., -1.4134, -0.9753,  0.1854]],

        [[-1.6205, -0.7327, -0.3251,  ..., -0.5632, -1.3061, -0.4032],
         [-1.5194, -0.7061, -0.3493,  ..., -0.4761, -0.9146, -0.3586],
         [-1.7520, -0.4196, -0.1062,  ..., -1.5307, -0.8031,  0.0283],
         [-1.6193, -0.8712,  0.0831,  ..., -1.1494, -1.3252, -0.2780]],

        ...,

        [[-1.7448, -1.1525, -0.4812,  ..., -1.4686, -0.5205,  0.0882],
         [-1.4731, -0.8754, -0.3846,  ..., -0.5493, -0.0268,  0.7208],
         [-1.6864, -0.9285, -0.5136,  ..., -1.2774, -0.5216, -0.2294],
         [-1.4890, -0.4186, -0.1926,  ..., -0.8010, -0.3971, -0.1416]],

        [[-2.0518, -0.4800, -0.3908,  ..., -0.8731, -0.7359, -0.2068],
         [-1.1672, -1.1255,  0.1253,  ..., -1.4064, -0.2958, -0.9934],
         [-1.3561, -0.7039, -0.3002,  ..., -1.2512, -1.0772, -0.7084],
         [-1.2699, -0.6975, -0.1781,  ..., -1.2836, -0.7261, -0.0289]],

        [[-0.6851, -0.6878, -0.0573,  ..., -1.1937, -0.9068, -0.2303],
         [-0.7071, -0.8946,  0.1776,  ..., -1.2548, -0.9740, -0.6880],
         [-1.6812, -1.0036, -0.0860,  ..., -1.3748, -0.9302, -0.1724],
         [-1.5619, -0.9378, -0.2580,  ..., -1.5174, -0.8087, -0.2696]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([696, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([600, 4, 256]), pos_embed.shape: torch.Size([600, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
encoder start :
	 src.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([600, 4, 256]), k.shape: torch.Size([600, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([600, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([600, 4, 256])
	 (after FFN) src2.shape: torch.Size([600, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([600, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.0434, -0.1312,  0.7684,  ..., -0.2526,  1.5378, -1.2777],
         [-2.2598,  0.2019,  0.0843,  ..., -1.3772,  0.7735, -1.0891],
         [-2.1984,  0.1952,  0.2419,  ..., -1.6343,  1.7910, -1.2569],
         [-1.1838,  0.4508,  0.1958,  ..., -0.7963,  1.6840, -1.1122]],

        [[-1.8417,  0.1991,  0.0053,  ..., -0.2366,  1.8135, -1.2034],
         [-1.9980,  0.5100, -0.6060,  ..., -1.5346,  1.8992, -0.9727],
         [-1.6403,  0.2570, -0.0430,  ..., -1.4951,  1.2575, -1.3235],
         [-1.7129,  0.2428,  0.2868,  ..., -1.3579,  0.2817, -0.9432]],

        [[-1.7391,  0.1040,  0.0443,  ..., -1.5792,  1.6547, -0.8923],
         [-1.9772,  0.1387, -0.4544,  ..., -1.4519,  1.5494, -1.1122],
         [-1.8632, -0.3219,  0.0525,  ..., -0.2556,  1.6158, -0.8447],
         [-1.6298,  0.4226, -0.1258,  ..., -1.6565,  2.2197, -0.8191]],

        ...,

        [[-1.8330,  0.3954, -0.1681,  ..., -1.3083,  1.5192, -1.1698],
         [-2.0885, -0.0404,  0.0954,  ..., -1.3550,  1.4339, -0.8695],
         [-2.0521,  0.1547,  0.2820,  ..., -1.6789,  1.5563, -1.1709],
         [-0.3001,  0.1365, -0.4880,  ..., -1.7210,  1.7489, -1.0293]],

        [[-1.8004,  0.6464, -0.5066,  ..., -1.7207,  1.5870, -0.8786],
         [-0.3211,  0.2332,  0.1612,  ..., -0.4731,  1.6084, -0.7910],
         [-1.8475,  0.3046,  0.0672,  ..., -1.6895,  1.8218, -0.8679],
         [-2.4548, -0.0349,  0.3095,  ..., -1.6748,  1.8388, -0.5379]],

        [[-1.7577,  0.2065, -0.7110,  ..., -1.4870,  1.7031, -1.2227],
         [-2.1931,  0.5011,  0.3092,  ..., -1.2672,  0.5253, -0.8531],
         [-1.8766,  0.1434,  0.8311,  ..., -1.6850,  1.4317, -1.3017],
         [-2.0571,  0.1244,  0.1611,  ..., -1.5560,  1.8998, -0.9593]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3806,  0.9040,  0.0465,  ..., -0.5810,  0.5390,  0.4334],
         [-0.7970,  0.7009, -0.3683,  ..., -0.9464,  0.1166,  0.8856],
         [-1.3011,  0.9050,  0.0119,  ..., -1.0531,  0.9198,  0.0350],
         [-0.4789,  0.6645, -0.3200,  ..., -0.4962,  0.7092,  0.3466]],

        [[-0.9193,  0.7339, -0.6147,  ..., -0.6697,  1.0243,  0.4797],
         [-0.9147,  1.1470, -0.6226,  ..., -1.5788,  0.8534,  0.5495],
         [-1.3141,  0.7404, -0.3473,  ..., -1.0148,  0.4483,  0.4697],
         [-0.8315,  0.6229, -0.2531,  ..., -0.9262,  0.2933,  0.6371]],

        [[-0.2923,  0.8109, -0.4227,  ..., -1.5485,  0.0459,  0.6866],
         [-0.6880,  0.5684, -0.4558,  ..., -1.0257,  0.6638,  0.3012],
         [-0.6648,  0.6236,  0.0927,  ...,  0.0587,  0.4203,  0.5404],
         [-0.9925,  0.9969, -0.1590,  ..., -1.3050,  0.9489,  0.7242]],

        ...,

        [[-0.6562,  0.9725, -0.4197,  ..., -0.5818,  0.7315,  0.0190],
         [-0.8298,  0.5618, -0.1039,  ..., -1.3437,  0.6860,  0.2204],
         [-0.8676,  0.5922, -0.2536,  ..., -0.9172,  0.5551,  0.1868],
         [ 0.0246,  0.6502, -0.6921,  ..., -1.0603,  0.6141,  1.2093]],

        [[-1.6318,  0.9463, -0.9071,  ..., -0.7917,  0.5200,  0.8551],
         [-0.5484,  0.7824,  0.0812,  ..., -0.6552,  0.7598,  0.6603],
         [-1.0801,  0.5829,  0.2187,  ..., -1.2015,  1.2948,  0.4087],
         [-1.2070,  0.6025, -0.0050,  ..., -1.0476,  1.0364,  0.6783]],

        [[-0.9421,  1.1964, -0.6059,  ..., -0.7294,  0.4526,  0.1426],
         [-0.8327,  0.6148,  0.1943,  ..., -0.8926, -0.0566,  0.8230],
         [-0.7136,  0.8630,  0.1064,  ..., -0.6001,  0.9239,  0.5930],
         [-1.6310,  0.7026,  0.0221,  ..., -1.0940,  0.9158,  0.5008]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-8.7625e-01, -1.3677e+00,  9.2465e-02,  ..., -1.1170e+00,
           1.0104e-01, -1.7277e+00],
         [-1.5358e-01, -1.9400e+00,  2.2965e-02,  ..., -9.8099e-01,
           1.6405e-01, -9.5615e-01],
         [-5.4896e-01, -1.5739e+00,  3.2513e-01,  ..., -1.2498e+00,
           7.9549e-01, -1.5442e+00],
         [-2.4539e-01, -1.6630e+00,  1.1600e-01,  ..., -9.9673e-01,
           5.4044e-01, -2.2718e+00]],

        [[-4.3797e-01, -1.0956e+00, -2.0190e-01,  ..., -8.4370e-01,
           7.9637e-01, -1.7481e+00],
         [-7.5764e-02, -1.5055e+00, -4.5336e-01,  ..., -1.4586e+00,
           5.9188e-01, -1.1728e+00],
         [-5.7467e-01, -1.5970e+00, -2.4205e-01,  ..., -1.1665e+00,
           3.5625e-01, -1.7027e+00],
         [-6.0786e-01, -1.6419e+00, -4.6399e-01,  ..., -9.8483e-01,
           1.0737e-01, -2.1277e+00]],

        [[-3.3177e-01, -1.7451e+00,  1.4355e-02,  ..., -1.2838e+00,
           5.7532e-01, -8.6594e-01],
         [-2.7868e-01, -8.6525e-01, -1.3949e-01,  ..., -1.3605e+00,
           6.5276e-01, -2.4396e+00],
         [-2.7292e-01, -1.1105e+00,  7.9964e-01,  ..., -6.2190e-01,
           5.8817e-01, -1.5233e+00],
         [-4.2742e-01, -1.4441e+00, -7.5022e-01,  ..., -1.2979e+00,
           7.8852e-01, -1.9250e+00]],

        ...,

        [[-3.6524e-01, -1.5106e+00,  2.9579e-01,  ..., -2.7937e-01,
           6.7266e-01, -1.2341e+00],
         [-3.8605e-01, -1.6486e+00, -3.7752e-02,  ..., -1.2732e+00,
           4.6804e-01, -2.3217e+00],
         [-3.4213e-01, -1.5399e+00, -4.0308e-01,  ..., -1.2505e+00,
           5.9776e-01, -1.7740e+00],
         [-1.2069e-01, -1.4169e+00, -1.3556e-01,  ..., -1.2197e+00,
           3.1081e-01, -1.6615e+00]],

        [[-6.5329e-01, -7.0352e-01, -6.1517e-01,  ..., -8.9282e-01,
           3.0762e-01, -1.6347e+00],
         [-2.1693e-01, -1.7726e+00,  1.5867e-01,  ..., -1.3753e+00,
           9.1318e-01, -1.8599e+00],
         [-7.6427e-01, -1.3815e+00,  3.7743e-02,  ..., -1.2045e+00,
           1.2429e+00, -1.6488e+00],
         [-6.1292e-01, -1.6023e+00,  8.4807e-02,  ..., -1.3604e+00,
          -6.8352e-02, -1.7732e+00]],

        [[-4.6792e-01, -1.1046e+00,  6.7401e-03,  ..., -8.7795e-01,
           5.4581e-01, -1.8162e+00],
         [-8.0873e-02, -1.8467e+00, -3.5427e-01,  ..., -1.1418e+00,
           3.3144e-02, -1.6473e+00],
         [-2.7142e-01, -6.9620e-01,  8.8102e-02,  ..., -9.9534e-01,
           7.4653e-01, -1.8626e+00],
         [-9.6432e-01, -1.4604e+00,  1.2722e-03,  ..., -6.4023e-01,
           2.0406e-01, -1.0776e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.2388, -1.4127, -0.1016,  ..., -1.2192, -0.0838, -0.8181],
         [-0.7231, -2.9183,  0.2309,  ..., -1.2842,  0.7374, -0.3238],
         [-0.9028, -2.5586,  0.0287,  ..., -0.9888,  1.0596, -0.4773],
         [-0.5821, -2.6148,  0.0975,  ..., -1.2325,  0.5299, -1.1899]],

        [[ 0.0197, -2.0686, -0.1807,  ...,  0.0589,  0.5838, -0.6046],
         [-0.8866, -2.6571, -0.3757,  ..., -0.9541,  0.1310, -0.2703],
         [-1.1283, -2.0699, -0.3863,  ..., -1.4233,  0.4229, -1.0723],
         [-1.0976, -2.4742, -0.4834,  ..., -1.1051,  0.0105, -1.0813]],

        [[-0.8262, -2.1547, -0.2787,  ..., -1.4197,  0.5838, -0.4011],
         [-0.8663, -1.8980, -0.3775,  ..., -1.0917,  0.5284, -0.9670],
         [ 0.1192, -1.1348,  0.0374,  ..., -0.8898,  0.4047, -0.5729],
         [-1.0765, -2.3164, -0.7006,  ..., -0.9078,  0.6825, -0.6499]],

        ...,

        [[-0.6239, -2.3127, -0.1571,  ..., -0.6926,  0.2059, -0.3320],
         [-0.8127, -2.3778, -0.2148,  ..., -1.1432,  0.2893, -0.8676],
         [-0.8513, -2.4869, -0.4661,  ..., -0.8809,  0.5127, -0.7008],
         [-0.5700, -2.5721, -0.2917,  ..., -1.1763,  0.3484, -0.4480]],

        [[-0.8799, -1.9737, -0.2075,  ..., -1.2363,  0.7541, -1.0265],
         [-0.6645, -2.3459, -0.1033,  ..., -1.3928,  0.6374, -0.4514],
         [-0.9970, -2.2616, -0.2756,  ..., -0.4365,  0.6550, -0.5118],
         [-0.8299, -1.4533, -0.1465,  ..., -1.2796, -0.1173, -0.2861]],

        [[-0.7986, -2.3108, -0.1064,  ..., -1.1684,  0.2464, -0.4665],
         [-0.6725, -2.6555, -0.2079,  ..., -1.0201,  0.0524, -0.8748],
         [-1.2318, -2.0657,  0.0114,  ..., -1.1841,  0.5283, -1.0552],
         [-0.9717, -2.2368, -0.3283,  ..., -0.7891, -0.1297, -0.2485]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3083, -0.4194,  0.2803,  ..., -1.0593, -0.0799, -0.7301],
         [-1.8374, -0.8725, -0.0370,  ..., -1.3043, -0.6254, -0.4680],
         [-1.9954, -0.7932, -0.0090,  ..., -1.4892, -0.4119, -0.5472],
         [-0.8620, -1.2777, -0.0978,  ..., -1.6899, -0.4272, -0.7594]],

        [[-1.2339, -0.8690, -0.0627,  ..., -0.8408, -0.1483, -0.3544],
         [-1.8675, -0.9220,  0.2909,  ..., -1.0002, -0.7803, -0.6999],
         [-2.0531, -0.6592,  0.1170,  ..., -0.6845,  0.1206, -0.1761],
         [-1.3952, -0.9365, -0.4161,  ..., -1.5498, -0.5698, -0.8054]],

        [[-1.7511, -0.5902, -0.1317,  ..., -1.0263, -0.7715, -0.7466],
         [-2.0106, -0.3117, -0.2209,  ..., -1.2789, -0.6222, -0.7449],
         [-1.4441, -0.5473,  0.0291,  ..., -1.1742,  0.5724, -0.3765],
         [-1.8918, -0.8768, -0.4956,  ..., -1.2227, -0.7380, -0.4142]],

        ...,

        [[-1.2557, -0.9174,  0.0030,  ..., -0.9986, -0.9696, -0.2971],
         [-1.8886, -0.9928,  0.1019,  ..., -1.3137, -0.2471, -1.2135],
         [-1.1996, -0.9871, -0.1021,  ..., -1.0801, -0.4436,  0.0566],
         [-1.7201, -0.9463,  0.0705,  ..., -1.2267, -0.6414, -0.5876]],

        [[-1.6974, -0.6528,  0.2403,  ..., -1.1542, -0.1321, -0.9594],
         [-1.3190, -0.9493, -0.3244,  ..., -1.4240, -0.5404, -0.9777],
         [-0.9278, -1.1960, -0.0670,  ..., -0.8404, -0.5803, -0.5084],
         [-1.7732, -0.4257, -0.3996,  ..., -1.3087, -0.3005, -0.3382]],

        [[-1.6191, -1.0047, -0.0311,  ..., -0.6500, -0.9816, -0.5718],
         [-1.7738, -0.8373, -0.1141,  ..., -0.9275, -1.0256, -0.8361],
         [-1.7093, -1.0754,  0.3645,  ..., -0.4646, -0.8439, -0.7009],
         [-2.2252, -0.6909, -0.3540,  ..., -0.9577, -1.1981, -0.3387]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([600, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([1056, 4, 256]), pos_embed.shape: torch.Size([1056, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
encoder start :
	 src.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([1056, 4, 256]), k.shape: torch.Size([1056, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1056, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1056, 4, 256])
	 (after FFN) src2.shape: torch.Size([1056, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1056, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8420e+00, -1.7589e-01, -1.0132e-01,  ..., -1.1378e+00,
           2.2260e+00, -1.0634e+00],
         [-2.3783e+00, -2.2654e-01,  1.4672e-01,  ..., -1.3457e+00,
           1.7549e+00, -1.1869e+00],
         [-2.0941e+00, -5.5318e-02, -1.2364e-02,  ..., -4.0419e-01,
           1.9443e+00, -1.6797e-01],
         [-2.2341e+00,  2.9639e-01, -2.2532e-01,  ..., -1.2231e+00,
           1.9931e+00, -1.1084e+00]],

        [[-2.2195e+00,  1.0194e-01, -1.3930e-01,  ..., -4.7110e-01,
           1.6273e+00, -1.0980e+00],
         [-1.8452e+00, -1.1599e-01,  2.5998e-01,  ..., -1.2141e+00,
           1.6376e+00, -1.2305e+00],
         [-1.9623e+00,  1.7706e-01, -6.7317e-02,  ..., -1.1337e+00,
           1.7612e+00, -8.7113e-01],
         [-2.1916e+00,  1.0266e-01, -4.4860e-01,  ..., -1.3725e+00,
           9.4764e-01, -1.0578e+00]],

        [[-1.8280e+00, -1.1514e-02, -2.5957e-01,  ..., -5.4919e-01,
           1.7137e+00, -5.7660e-01],
         [-1.9964e+00, -3.7975e-01,  2.6514e-01,  ..., -1.2340e+00,
           8.9032e-01, -8.1615e-01],
         [-2.2140e+00,  1.1665e-01,  2.5627e-01,  ..., -1.3001e+00,
           1.9542e+00, -1.1273e+00],
         [-2.2275e+00,  4.7415e-01, -1.6937e-01,  ..., -1.0145e+00,
           2.1136e+00, -1.0975e+00]],

        ...,

        [[-2.1931e+00,  1.1642e-01,  5.8899e-01,  ..., -1.3734e+00,
           2.0185e+00, -1.2110e+00],
         [-1.9257e+00, -7.7313e-04,  2.2778e-01,  ..., -1.0628e+00,
           1.6996e+00, -1.2373e+00],
         [-2.0791e+00, -1.2051e-01,  2.4728e-01,  ..., -1.0156e+00,
           1.7148e+00, -1.2028e+00],
         [-1.8207e+00,  6.1799e-01, -2.0016e-01,  ..., -1.4911e+00,
           1.7955e+00, -1.4128e+00]],

        [[-1.9568e+00, -4.9821e-02,  5.4723e-01,  ..., -1.2790e+00,
           1.7367e+00, -1.1966e+00],
         [-2.3333e+00,  3.9926e-01, -1.8070e-01,  ..., -1.1201e+00,
           1.4356e+00, -1.3933e+00],
         [-2.3486e+00,  8.9615e-02, -3.7227e-01,  ..., -2.2302e-01,
           1.8680e+00, -1.2670e+00],
         [-2.0095e+00, -2.0959e-01,  4.0238e-02,  ..., -1.3015e+00,
           1.5826e+00, -1.5639e+00]],

        [[-2.2332e+00, -2.2913e-02,  2.7568e-01,  ..., -1.3299e+00,
           1.1651e+00, -1.1791e+00],
         [-2.0469e+00, -2.2120e-01, -1.5840e-01,  ..., -1.3498e+00,
           1.9947e+00, -9.3605e-01],
         [-2.1582e+00,  3.4450e-02, -4.2361e-01,  ..., -1.2029e+00,
           1.8318e+00, -6.9641e-01],
         [-2.0094e+00, -5.0101e-02,  4.3742e-02,  ..., -1.3541e+00,
           1.8556e+00, -1.3255e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.5396,  0.7173,  0.0146,  ..., -0.8263,  1.0053,  0.3768],
         [-0.8033,  0.2824,  0.1608,  ..., -0.3553,  1.0183,  0.2423],
         [-1.2414,  0.5974,  0.0590,  ...,  0.1691,  1.0102,  0.6024],
         [-1.0912,  1.0409,  0.2064,  ..., -0.7049,  1.3082,  0.2114]],

        [[-0.7614,  0.0785,  0.0808,  ..., -0.9404,  0.6920,  0.5031],
         [-0.9644,  0.5854,  0.2017,  ..., -0.9376,  0.8018,  0.2852],
         [-0.7822,  0.5819,  0.0056,  ..., -0.3353,  1.2236,  0.5171],
         [-1.7476,  0.4828, -0.3527,  ..., -0.6558,  0.4137,  0.0034]],

        [[-0.7054,  0.7687, -0.2394,  ..., -0.3601,  1.1609,  0.4745],
         [-1.3352,  0.3885,  0.3815,  ..., -0.2987,  0.8603,  0.6603],
         [-0.6609,  0.5057,  0.0630,  ..., -0.8267,  1.2105,  0.2670],
         [-1.0082,  0.8272, -0.1312,  ..., -0.5529,  1.3087,  0.2546]],

        ...,

        [[-0.8059,  0.6119,  0.0845,  ..., -0.5704,  0.8136,  0.0598],
         [-1.0969,  0.4393,  0.3008,  ..., -0.8879,  1.2714, -0.1217],
         [-0.8776,  0.6795,  0.3272,  ..., -0.5440,  0.9446,  0.2834],
         [-1.1042,  0.5019,  0.2052,  ..., -0.5352,  0.8337,  0.4348]],

        [[-0.9501,  0.9981,  0.3240,  ..., -0.2750,  0.9691,  0.1484],
         [-1.2084,  0.8727,  0.3289,  ..., -0.4377,  0.6860, -0.0837],
         [-0.3690,  0.7279, -0.3345,  ..., -0.3484,  0.7147,  0.0044],
         [-0.8610,  0.5942,  0.2489,  ..., -0.6925,  0.8433,  0.3596]],

        [[-1.2532,  0.5230,  0.3084,  ..., -0.7006,  0.8301,  0.1259],
         [-1.0864,  0.5784,  0.2449,  ..., -0.3820,  1.4467,  0.3910],
         [-0.6881,  0.2151, -0.1755,  ..., -0.3779,  1.0225,  0.6118],
         [-0.9875,  0.4663,  0.4163,  ..., -1.0102,  0.8614,  0.2813]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.1102, -0.8338,  0.0618,  ..., -0.9768,  0.8511, -1.9624],
         [-0.3959, -2.0088,  0.1029,  ..., -0.8000,  0.6542, -1.5287],
         [-0.6818, -1.5561,  0.6381,  ..., -0.4762,  0.5273, -2.0444],
         [-0.1382, -1.2529, -0.0587,  ..., -1.1860,  0.6504, -1.4164]],

        [[-0.4739, -0.4270,  0.1267,  ..., -1.1831,  0.2528, -0.9990],
         [-0.2708, -1.6829,  0.1929,  ..., -0.9839,  0.5080, -2.1306],
         [-0.3516, -1.1363, -0.0655,  ..., -0.3186,  0.8988, -1.8241],
         [-0.7562, -1.6304,  0.1076,  ..., -1.1561,  0.4410, -2.2818]],

        [[-0.6107, -1.0486, -0.1515,  ..., -0.5447, -0.0074, -2.0154],
         [-0.7922, -1.5566,  0.1552,  ..., -1.0083,  0.2484, -1.9546],
         [-0.3426, -2.1172,  0.3477,  ..., -0.9534,  0.9200, -1.5205],
         [-0.5157, -1.5016,  0.1834,  ..., -0.3648,  0.9536, -2.0019]],

        ...,

        [[-0.4551, -1.3524,  0.6206,  ..., -0.5684,  0.2553, -2.0989],
         [-0.5368, -2.1027,  0.0945,  ..., -1.3968,  1.3368, -2.6123],
         [-0.3998, -1.6403,  0.1725,  ..., -0.5821,  0.8242, -1.3865],
         [-0.4730, -1.8399,  0.6739,  ..., -1.0923,  0.5753, -1.8739]],

        [[-0.4765, -1.3940,  0.3097,  ..., -0.7508,  0.6280, -2.0758],
         [-0.6400, -1.4437,  0.6085,  ..., -0.6798,  0.8433, -2.0665],
         [-0.2356, -1.6273, -0.2026,  ..., -0.9190,  0.0168, -1.6247],
         [-0.0837, -1.6284,  0.0131,  ..., -1.1160,  0.6046, -1.4209]],

        [[-0.6546, -1.4611, -0.0374,  ..., -0.6866,  0.6976, -1.8320],
         [-0.5265, -1.7738,  0.1233,  ..., -0.8016,  0.8083, -2.4846],
         [-0.0128, -1.9491,  0.0771,  ..., -0.6182,  0.7011, -1.8130],
         [-0.5012, -1.9794,  0.3461,  ..., -1.3531,  0.7819, -2.1484]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-6.3372e-01, -1.8739e+00, -2.7009e-01,  ..., -9.5890e-01,
           1.1106e+00, -6.0434e-01],
         [-1.1730e+00, -2.6999e+00, -1.3238e-01,  ..., -9.7809e-01,
           3.8672e-01, -4.2892e-01],
         [-9.2177e-01, -2.0435e+00, -2.5575e-02,  ...,  1.8028e-01,
           5.0247e-01, -7.8053e-01],
         [-8.8081e-01, -2.3139e+00, -4.1354e-01,  ..., -9.7360e-01,
           1.3009e-01, -2.3093e-01]],

        [[-1.0562e+00, -1.8592e+00, -1.4014e-02,  ..., -1.1395e+00,
           6.8650e-01,  6.4273e-02],
         [-6.8016e-01, -2.5317e+00, -3.5442e-01,  ..., -1.3308e+00,
           4.0147e-01, -1.0857e+00],
         [-5.6822e-01, -2.2139e+00,  1.4693e-01,  ..., -4.5158e-01,
           6.9692e-01, -5.5960e-01],
         [-1.1207e+00, -2.2805e+00, -4.7520e-01,  ..., -1.0231e+00,
           2.3227e-01, -5.7675e-01]],

        [[-9.5550e-01, -2.0919e+00,  7.5438e-02,  ..., -9.6657e-01,
           3.1914e-01, -7.1583e-01],
         [-1.2589e+00, -2.2481e+00, -1.1511e-01,  ..., -1.0418e+00,
           1.0291e-01, -5.6589e-01],
         [-1.0075e+00, -1.4530e+00,  1.6066e-01,  ..., -1.1225e+00,
           8.4897e-01, -6.5719e-01],
         [-1.1468e+00, -2.4536e+00,  2.0879e-03,  ..., -9.0992e-01,
           1.1665e+00, -4.0204e-01]],

        ...,

        [[-8.7999e-01, -2.0242e+00,  1.4258e-01,  ..., -7.3095e-01,
           2.6628e-01, -9.3323e-01],
         [-1.0266e+00, -2.2882e+00, -1.8270e-01,  ..., -1.3786e+00,
           7.6651e-01, -1.0829e+00],
         [-9.5472e-01, -1.1462e+00,  1.7725e-01,  ..., -8.0436e-01,
           8.4521e-01, -4.4981e-01],
         [-1.3256e+00, -1.7013e+00,  2.3474e-01,  ..., -9.8875e-01,
           7.4548e-01, -4.7341e-01]],

        [[-1.0340e+00, -2.0283e+00,  1.9725e-01,  ..., -8.0968e-01,
           3.5764e-01, -6.3862e-01],
         [-9.3491e-01, -2.3242e+00,  2.3500e-02,  ..., -8.4456e-01,
           5.5557e-02, -5.9644e-01],
         [-1.1467e+00, -2.5261e+00, -3.0592e-01,  ..., -1.0903e+00,
           3.9386e-01, -3.9245e-01],
         [ 2.0462e-02, -1.2074e+00,  8.9394e-02,  ..., -8.5118e-01,
           5.3425e-01, -9.9793e-02]],

        [[-1.1530e+00, -1.8311e+00, -9.1908e-02,  ..., -7.3986e-01,
           3.4655e-01, -6.4022e-01],
         [-9.9936e-01, -2.3711e+00, -6.8966e-02,  ..., -4.3507e-02,
           3.0008e-01, -8.2776e-01],
         [ 1.0712e-01, -2.5108e+00, -1.5800e-01,  ..., -8.6384e-01,
           8.2279e-01, -5.7389e-01],
         [-1.2279e+00, -1.7572e+00,  2.4030e-01,  ..., -1.2435e+00,
           5.0308e-01, -7.2946e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8820, -0.5948, -0.4333,  ..., -1.1494, -0.4335, -0.7358],
         [-2.4049, -0.8824, -0.5697,  ..., -1.0766, -0.4397, -0.6935],
         [-1.5095, -1.0061, -0.0836,  ..., -0.6539, -0.1123, -0.1861],
         [-2.3304, -0.6234, -0.4449,  ..., -1.3944, -0.5927, -0.4234]],

        [[-1.8435, -0.6394,  0.1897,  ..., -1.3609, -0.8931, -0.7218],
         [-1.9907, -0.5231, -0.2194,  ..., -1.3830, -0.2522, -0.5942],
         [-2.2539, -0.8628,  0.1414,  ..., -0.6781,  0.2234, -0.6690],
         [-2.2450, -0.6215, -0.4937,  ..., -1.3811, -0.5398, -0.6002]],

        [[-2.2088, -0.7587, -0.0244,  ..., -1.3791,  0.2112, -0.5749],
         [-2.3336, -0.6894, -0.5675,  ..., -1.2529, -0.5928, -0.8055],
         [-2.2965,  0.1820, -0.1536,  ..., -1.0267, -0.1830, -0.1022],
         [-2.4784, -0.5744, -0.5594,  ..., -1.2819, -0.1779, -0.4614]],

        ...,

        [[-2.1482, -0.5960,  0.0524,  ..., -1.0653, -1.0026, -0.7860],
         [-2.1730, -0.8926, -0.1440,  ..., -0.6588, -0.2607, -0.9689],
         [-2.1536, -0.5791, -0.2534,  ..., -1.4998, -0.1517, -0.6710],
         [-1.4512, -0.4486, -0.2750,  ..., -1.6786,  0.1592, -0.7086]],

        [[-1.2272, -1.1486, -0.0939,  ..., -1.3102, -0.9275, -0.5279],
         [-2.0475, -0.9794, -0.2407,  ..., -1.0291, -0.7280, -0.7159],
         [-2.3114, -1.1946, -0.4193,  ..., -1.2584, -0.4478, -0.4366],
         [-1.0694, -0.1125,  0.2613,  ..., -1.6149, -0.5496, -0.4397]],

        [[-2.2819, -0.6412,  0.2644,  ..., -0.3021, -0.8537, -0.7420],
         [-2.1754, -0.8654, -0.4717,  ..., -1.0732,  0.0223, -0.5597],
         [-1.8793, -0.8028, -0.5045,  ..., -0.4420, -0.5649, -0.7254],
         [-2.6730, -0.5185, -0.0150,  ..., -0.9168, -0.5640, -0.6472]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1056, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([546, 4, 256]), pos_embed.shape: torch.Size([546, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
encoder start :
	 src.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([546, 4, 256]), k.shape: torch.Size([546, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([546, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([546, 4, 256])
	 (after FFN) src2.shape: torch.Size([546, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([546, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3096, -0.7253, -0.3910,  ..., -1.3290,  0.8407, -0.1586],
         [-2.1994, -0.1599, -0.3717,  ..., -0.9032,  1.9259, -1.2524],
         [-2.2027, -0.6587,  0.1782,  ..., -0.9615,  1.8985, -1.0190],
         [-2.6193, -0.3494, -0.6286,  ..., -0.2795,  1.1032, -1.1881]],

        [[-2.2500, -0.6015, -0.0175,  ..., -0.7091,  2.1769, -1.3707],
         [-1.9695, -0.1622, -0.4334,  ..., -1.0895,  2.0526, -1.4405],
         [-1.8869, -0.2306,  0.4153,  ..., -0.3145,  2.2325, -1.0542],
         [-2.2187, -0.3996, -0.3275,  ..., -1.2761,  1.8235, -1.2247]],

        [[-2.0436, -0.5195, -0.2797,  ..., -0.7849,  1.9586, -1.0315],
         [-0.4185, -0.5803, -0.0520,  ..., -1.0858,  1.9098, -1.1672],
         [-2.2027, -0.4395,  0.4233,  ..., -0.8932,  1.9831, -0.7213],
         [-2.1731, -0.4940,  0.0289,  ..., -1.1670,  1.7283, -1.0676]],

        ...,

        [[-2.3028, -0.3218, -0.2668,  ..., -1.3379,  0.6426, -1.3477],
         [-2.1892, -0.7295, -0.2190,  ..., -1.2682,  2.0992, -1.0300],
         [-2.1193, -0.7621, -0.1287,  ..., -0.7323,  2.1879, -1.0616],
         [-2.3615, -0.9271,  0.0712,  ..., -0.8006,  1.4457, -0.9712]],

        [[-2.3033, -0.5202,  0.5452,  ..., -1.2848,  1.4318, -1.1996],
         [-0.2350,  0.2049, -0.5480,  ..., -1.0838,  2.1208, -1.3156],
         [-1.9987, -0.2714, -0.1842,  ..., -0.7565,  2.2025, -1.2418],
         [-2.2297, -0.4902, -0.1582,  ..., -0.7713,  2.1011, -0.7462]],

        [[-0.5287, -0.8166, -0.4159,  ..., -0.9271,  1.0125, -1.1946],
         [-1.8453, -0.5247, -0.3919,  ..., -1.0336,  1.9327, -1.2464],
         [-1.7004, -0.5941, -0.2985,  ..., -0.9104,  1.9686, -1.2671],
         [-2.1982, -0.6182,  0.5287,  ..., -1.1910,  2.0177, -1.3522]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.2208e+00,  5.6496e-02,  1.1857e-01,  ..., -1.3161e+00,
           1.3116e+00,  5.2000e-01],
         [-8.8488e-01,  4.5618e-01, -6.3227e-02,  ...,  1.6730e-01,
           1.4115e+00,  7.7102e-02],
         [-1.3414e+00,  3.4718e-01,  8.1701e-01,  ..., -6.8418e-01,
           1.1530e+00,  4.0703e-01],
         [-1.3691e+00,  1.1106e-01, -1.9139e-01,  ..., -7.3818e-02,
           9.1614e-01, -2.5235e-01]],

        [[-9.8306e-01,  1.6832e-01,  2.8112e-01,  ..., -2.3036e-01,
           1.5842e+00, -1.6113e-01],
         [-8.1859e-01,  3.9485e-01, -9.2277e-02,  ..., -1.2216e-01,
           1.4406e+00, -1.9577e-01],
         [-7.3844e-01,  4.0116e-01,  2.9413e-01,  ...,  1.6128e-01,
           1.6897e+00,  2.3987e-01],
         [-1.0346e+00,  1.8683e-01,  1.5793e-01,  ..., -5.0209e-01,
           1.2090e+00, -1.4918e-01]],

        [[-1.0126e+00,  2.4021e-01,  6.0931e-01,  ..., -4.6002e-01,
           1.2842e+00,  2.2864e-01],
         [-4.7069e-01,  3.7969e-01,  2.9454e-01,  ..., -1.2641e-01,
           1.4197e+00, -7.7630e-04],
         [-7.9601e-01,  3.6253e-01,  3.6126e-01,  ...,  1.1876e-01,
           1.2238e+00,  3.8995e-01],
         [-1.5148e+00,  1.3348e-02,  2.6841e-01,  ..., -2.8055e-01,
           1.4081e+00,  1.1842e-01]],

        ...,

        [[-1.3111e+00,  9.9546e-02, -2.0513e-02,  ..., -1.7923e-01,
           8.5088e-01, -7.1668e-02],
         [-8.9840e-01,  1.5506e-01, -1.1958e-01,  ..., -3.3374e-01,
           1.2585e+00,  5.9514e-02],
         [-9.9200e-01,  1.0849e-01,  5.0784e-01,  ..., -2.0179e-01,
           1.4007e+00,  4.7816e-01],
         [-1.1388e+00, -2.8563e-01,  3.5286e-01,  ..., -1.5579e-01,
           1.2083e+00,  2.8996e-01]],

        [[-1.6515e+00,  4.8867e-01,  3.8803e-01,  ..., -2.5273e-01,
           1.2349e+00, -2.4793e-01],
         [-2.8413e-01,  8.0287e-01, -3.3597e-02,  ..., -1.2243e-01,
           1.3406e+00, -1.3378e-01],
         [-9.2229e-01,  4.9259e-01,  1.4564e-01,  ...,  5.3030e-02,
           1.4266e+00, -2.6410e-02],
         [-1.1045e+00, -3.5693e-01,  3.5979e-01,  ..., -2.3411e-01,
           1.5279e+00,  2.0166e-01]],

        [[-7.4502e-01,  2.8020e-01,  3.4999e-01,  ..., -2.5252e-01,
           8.7163e-01,  6.8086e-02],
         [-4.0971e-01,  1.4357e-01,  7.2762e-02,  ..., -2.7310e-01,
           1.2831e+00,  1.5405e-01],
         [-5.6053e-01,  5.7707e-01,  2.4121e-01,  ...,  3.6426e-03,
           1.2457e+00,  4.4174e-02],
         [-1.4690e+00,  3.0547e-02,  6.4991e-01,  ..., -4.0648e-01,
           1.4211e+00,  1.4487e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.8151, -1.8711, -0.3487,  ..., -0.9146,  0.7629, -1.5711],
         [-0.4787, -1.7108, -0.0949,  ..., -0.4681,  0.8397, -2.4079],
         [-0.9121, -1.4994,  0.8425,  ..., -1.0115,  0.9222, -2.1917],
         [-0.5718, -1.9149, -0.3112,  ..., -0.5491,  0.6775, -2.5617]],

        [[-0.4623, -2.0170, -0.0770,  ..., -0.2614,  0.3100, -2.3344],
         [-0.4407, -1.7912, -0.4659,  ...,  0.0733,  0.7818, -2.2908],
         [-0.3941, -1.9197, -0.2647,  ..., -0.5355,  1.1328, -1.1222],
         [-0.5622, -1.5431,  0.0118,  ..., -0.7633,  0.7275, -2.3596]],

        [[-0.5874, -2.0920,  0.1606,  ..., -0.9126,  0.9060, -2.0852],
         [-0.4033, -2.0661,  0.0647,  ..., -0.6939,  0.7579, -1.6564],
         [-0.4214, -1.7801,  0.6759,  ..., -0.4699,  0.7488, -2.2929],
         [-0.4694, -2.0300,  0.2064,  ..., -0.8704,  0.6699, -2.4739]],

        ...,

        [[-0.9829, -2.0225,  0.0392,  ..., -0.6726,  0.6744, -1.3786],
         [-0.3271, -2.0926,  0.4331,  ..., -0.9297,  0.7814, -2.1860],
         [-0.6837, -1.8291,  0.1140,  ..., -0.7939,  0.7975, -0.9172],
         [-0.6780, -1.7902,  0.6345,  ..., -0.5807,  1.0524, -1.7315]],

        [[-0.9917, -1.7480,  0.0258,  ..., -0.6134,  1.0089, -2.5710],
         [-0.3781, -1.6794, -0.1645,  ..., -0.4303,  0.6562, -2.4392],
         [-0.5295, -1.6921,  0.1376,  ..., -0.2032,  0.6758, -1.7417],
         [-0.2758, -1.7044,  0.0986,  ..., -0.4558,  0.5157, -1.7157]],

        [[-0.3472, -1.7055,  0.2992,  ..., -0.7300,  0.7552, -1.5076],
         [-0.1539, -1.8076,  0.2675,  ..., -0.4926,  0.7851, -2.1525],
         [-0.3407, -1.5690,  0.0644,  ..., -0.3313, -0.0081, -2.2372],
         [-0.7561, -0.9986,  0.2121,  ..., -0.4863,  0.9497, -2.3491]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.0283, -2.3202, -0.0601,  ..., -1.1520,  0.8895, -0.2244],
         [-1.1238, -1.9275,  0.1093,  ..., -0.6394,  0.3796, -0.5667],
         [-0.9390, -2.0676,  0.3921,  ..., -1.4409,  1.2346, -0.4877],
         [-1.0650, -1.2939, -0.2675,  ..., -1.0383,  0.5202, -0.5924]],

        [[-1.0772, -1.3865,  0.4118,  ..., -0.6459,  0.8907, -0.7741],
         [-0.9295, -1.9831, -0.0622,  ..., -0.1351,  0.8539, -0.8000],
         [-0.8991, -2.1555, -0.0255,  ..., -0.8190,  0.7479,  0.1341],
         [-0.8321, -1.9305, -0.3800,  ..., -0.8902,  0.6534, -0.6968]],

        [[-0.5458, -2.3213,  0.3726,  ..., -0.7763,  0.6848, -0.6603],
         [-0.8560, -2.3609,  0.1162,  ...,  0.0707,  0.2371, -0.4549],
         [-0.1703, -2.0938,  0.0136,  ..., -0.4822,  0.7129, -0.7551],
         [-0.6748, -2.3221, -0.3037,  ..., -0.1789,  0.5214, -0.8974]],

        ...,

        [[-1.2693, -2.3709,  0.2249,  ..., -1.2749,  0.4524, -0.3406],
         [-0.2332, -2.3567,  0.0094,  ..., -1.1491,  0.6457, -0.7600],
         [-0.2564, -2.4756,  0.0901,  ..., -1.2400,  0.3144, -0.1302],
         [-0.7503, -2.3972, -0.0810,  ..., -0.8100,  0.7564, -0.5384]],

        [[-1.3879, -2.4035,  0.0029,  ..., -0.9131,  0.7530, -1.0092],
         [-1.0561, -1.7751, -0.2250,  ..., -1.1746,  0.4210, -0.8636],
         [-0.8450, -2.0042,  0.1175,  ..., -0.6890,  0.6860, -0.4321],
         [-1.0036, -2.3561, -0.1240,  ..., -0.9335,  0.7190, -0.2430]],

        [[-0.1573, -2.1147,  0.1849,  ..., -0.9294,  0.7102, -0.2140],
         [-0.9654, -2.3162, -0.0242,  ..., -0.7746,  0.5066, -0.8830],
         [-0.8448, -2.0959, -0.1698,  ..., -0.8808,  0.2948, -0.3204],
         [-0.8622, -1.9338,  0.3756,  ..., -0.9605,  0.8317, -0.7123]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1653, -0.7428, -0.3260,  ..., -1.1707, -0.1822, -0.3949],
         [-2.4949, -0.6880, -0.0558,  ..., -1.3062, -0.3819, -0.9760],
         [-2.5156, -0.5705, -0.2909,  ..., -1.5979, -0.3619, -0.9161],
         [-2.4918, -0.0248, -0.1006,  ..., -1.4222,  0.2701, -0.9718]],

        [[-1.8706, -0.4525,  0.3556,  ..., -0.6845, -0.2650, -0.6651],
         [-1.9464, -0.9337,  0.0145,  ..., -0.9144,  0.0218, -1.0631],
         [-2.1459, -0.7647, -0.3884,  ..., -1.4654, -0.4377, -0.5677],
         [-1.9723, -0.5853, -0.7303,  ..., -1.3296, -0.3972, -0.8828]],

        [[-1.9678, -0.9118, -0.1851,  ..., -1.1124,  0.2753, -1.1020],
         [-2.3059, -1.0576,  0.0803,  ..., -0.8205, -0.4312, -1.0498],
         [-1.1602, -0.5402, -0.0196,  ..., -1.2786, -0.5273, -1.0450],
         [-1.5066, -0.7288, -0.8261,  ..., -0.7098, -0.2783, -0.9615]],

        ...,

        [[-1.9832, -1.1796, -0.1633,  ..., -1.4142, -0.7080, -0.6308],
         [-1.1535, -1.0590, -0.4943,  ..., -1.6091,  0.1563, -0.7729],
         [-2.0653, -0.9200, -0.2545,  ..., -1.5582, -0.8338, -0.6525],
         [-2.3652, -0.6054, -0.5138,  ..., -1.4263, -0.2686, -0.4465]],

        [[-1.6533, -1.1853, -0.3087,  ..., -1.5056, -0.1051, -1.0228],
         [-0.7648, -0.7291, -0.3530,  ..., -1.5582, -0.4327, -1.1847],
         [-2.3464, -0.4766, -0.2509,  ..., -1.2296, -0.3220, -0.7030],
         [-2.4679, -0.8452, -0.8663,  ..., -1.1332, -0.0957, -0.2796]],

        [[-0.7669, -0.8170, -0.1779,  ..., -1.2724, -0.3409, -0.9161],
         [-2.1411, -0.9068, -0.2855,  ..., -1.1573, -0.0645, -1.1695],
         [-2.2322, -0.7201, -0.4692,  ..., -0.4432, -0.4619, -0.3880],
         [-2.4704, -0.7734, -0.0597,  ..., -1.4280,  0.1092, -1.0388]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([546, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([704, 4, 256]), pos_embed.shape: torch.Size([704, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
encoder start :
	 src.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([704, 4, 256]), k.shape: torch.Size([704, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([704, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([704, 4, 256])
	 (after FFN) src2.shape: torch.Size([704, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([704, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3609, -0.7114, -0.1288,  ..., -0.7423,  1.8784, -0.8451],
         [-2.0019, -0.8501, -0.4139,  ..., -0.7351,  1.8485, -1.1709],
         [-2.1137, -0.6254, -0.1258,  ..., -0.7695,  1.6720, -1.2860],
         [-2.0751, -0.5426, -0.2678,  ..., -1.1417,  1.7419, -1.3114]],

        [[-0.2859, -0.8919, -0.4353,  ..., -1.0978,  2.1260, -1.3728],
         [-1.9667, -0.8823, -0.6139,  ..., -0.9478,  2.3629, -1.1327],
         [-2.1477, -0.7262, -0.2741,  ..., -1.1575,  1.9061, -1.2166],
         [-2.3216, -0.9227, -0.5008,  ..., -0.8806,  1.8110, -1.4164]],

        [[-2.2806, -0.9312, -0.5133,  ..., -0.8544,  1.9610, -0.9638],
         [-2.2605, -0.5849, -0.4930,  ..., -0.5543,  1.3608, -1.0923],
         [-2.1748, -0.6107, -0.2389,  ..., -1.2336,  2.0107, -1.3066],
         [-2.4539, -0.4829, -0.1474,  ..., -1.0285,  1.9654, -1.0584]],

        ...,

        [[-2.3229, -0.5467, -0.1785,  ..., -0.4825,  1.9006, -1.2128],
         [-2.3311, -0.8892, -0.3510,  ..., -0.8813,  0.5776, -1.3954],
         [-2.3631, -0.7077, -0.5007,  ..., -1.1783,  1.7238, -1.0226],
         [-2.3624, -0.0198, -0.2121,  ..., -0.8718,  2.1025, -1.7455]],

        [[-2.2193, -0.9860,  0.2555,  ..., -0.6653,  1.7103, -1.2304],
         [-2.2004, -0.6388, -0.4566,  ..., -1.1872,  1.8114, -1.3179],
         [-2.4328, -1.0199, -0.2982,  ..., -0.3388,  1.5389, -0.8694],
         [-2.2462, -0.6650, -0.1117,  ..., -0.7181,  2.0479, -1.4641]],

        [[-2.1298,  0.0219, -0.3917,  ..., -0.7400,  1.7173, -0.8330],
         [-2.4068, -0.6037, -0.8967,  ..., -0.9089,  2.0594, -1.2003],
         [-2.6571, -0.7182, -0.1327,  ..., -0.3764,  1.8887, -0.8608],
         [-2.3583, -0.7928, -0.2511,  ..., -0.7922,  1.9381, -1.2850]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9903,  0.0653,  0.3161,  ..., -0.1206,  0.8923, -0.0455],
         [-0.6940,  0.1099,  0.0251,  ..., -0.5061,  1.2671, -0.2263],
         [-1.1073, -0.0395,  0.4017,  ..., -0.0095,  1.1325, -0.4229],
         [-0.7027,  0.0816,  0.4206,  ..., -0.4628,  0.8141, -0.2234]],

        [[ 0.1305,  0.0129,  0.5630,  ...,  0.0584,  1.3660, -0.2369],
         [-1.1425, -0.2577,  0.3264,  ..., -0.1707,  1.6120, -0.0023],
         [-0.8019,  0.0357,  0.0946,  ..., -0.5242,  1.0194, -0.1116],
         [-1.3225, -0.2641,  0.1394,  ..., -0.1412,  1.5617, -0.2432]],

        [[-1.7107, -0.0837,  0.0871,  ..., -0.4314,  0.8772,  0.1367],
         [-1.4647,  0.5399, -0.0961,  ...,  0.0714,  1.0432,  0.1433],
         [-1.3619,  0.0964,  0.0603,  ..., -0.4836,  1.0680,  0.0971],
         [-0.8508,  0.1229,  0.4174,  ..., -0.2300,  1.0405, -0.1350]],

        ...,

        [[-1.3736,  0.0666, -0.1779,  ..., -0.3298,  1.2173, -0.2051],
         [-1.5231, -0.0881,  0.4854,  ..., -0.4244,  0.8190, -0.3860],
         [-0.9792, -0.2664,  0.0748,  ..., -0.2269,  1.2827,  0.1105],
         [-1.2481,  0.3014,  0.4327,  ..., -0.8891,  1.4724, -0.5790]],

        [[-1.2404, -0.2089,  0.3032,  ..., -0.3337,  1.2054,  0.0535],
         [-1.0966,  0.1082,  0.1541,  ..., -0.3783,  1.1874, -0.3025],
         [-1.1490, -0.2251, -0.5520,  ...,  0.0469,  1.1018,  0.6046],
         [-1.2341, -0.1627, -0.0300,  ..., -0.3491,  1.3510, -0.2290]],

        [[-1.5803,  0.3951, -0.1313,  ...,  0.0733,  0.8370,  0.4054],
         [-1.5644,  0.3491, -0.2351,  ..., -0.3095,  1.3066, -0.0655],
         [-1.4934, -0.4588,  0.3708,  ..., -0.1222,  0.6800,  0.0705],
         [-1.4547, -0.4273,  0.5448,  ..., -0.2323,  1.1887, -0.2293]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.7869, -1.6151,  0.3442,  ..., -0.3596,  1.0208, -1.4249],
         [-0.3745, -1.5839, -0.1015,  ..., -0.5504,  1.0584, -1.7535],
         [-0.7188, -1.6891,  0.5802,  ..., -0.3427,  0.8484, -2.5546],
         [-0.3542, -1.6234,  0.6789,  ..., -0.6010,  0.5666, -2.0650]],

        [[-0.3849, -1.1338,  0.2279,  ..., -0.2997,  0.9188, -2.2245],
         [-0.8626, -1.6844,  0.7982,  ..., -0.6376,  1.1157,  0.0372],
         [-0.6006, -1.9089,  0.1696,  ..., -0.4084,  0.9074, -1.9608],
         [-0.9782, -1.6859,  0.1917,  ..., -0.5643,  0.3401, -2.3998]],

        [[-0.9657, -1.9804,  0.3898,  ..., -0.7373,  0.3303, -2.0765],
         [-0.2588, -1.6813,  0.0053,  ..., -0.1831,  0.6520, -1.9881],
         [-0.8072, -1.9310, -0.0343,  ..., -0.6097,  0.8124, -2.4665],
         [-0.6143, -1.9860,  0.1654,  ..., -0.2810,  0.7283, -1.9779]],

        ...,

        [[-0.5243, -1.8770, -0.3526,  ...,  0.1629,  0.7135, -1.5063],
         [-0.8669, -1.2136,  0.4126,  ..., -0.8755,  0.3530, -2.3050],
         [-0.8118, -2.0767,  0.0123,  ..., -0.8835,  0.9102, -2.1722],
         [-0.8376, -1.6117,  0.3247,  ..., -0.6769,  0.6177, -2.4659]],

        [[-0.3619, -2.0838,  0.2680,  ..., -0.0366,  0.8206, -1.7956],
         [-0.5238, -2.1263,  0.2045,  ..., -0.3696,  0.9646, -0.8492],
         [-0.7684, -2.1040, -0.4673,  ..., -0.2390,  0.3325, -2.1168],
         [-0.9912, -1.8466,  0.1677,  ..., -0.5439,  0.9234, -2.2465]],

        [[-0.8777, -1.0392,  0.0046,  ..., -0.2828,  0.6563, -2.1410],
         [-0.9909, -0.7744,  0.1801,  ..., -0.5583,  1.0568, -2.1492],
         [-0.9123, -2.2216,  0.1378,  ..., -0.2979,  0.5315, -2.1594],
         [-1.0625, -1.8242,  0.3837,  ..., -0.5722,  0.6613, -2.4219]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9775, -2.1894,  0.4653,  ..., -0.8680,  0.7799, -0.3059],
         [-1.1826, -2.1136, -0.1797,  ...,  0.0131,  0.9937, -0.3145],
         [-1.1491, -1.1933,  0.3137,  ..., -1.0740,  1.1472, -0.8615],
         [-0.5081, -2.2097,  0.5450,  ..., -0.9335,  0.4075, -0.6510]],

        [[-0.8851, -1.7704,  0.1006,  ..., -0.3908,  0.6575, -0.7350],
         [-0.7994, -2.5003,  0.7092,  ..., -0.0452,  1.1724,  0.1309],
         [-1.3327, -2.1169,  0.0101,  ...,  0.1455,  0.7492, -0.2651],
         [-1.4141, -1.8080, -0.2017,  ..., -0.8877,  0.3370, -0.5745]],

        [[-1.1399, -2.3810, -0.1678,  ..., -0.5687,  0.0318, -0.4106],
         [-0.9315, -1.2206,  0.4079,  ..., -0.6840,  0.7456, -0.2159],
         [-0.8520, -2.0835, -0.0489,  ..., -0.9486,  0.7307, -0.6513],
         [-0.8227, -2.3045, -0.0455,  ..., -0.6211,  0.5629, -0.3945]],

        ...,

        [[-0.7563, -1.6797, -0.3587,  ..., -0.6468,  0.5266, -0.3775],
         [-0.7834, -2.2458,  0.0749,  ..., -0.8035,  0.7510, -0.5633],
         [-0.9266, -2.3692,  0.4804,  ..., -0.8936,  0.7493, -0.3289],
         [-0.9648, -2.0968, -0.0771,  ..., -0.6215,  0.7228, -0.5868]],

        [[-0.6782, -1.9997,  0.1216,  ..., -0.7039,  1.0324, -0.3301],
         [-0.6516, -2.4938,  0.1993,  ..., -0.7792,  0.9215,  0.0861],
         [-0.3897, -2.3430, -0.2571,  ..., -0.6347,  0.6663, -0.4540],
         [-0.8108, -2.1958,  0.0450,  ..., -0.0627,  0.6752, -0.5817]],

        [[-1.0564, -1.5942,  0.3221,  ...,  0.0203,  0.5725, -0.4495],
         [-0.8992, -2.0639,  0.1421,  ..., -0.8305,  0.7022, -0.9580],
         [-1.0064, -2.2607,  0.0717,  ..., -0.8043,  0.8260, -0.4850],
         [-1.2620, -2.4903,  0.2686,  ..., -1.0616,  0.5709, -0.5856]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9819, -1.3380, -0.3941,  ..., -1.4373, -0.2588, -0.9911],
         [-2.4626, -0.8299, -0.5732,  ..., -1.0778, -0.2809, -0.6670],
         [-2.6156, -0.5166, -0.1262,  ..., -1.2319,  0.1757, -1.1248],
         [-2.0926, -0.9916, -0.4019,  ..., -1.2714,  0.3261, -1.2214]],

        [[-2.6390, -0.8247, -0.3430,  ..., -1.0323, -0.3927, -0.9347],
         [-2.4980, -1.2699, -0.1086,  ..., -1.1705, -0.1109, -0.6348],
         [-2.1391, -1.3060, -0.0050,  ..., -0.9982, -0.5974, -0.9366],
         [-2.5210, -0.7342, -0.2559,  ..., -1.1369, -0.0554, -0.4280]],

        [[-1.7608, -1.0605, -0.1836,  ..., -1.2354, -0.5694, -0.7798],
         [-2.2807, -0.8242,  0.1145,  ..., -1.1909,  0.0851, -0.3469],
         [-2.5520, -0.9289, -0.0846,  ..., -1.4298, -0.2379, -0.5481],
         [-2.4648, -1.0717, -0.5883,  ..., -1.5678, -0.3251, -0.7377]],

        ...,

        [[-2.0660, -0.6725, -0.9790,  ..., -0.0843, -0.7145, -1.2061],
         [-2.2650, -0.8071, -0.3176,  ..., -1.3248, -0.6396, -0.9738],
         [-2.4080, -1.2778, -0.3559,  ..., -1.3479, -0.3100, -0.8117],
         [-2.2888, -1.2265, -0.4504,  ..., -1.2043, -0.5707, -1.1200]],

        [[-2.2507, -1.0344, -0.5678,  ..., -0.5801, -0.3332, -0.8758],
         [-2.1432, -1.2493, -0.0880,  ..., -1.3513,  0.1143, -0.7546],
         [-1.9501, -1.3796, -0.9095,  ..., -0.5567, -0.4712, -0.9699],
         [-2.2527, -1.1643, -0.1159,  ..., -0.9743,  0.0281, -0.4900]],

        [[-2.1694, -0.8302, -0.2837,  ..., -1.0155,  0.0127, -0.8309],
         [-1.7142, -0.5259, -0.4530,  ..., -1.2042, -0.4680, -1.0202],
         [-2.4613, -1.1938, -0.0626,  ..., -0.4213, -0.2621, -1.0497],
         [-2.4032, -1.4193, -0.2905,  ..., -0.5893, -0.5313, -0.6785]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([704, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([744, 4, 256]), pos_embed.shape: torch.Size([744, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
encoder start :
	 src.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([744, 4, 256]), k.shape: torch.Size([744, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([744, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([744, 4, 256])
	 (after FFN) src2.shape: torch.Size([744, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([744, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7350, -0.8306, -0.2977,  ..., -0.9354,  1.8318, -1.3192],
         [-2.4275, -1.1839, -0.1715,  ..., -0.6335,  1.5816, -1.5255],
         [-0.7861, -0.2277, -0.8221,  ..., -1.2400,  1.7945, -1.0909],
         [-2.4048, -1.2186, -0.0420,  ..., -0.5866,  2.0154, -1.3087]],

        [[-2.0254, -0.8203, -0.5065,  ..., -0.7290,  1.7111, -1.2823],
         [-2.6476, -0.2017, -0.5646,  ..., -1.3254,  1.8205, -1.3625],
         [-0.7499, -0.5389, -0.1457,  ..., -0.9944,  1.7957, -1.3822],
         [-2.5018, -1.0121, -0.2952,  ..., -1.1676,  0.6012, -1.1098]],

        [[-2.2363, -1.0240, -0.3085,  ..., -0.7859,  1.3836, -0.8446],
         [-2.2513, -0.8409, -0.2022,  ..., -0.7886,  1.8129, -1.1720],
         [-2.3342, -0.8719, -0.5494,  ..., -0.9816,  1.7887, -1.1521],
         [-2.3438, -1.0937, -0.2806,  ..., -0.2932,  1.4445, -0.8504]],

        ...,

        [[-0.5954, -0.6937, -0.1230,  ..., -1.0038,  1.9849, -1.1987],
         [-2.3202, -1.1997,  0.0674,  ..., -0.8769,  1.4605, -1.0645],
         [-2.3811, -0.8757, -0.3376,  ..., -1.0232,  1.7063, -1.1422],
         [-2.2128, -1.4141, -0.4067,  ..., -1.0312,  2.1952, -1.0734]],

        [[-2.2939, -0.1665, -0.5240,  ..., -0.8492,  1.3188, -1.3779],
         [-2.3166, -1.0443, -0.3008,  ..., -0.6543,  1.3747, -0.9696],
         [-2.6820,  0.0536, -0.2632,  ..., -1.3516,  0.8850, -1.1079],
         [-2.2283, -1.1118, -0.3661,  ..., -0.8526,  1.8481, -0.7897]],

        [[-0.4667, -1.1347, -0.4880,  ..., -1.3367,  2.0521, -1.3013],
         [-1.9217,  0.0836, -0.4484,  ..., -0.4793,  1.6882, -0.5836],
         [-0.2340, -1.0864, -0.5958,  ..., -0.7972,  1.9263, -0.8212],
         [-2.4548, -1.1440, -0.3219,  ..., -0.8327,  1.9194, -1.3208]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6902, -0.1505,  0.3709,  ..., -0.4932,  0.9687, -0.1610],
         [-1.2780, -0.3627,  0.4984,  ..., -0.1034,  0.9943, -0.0982],
         [-0.6955,  0.1778, -0.0049,  ...,  0.0421,  0.9886, -0.0828],
         [-1.2866, -0.4095,  0.4831,  ..., -0.3927,  1.2329, -0.3396]],

        [[-1.3790, -0.2501,  0.3604,  ..., -0.2337,  1.0786,  0.0312],
         [-1.3089,  0.1172,  0.1044,  ..., -0.8063,  1.3022, -0.5316],
         [-0.8365, -0.1145,  0.3310,  ..., -0.7948,  0.6732, -0.1054],
         [-1.4799, -0.2299,  0.2654,  ..., -0.2463,  0.3771, -0.1679]],

        [[-0.8846, -0.3609, -0.2322,  ..., -0.3006,  0.8241,  0.0250],
         [-1.7626, -0.1862,  0.1567,  ..., -0.4135,  1.0733, -0.3256],
         [-1.3877, -0.9961, -0.2318,  ..., -0.3214,  1.3303, -0.0386],
         [-1.2020, -0.3227, -0.0472,  ...,  0.1383,  0.8211, -0.1195]],

        ...,

        [[-0.6240, -0.5720,  0.2937,  ..., -0.4961,  1.4238, -0.4279],
         [-1.3671, -0.4634,  0.4979,  ...,  0.0744,  1.1349, -0.2660],
         [-1.5035, -0.2576,  0.5137,  ..., -0.8321,  1.6527, -0.3870],
         [-1.4821, -0.2658,  0.1926,  ..., -0.4821,  1.0294, -0.2623]],

        [[-1.2333,  0.0626,  0.1162,  ..., -0.2314,  0.8972, -0.4933],
         [-1.3228, -0.2908,  0.2659,  ..., -0.3023,  1.1752, -0.3464],
         [-1.4426,  0.0860,  0.2535,  ..., -0.4105,  0.6843, -0.1825],
         [-1.2969, -0.3690, -0.0100,  ..., -0.2930,  1.0290, -0.2304]],

        [[-0.6057, -0.2150, -0.0133,  ..., -0.5326,  1.2155, -0.2380],
         [-1.1907,  0.2657,  0.0728,  ..., -0.0274,  1.0414, -0.1718],
         [-0.2954, -0.3123,  0.0854,  ..., -0.1807,  1.0534, -0.1847],
         [-1.3433, -0.3183,  0.2714,  ..., -0.7169,  0.9110, -0.1106]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6857, -2.1086,  0.2332,  ..., -1.0727,  0.8902, -2.3293],
         [-1.0124, -2.2874,  0.2230,  ..., -0.6803,  0.1612, -2.3760],
         [-0.3640, -1.6522,  0.3434,  ...,  0.0607,  0.9201, -2.1215],
         [-0.7373, -2.2359,  0.0310,  ..., -0.4291,  0.9781, -0.1439]],

        [[-0.9047, -1.9057,  0.4286,  ..., -0.2579,  0.9721, -2.1965],
         [-0.8600, -2.1846,  0.6765,  ..., -0.7703,  0.8737, -2.2455],
         [-0.6717, -1.8516,  0.2017,  ..., -0.4885,  0.6695, -2.1114],
         [-0.8364, -1.8910,  0.0371,  ...,  0.1089,  0.4370, -2.1002]],

        [[-0.7041, -1.8093,  0.0806,  ..., -0.3715,  0.8089, -1.9622],
         [-0.9514, -1.2418,  0.2230,  ..., -0.7665,  0.7682, -2.0523],
         [-0.8692, -2.6327, -0.0837,  ..., -0.5609,  0.5357, -2.0743],
         [-0.7137, -2.0855,  0.2810,  ...,  0.1441,  0.7070, -2.0412]],

        ...,

        [[-0.6334, -2.0853,  0.4847,  ..., -0.8290,  0.8344, -2.2992],
         [-1.0209, -2.0199,  0.3922,  ..., -0.5348,  0.7406, -2.2042],
         [-0.7878, -2.2880,  0.8672,  ..., -0.5475,  1.1132, -2.3304],
         [-0.8089, -1.7839,  0.2117,  ..., -0.6006,  0.6832, -2.1860]],

        [[-1.0895, -2.0501,  0.1575,  ..., -0.4003,  0.8784, -2.4581],
         [-0.4380, -1.1703,  0.7078,  ..., -0.2400,  0.9825, -1.0882],
         [-0.9801, -1.9454, -0.0797,  ..., -0.5994,  0.3647, -2.3395],
         [-0.9044, -2.2356,  0.0159,  ..., -0.6497,  0.6545, -1.6623]],

        [[-0.2713, -0.3459,  0.5887,  ..., -0.4637,  0.8574, -1.8935],
         [-0.7862, -1.6422,  0.0701,  ..., -0.1789,  0.8501, -2.3969],
         [-0.3109, -1.3827,  0.0140,  ..., -0.7717,  0.8475, -2.1161],
         [-0.9736, -0.7667,  0.2712,  ..., -0.4011,  0.5523, -2.2108]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6646, -2.4484,  0.0320,  ..., -1.3127,  0.9950, -0.3837],
         [-0.4118, -2.6641, -0.1555,  ..., -1.0589,  0.3247, -0.7001],
         [-1.0195, -2.3586,  0.0728,  ..., -0.4578,  0.3495, -0.5519],
         [-0.6828, -2.5240, -0.1472,  ..., -0.5585,  1.0455,  0.7619]],

        [[-1.0562, -2.2394,  0.0079,  ..., -0.4745,  0.8700, -0.3716],
         [-0.7918, -2.5817,  0.1497,  ..., -0.9377,  0.7756, -0.2335],
         [-1.0430, -2.3486,  0.1400,  ..., -0.3798,  0.6800, -0.5750],
         [-1.1572, -2.5535,  0.2242,  ...,  0.2404,  0.4256, -0.2375]],

        [[-0.3133, -2.5461, -0.1967,  ..., -0.7926,  0.7300, -0.6499],
         [-0.8041, -1.9072,  0.2362,  ..., -0.9291,  0.4132, -0.1906],
         [-1.2294, -2.2562, -0.1052,  ..., -0.9220,  0.5965, -0.3201],
         [-1.1389, -2.2910,  0.3504,  ..., -0.8792,  0.9704, -0.4826]],

        ...,

        [[-0.9508, -2.3518,  0.3420,  ..., -0.3939,  0.6777, -0.1924],
         [-1.1632, -2.4835, -0.0969,  ..., -0.9071,  0.5959, -0.7865],
         [-0.8300, -2.4722,  0.3033,  ..., -0.6690,  0.9447, -0.2697],
         [-1.1928, -2.3081,  0.2664,  ..., -0.9759,  0.0102, -0.1024]],

        [[-0.5232, -2.7191,  0.0718,  ..., -0.5218,  0.9553, -0.8200],
         [-0.7899, -1.7621,  0.4215,  ..., -0.5080,  0.2727,  0.2564],
         [-1.0474, -2.4202, -0.1552,  ..., -0.4615,  0.8261, -0.3943],
         [-0.8991, -2.0205, -0.0173,  ..., -0.7453,  0.7778, -0.4569]],

        [[-0.7012, -1.8100,  0.3416,  ..., -0.0191,  0.8026, -0.1105],
         [-0.8263, -2.2567,  0.0144,  ..., -0.5742,  0.6276, -0.3417],
         [-0.7667, -2.2338,  0.1650,  ..., -0.7692,  0.6990, -0.3733],
         [-0.9712, -2.1233,  0.2917,  ...,  0.0593,  0.3367, -0.9411]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1757, -1.2263, -0.6992,  ..., -1.2862, -0.3836,  0.1987],
         [-1.7656, -1.4683, -0.4661,  ..., -1.5798, -0.8098, -1.0494],
         [-2.3298, -1.3042, -0.0581,  ..., -1.2135, -0.4427,  0.0372],
         [-2.2959, -1.3909, -0.6737,  ..., -1.0315, -0.0712, -0.4826]],

        [[-1.8116, -1.0670, -0.6491,  ..., -0.3680, -0.2783, -1.0688],
         [-2.1523, -1.4602, -0.3895,  ..., -1.4774,  0.0026, -0.6116],
         [-2.5488, -1.4933, -0.0867,  ..., -1.1435, -0.3626, -0.5680],
         [-2.3915, -1.5063, -0.3893,  ..., -0.8980, -0.2020, -0.3410]],

        [[-1.8308, -1.2923, -0.9607,  ..., -1.1459, -0.0925, -0.2918],
         [-2.1584, -0.8777, -0.4841,  ..., -1.2318, -0.1813, -0.9046],
         [-2.5603, -1.4241, -0.3368,  ..., -0.5029, -0.5674, -0.6063],
         [-2.1822, -1.4922, -0.1706,  ..., -1.4337, -0.3136, -0.3787]],

        ...,

        [[-2.5436, -1.3694, -0.0076,  ..., -1.4949, -0.0853, -0.5771],
         [-2.5365, -1.0350, -0.5627,  ..., -1.6894, -0.1076, -0.8706],
         [-1.4593, -1.2229, -0.3707,  ..., -1.6001, -0.2917, -0.7722],
         [-2.4999, -1.3695, -0.3605,  ..., -0.4450, -0.1415, -0.7644]],

        [[-2.2721, -1.7532, -0.3786,  ..., -1.0067, -0.4426, -1.1826],
         [-2.2806, -1.0462, -0.1814,  ..., -0.9645, -0.6355, -0.0961],
         [-2.3974, -1.6974, -0.7706,  ..., -0.9055, -0.6530, -1.2064],
         [-2.4718, -0.8950, -0.6937,  ..., -1.1912, -0.2224, -1.3030]],

        [[-2.1285, -1.1914,  0.0275,  ..., -0.0506, -0.5303, -0.7401],
         [-1.4325, -1.3518, -0.1874,  ..., -1.4616, -0.5495, -0.8008],
         [-1.3295, -1.1493, -0.5799,  ..., -1.5558, -0.3334, -0.9750],
         [-2.2770, -1.1478, -0.3196,  ..., -0.8182, -0.0113, -0.9973]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([744, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
Epoch: [0]  [   10/29571]  eta: 2:09:02  lr: 0.000100  class_error: 100.00  loss: 58.2481 (58.2636)  loss_ce: 2.7672 (2.5275)  loss_bbox: 4.9966 (4.8811)  loss_giou: 2.2781 (2.2505)  loss_ce_0: 2.7359 (2.5604)  loss_bbox_0: 5.0466 (4.9464)  loss_giou_0: 2.2439 (2.2426)  loss_ce_1: 2.6267 (2.4835)  loss_bbox_1: 5.0839 (4.9619)  loss_giou_1: 2.2426 (2.2354)  loss_ce_2: 2.5166 (2.5263)  loss_bbox_2: 5.1082 (4.9299)  loss_giou_2: 2.2566 (2.2436)  loss_ce_3: 2.8127 (2.5947)  loss_bbox_3: 5.1494 (4.9053)  loss_giou_3: 2.2552 (2.2499)  loss_ce_4: 2.7944 (2.5925)  loss_bbox_4: 4.9779 (4.8775)  loss_giou_4: 2.2658 (2.2546)  loss_ce_unscaled: 2.7672 (2.5275)  class_error_unscaled: 100.0000 (98.4655)  loss_bbox_unscaled: 0.9993 (0.9762)  loss_giou_unscaled: 1.1391 (1.1253)  cardinality_error_unscaled: 5.2500 (15.0682)  loss_ce_0_unscaled: 2.7359 (2.5604)  loss_bbox_0_unscaled: 1.0093 (0.9893)  loss_giou_0_unscaled: 1.1220 (1.1213)  cardinality_error_0_unscaled: 5.2500 (14.7955)  loss_ce_1_unscaled: 2.6267 (2.4835)  loss_bbox_1_unscaled: 1.0168 (0.9924)  loss_giou_1_unscaled: 1.1213 (1.1177)  cardinality_error_1_unscaled: 5.2500 (15.4545)  loss_ce_2_unscaled: 2.5166 (2.5263)  loss_bbox_2_unscaled: 1.0216 (0.9860)  loss_giou_2_unscaled: 1.1283 (1.1218)  cardinality_error_2_unscaled: 5.2500 (17.9545)  loss_ce_3_unscaled: 2.8127 (2.5947)  loss_bbox_3_unscaled: 1.0299 (0.9811)  loss_giou_3_unscaled: 1.1276 (1.1249)  cardinality_error_3_unscaled: 5.2500 (14.7273)  loss_ce_4_unscaled: 2.7944 (2.5925)  loss_bbox_4_unscaled: 0.9956 (0.9755)  loss_giou_4_unscaled: 1.1329 (1.1273)  cardinality_error_4_unscaled: 5.2500 (15.2045)  time: 0.2619  data: 0.0350  max mem: 7055
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([713, 4, 256]), pos_embed.shape: torch.Size([713, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
encoder start :
	 src.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([713, 4, 256]), k.shape: torch.Size([713, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([713, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([713, 4, 256])
	 (after FFN) src2.shape: torch.Size([713, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([713, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.2966, -1.1846, -0.6045,  ..., -1.0159,  1.7072, -1.2965],
         [-2.5731, -1.1629,  0.0354,  ..., -1.1098,  1.9588, -1.5082],
         [-2.1974, -0.9701, -0.6236,  ..., -0.6825,  1.6469, -0.5714],
         [-2.0929, -0.9885, -0.1621,  ..., -0.6285,  1.8933, -1.3495]],

        [[-2.4381, -0.9539, -0.3628,  ..., -0.6935,  1.9161, -0.8023],
         [-2.4375, -1.1139, -0.3510,  ..., -0.9662,  2.0346, -1.4098],
         [-2.4665, -1.0674, -0.3368,  ..., -0.6847,  1.9538, -1.6302],
         [-0.4796, -1.0887, -0.3541,  ..., -0.7105,  1.4910, -1.5126]],

        [[-2.6803, -0.9763, -0.1958,  ..., -1.0645,  1.8284, -1.4042],
         [-0.7118, -1.0963, -0.3773,  ..., -0.8569,  1.5385, -1.2603],
         [-2.3455, -0.7331, -0.3843,  ..., -0.9850,  2.0360, -0.3495],
         [-2.5481, -0.3211, -0.3176,  ..., -0.9952,  1.9519, -1.1629]],

        ...,

        [[-2.6240, -0.8877, -0.2834,  ..., -1.2881,  1.6641, -1.5568],
         [-2.2087, -1.1225, -0.2037,  ..., -0.8387,  1.5506, -1.1929],
         [-2.4318, -0.8775, -0.3035,  ..., -0.7744,  1.7851, -0.5452],
         [-2.1306, -0.7768, -0.5739,  ..., -0.9143,  1.8436, -0.9553]],

        [[-2.5238, -1.1438, -0.3573,  ..., -0.4910,  1.8001, -1.3262],
         [-2.4664, -1.1770, -0.4079,  ..., -1.3233,  0.9297, -0.8344],
         [-2.1236, -1.1218, -0.1783,  ..., -0.6899,  1.3659, -1.3836],
         [-2.4621, -0.9195, -0.2421,  ..., -0.8434,  1.8344, -1.1063]],

        [[-0.4545, -1.0817, -0.4473,  ..., -0.7807,  1.8557, -0.6845],
         [-0.6099, -1.2336, -0.0756,  ..., -1.0108,  2.1357, -1.2376],
         [-2.6012, -1.0173, -0.4396,  ..., -0.9961,  0.5205, -1.4670],
         [-2.5563, -0.9683, -0.2720,  ..., -0.8353,  1.6096, -0.8703]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5683e+00, -4.2577e-01,  5.9223e-02,  ..., -3.7248e-01,
           1.0790e+00, -5.4866e-01],
         [-1.5503e+00, -4.3837e-01,  6.4818e-01,  ..., -8.1544e-01,
           1.1391e+00, -4.5036e-01],
         [-9.0374e-01, -5.2671e-01, -1.2011e-03,  ..., -6.5571e-01,
           7.8322e-01,  8.1421e-02],
         [-1.2200e+00, -4.6502e-01,  3.0441e-01,  ..., -1.7738e-01,
           1.3665e+00, -4.7962e-01]],

        [[-1.0120e+00, -3.4940e-01,  2.9322e-01,  ..., -2.0354e-01,
           9.9130e-01, -2.0151e-01],
         [-1.3977e+00, -6.7141e-01,  5.0487e-01,  ..., -4.4235e-01,
           1.0999e+00, -5.9284e-01],
         [-1.2303e+00, -5.3687e-01,  3.9544e-01,  ..., -4.4822e-01,
           1.0645e+00, -6.1586e-01],
         [-5.9166e-01, -6.6845e-01,  2.5669e-01,  ..., -5.9351e-01,
           5.9514e-01, -3.3419e-01]],

        [[-1.6922e+00, -1.8365e-01,  7.8341e-01,  ..., -3.8108e-01,
           8.8880e-01, -5.6624e-01],
         [-6.7715e-01, -4.6217e-01,  5.9598e-01,  ..., -5.2544e-01,
           1.0886e+00, -5.3360e-01],
         [-1.5691e+00, -5.0755e-01,  5.1829e-02,  ..., -5.4628e-01,
           1.3006e+00,  5.8626e-03],
         [-1.4788e+00,  1.1853e-01,  2.0623e-01,  ..., -6.9877e-01,
           1.4645e+00, -3.4665e-01]],

        ...,

        [[-1.4375e+00, -1.3013e-01,  4.6646e-01,  ..., -6.5148e-01,
           8.0316e-01, -5.5714e-01],
         [-1.4761e+00, -4.6615e-01,  4.5110e-01,  ..., -5.1224e-01,
           8.2301e-01, -3.2629e-01],
         [-1.4706e+00, -8.9638e-01,  4.5441e-01,  ..., -2.6372e-01,
           9.2908e-01, -1.5095e-01],
         [-1.2282e+00, -4.3259e-01,  5.0889e-01,  ..., -1.0645e+00,
           9.1335e-01, -2.4168e-01]],

        [[-1.3419e+00, -6.0154e-01,  4.1403e-01,  ..., -3.9288e-01,
           1.1446e+00, -6.3931e-01],
         [-1.7006e+00, -5.2297e-01,  4.2775e-01,  ..., -8.8610e-01,
           6.7371e-01,  6.8814e-02],
         [-1.2175e+00, -4.3418e-01,  1.0335e-01,  ..., -4.8693e-01,
           1.0590e+00, -1.6810e-01],
         [-1.3293e+00, -4.7708e-01,  8.2526e-01,  ..., -6.6736e-01,
           1.2287e+00, -5.4578e-01]],

        [[-2.5162e-01, -5.0315e-01,  8.9497e-02,  ..., -5.0809e-01,
           9.3127e-01, -1.5596e-01],
         [-3.8723e-01, -8.9917e-01,  5.2442e-01,  ..., -4.5439e-01,
           8.5034e-01, -3.1832e-01],
         [-1.0362e+00, -5.3614e-01,  3.2142e-01,  ..., -4.1438e-01,
           7.0859e-01, -4.2610e-01],
         [-1.3951e+00,  3.3986e-02,  5.4366e-01,  ..., -3.4022e-01,
           8.4637e-01, -3.1617e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.2225, -2.3499,  0.2648,  ..., -0.1521,  0.6120, -2.2629],
         [-0.9227, -1.9929,  0.3053,  ..., -0.7830,  0.6222, -2.1355],
         [-0.4343, -0.3910,  0.4431,  ..., -0.8166,  0.6842, -1.9544],
         [-0.8850, -1.9138,  0.2612,  ..., -0.9115,  1.2053, -2.5469]],

        [[-1.2705, -2.3479,  0.4331,  ..., -0.2402,  0.7343, -2.2542],
         [-0.8802, -2.0928,  0.4692,  ..., -0.4983,  0.4730, -2.1695],
         [-0.8420, -1.6757,  0.6169,  ..., -0.0530,  0.1283, -2.1026],
         [-0.9152, -2.1000,  0.2444,  ..., -0.5773,  0.7290, -2.2674]],

        [[-1.1723, -2.2248,  0.8998,  ...,  0.1346,  0.8287, -2.5038],
         [-0.6589, -2.0793,  0.4947,  ..., -0.7927,  1.0134, -2.7149],
         [-1.0488, -1.8216,  0.5529,  ..., -0.4159,  0.1835, -2.3740],
         [-0.8635, -1.0762,  0.3684,  ..., -0.6639,  0.8250, -2.2274]],

        ...,

        [[-0.7153, -1.8502,  0.2053,  ..., -0.5616,  0.9582, -2.5067],
         [-0.9911, -2.0782,  0.5719,  ..., -0.6669,  0.3850, -1.7569],
         [-1.0758, -2.2892,  0.9536,  ..., -0.0504,  0.7119, -1.7073],
         [-0.9063, -2.2483,  0.1847,  ..., -0.7975,  0.5257, -2.2760]],

        [[-1.1202, -2.4041,  0.4617,  ..., -0.6212,  0.9359, -2.4128],
         [-0.9509, -1.2400,  0.3685,  ..., -0.8493,  0.6891, -2.0093],
         [-1.0144, -2.1234, -0.0590,  ..., -0.4090,  0.9457, -1.3475],
         [-1.0096, -2.0567,  0.3988,  ..., -0.7502,  0.6956, -2.4559]],

        [[-0.7406, -1.3393,  0.4988,  ..., -0.2292,  0.8428, -1.8446],
         [-0.7535, -2.2874,  0.3532,  ..., -0.4429,  0.2771, -1.9700],
         [-1.0226, -1.9279,  0.8989,  ..., -0.2874,  0.8512, -2.5412],
         [-0.8980, -1.1275,  0.2573,  ..., -0.4348,  0.7224, -2.4699]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3145, -2.6735,  0.3618,  ..., -0.7085,  0.7470, -0.2910],
         [-0.7273, -2.4834,  0.1446,  ..., -1.0985,  0.4103, -0.1881],
         [-0.7603, -0.8762,  0.3045,  ..., -1.0242,  0.5879, -0.4078],
         [-0.8885, -2.3534,  0.3742,  ..., -0.9663,  0.9703, -0.5825]],

        [[-1.2898, -2.6024,  0.0695,  ..., -0.7527,  0.5960, -0.3959],
         [-0.9335, -2.5033,  0.2258,  ..., -0.7719,  0.8302, -0.5165],
         [-1.0111, -2.6763,  0.5203,  ...,  0.0455,  0.9034, -0.2451],
         [-0.8147, -2.3352,  0.4605,  ..., -0.8390,  0.7863, -0.1634]],

        [[-1.3529, -2.4862,  0.7717,  ..., -0.6202,  0.8016, -0.5100],
         [-0.9554, -2.6706,  0.2578,  ..., -0.9396,  1.1513, -0.6092],
         [-0.8310, -2.5170,  0.4206,  ..., -1.1220,  0.3093, -0.4091],
         [-1.1234, -2.2466,  0.0813,  ..., -1.2014,  0.8886, -0.6037]],

        ...,

        [[-0.8303, -2.4474,  0.2381,  ..., -1.1801,  1.1382, -0.2657],
         [-0.7089, -1.5130,  0.6019,  ..., -0.9305,  0.4173,  0.1349],
         [-0.9650, -2.7152,  0.2406,  ..., -0.7231,  1.0265, -0.1810],
         [-0.9101, -2.4394,  0.2336,  ..., -1.4058,  0.6049, -0.6998]],

        [[-1.2883, -2.6302,  0.2180,  ..., -0.6471,  0.7672, -0.9754],
         [-1.4123, -2.3124,  0.4643,  ..., -1.0179,  1.2011, -0.2622],
         [-1.1703, -2.2745,  0.1784,  ..., -0.7308,  0.9245,  0.1907],
         [-0.9940, -2.3308,  0.4222,  ..., -1.1540,  1.1531, -0.5608]],

        [[-0.2371, -2.7030,  0.0078,  ..., -0.7321,  1.0003, -0.2086],
         [-1.1888, -2.7141,  0.1962,  ..., -0.6453,  0.3715, -0.5716],
         [-1.2607, -2.3525,  0.4665,  ..., -0.7199,  0.2515, -0.6389],
         [-0.7510, -2.1623,  0.0359,  ..., -0.8778,  0.7812, -0.6465]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1267, -1.7347, -0.4219,  ..., -1.4001, -0.2040, -0.6857],
         [-2.3731, -1.5596, -0.1775,  ..., -1.3604, -0.8226,  0.0961],
         [-1.6350, -0.6889, -0.5477,  ..., -1.5648, -0.3510, -0.9297],
         [-2.3642, -1.2222, -0.0728,  ..., -1.5737, -0.2899, -1.2426]],

        [[-2.3583, -1.5163, -0.5135,  ..., -1.4260, -0.2039, -0.9203],
         [-2.5024, -1.3689, -0.5049,  ..., -1.5327, -0.1374, -0.4629],
         [-2.7539, -1.5794,  0.3373,  ..., -0.8850, -0.5707, -0.7871],
         [-1.2897, -1.0727, -0.2418,  ..., -1.4112, -0.4477, -0.8704]],

        [[-2.4076, -1.4613, -0.1208,  ..., -1.1010, -0.5072, -1.1288],
         [-1.2248, -1.1515, -0.3982,  ..., -1.5759,  0.1351, -0.9162],
         [-2.2387, -1.5374, -0.7182,  ..., -1.1122,  0.0143, -1.1938],
         [-2.0497, -1.3261,  0.0388,  ..., -1.5832, -0.4236, -1.1990]],

        ...,

        [[-2.5682, -1.4283,  0.1076,  ..., -1.6654, -0.3111, -0.9982],
         [-2.0288, -1.0419, -0.2501,  ..., -1.4800, -0.7256, -0.7991],
         [-2.3475, -1.5362, -0.4292,  ..., -1.3603,  0.0725, -0.8612],
         [-2.3585, -1.0881, -0.5113,  ..., -1.6971, -0.2755, -1.0681]],

        [[-1.2552, -1.8852, -0.2091,  ..., -1.4186, -0.4651, -1.3568],
         [-2.5135, -1.0820, -0.2493,  ..., -0.5548, -0.3900, -0.9468],
         [-1.6377, -1.2401, -0.2289,  ..., -1.5563, -0.3318, -0.8412],
         [-2.0704, -1.5448, -0.1236,  ..., -1.5147, -0.4781, -0.7687]],

        [[-2.0581, -1.4616, -0.3174,  ..., -1.2275, -0.0592, -0.3365],
         [-2.4150, -1.3394, -0.4572,  ..., -1.3638, -0.6044, -1.1089],
         [-2.2577, -1.1801,  0.0182,  ..., -0.8964, -0.7543, -0.4647],
         [-2.2851, -1.0756, -0.3664,  ..., -1.4270, -0.2989, -0.8709]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([713, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([768, 4, 256]), pos_embed.shape: torch.Size([768, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
encoder start :
	 src.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([768, 4, 256]), k.shape: torch.Size([768, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([768, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([768, 4, 256])
	 (after FFN) src2.shape: torch.Size([768, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([768, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3989, -1.0298, -0.6383,  ..., -0.8613,  1.7394, -0.8464],
         [-2.8500, -1.3039, -0.0793,  ..., -0.4674,  1.8195, -1.3586],
         [-2.3168, -0.5107, -0.3667,  ..., -1.0044,  1.8700, -1.2738],
         [-2.6092, -0.4013, -0.5208,  ..., -0.7052,  0.5609, -1.3040]],

        [[-0.7597, -1.0096, -0.1701,  ..., -0.6771,  1.8468, -0.4382],
         [-2.1736, -0.7098, -0.6484,  ..., -0.6444,  1.8747, -1.5511],
         [-2.5361, -0.3223, -0.0284,  ..., -1.0754,  1.5385, -0.8187],
         [-0.7083, -1.4106, -0.1374,  ..., -0.4879,  1.6643, -1.4525]],

        [[-2.2275, -1.1925,  0.0785,  ..., -0.6964,  1.7073, -1.3743],
         [-2.3033, -1.1989, -0.4713,  ..., -0.7522,  1.7147, -1.1417],
         [-2.3807, -1.2502, -0.8287,  ..., -0.5500,  1.6960, -1.2679],
         [-2.4900, -0.7851, -0.2265,  ..., -0.6514,  1.8428, -1.3225]],

        ...,

        [[-0.8292, -1.1649, -0.3004,  ..., -0.9900,  1.8541, -0.8842],
         [-2.7793, -0.8836, -0.8129,  ..., -1.0505,  1.8206, -1.3434],
         [-2.6415, -1.0311, -0.0312,  ..., -1.2250,  1.9505, -0.8358],
         [-2.4248, -1.2062, -0.1239,  ..., -0.9278,  0.6965, -1.0751]],

        [[-2.4962, -1.2248,  0.0848,  ..., -0.8890,  1.9340, -0.6234],
         [-2.3159, -1.0576, -0.2338,  ..., -0.9324,  1.3436, -1.6088],
         [-2.9587, -1.0566, -0.3068,  ..., -0.7337,  1.9392, -1.1381],
         [-2.4244, -1.1343, -0.1913,  ..., -0.8349,  1.4910, -1.3987]],

        [[-1.9587, -1.3069, -0.3886,  ..., -0.9245,  1.7312, -1.4502],
         [-2.5784, -1.2500, -0.4839,  ..., -0.9234,  0.5231, -1.5062],
         [-2.7147, -0.6720,  0.3276,  ..., -1.1347,  1.9461, -1.4823],
         [-2.4906, -1.1394, -0.3127,  ..., -0.7648,  1.8816, -1.6261]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3813, -0.1873,  0.1703,  ..., -0.3788,  0.9686, -0.4039],
         [-1.2995, -0.3421,  0.3385,  ..., -0.4372,  0.5664, -0.6663],
         [-1.2171, -0.2337,  0.3625,  ..., -0.6043,  1.0954, -0.4124],
         [-1.1651, -0.4971,  0.6202,  ..., -0.6048,  0.6065, -0.1331]],

        [[-1.0015, -0.5109,  0.6386,  ..., -0.9895,  0.9077,  0.0341],
         [-1.0751, -0.4032,  0.1327,  ..., -0.3335,  0.9598, -0.5286],
         [-1.4485, -0.2657,  0.6586,  ..., -0.4851,  0.8804, -0.2712],
         [-0.4741, -0.6198,  0.2868,  ..., -0.4587,  0.7281, -0.6173]],

        [[-1.4222, -0.4052,  0.4101,  ..., -0.4530,  0.6834, -0.6884],
         [-1.2453, -0.6715,  0.1275,  ..., -0.3164,  0.8176, -0.7491],
         [-1.2987, -0.3810, -0.1033,  ..., -0.4843,  0.7530, -0.5121],
         [-1.0123, -0.4814,  0.2013,  ..., -0.2375,  0.8824, -0.4628]],

        ...,

        [[-0.2954, -1.1250,  0.6882,  ..., -0.6135,  0.8903, -0.2769],
         [-1.8068, -1.0951,  0.5112,  ..., -0.6229,  0.5817, -0.4559],
         [-1.7006, -0.2817,  0.5742,  ..., -0.8513,  1.0275, -0.7049],
         [-1.2917, -0.6807,  0.8653,  ..., -0.6315,  0.3875, -0.4167]],

        [[-1.0480, -0.5385,  0.6475,  ..., -0.6144,  0.8057, -0.1046],
         [-1.6615, -0.2768,  0.3998,  ..., -0.4802,  0.6354, -0.7095],
         [-1.6569, -1.1525,  0.6679,  ..., -0.2670,  0.8288, -0.3907],
         [-1.1932, -0.4742,  0.3070,  ..., -0.5530,  0.8526, -0.3862]],

        [[-1.4206, -0.6141,  0.0908,  ..., -0.7937,  0.9707, -0.4451],
         [-1.6446, -0.4974, -0.3399,  ..., -0.5656,  0.3580, -0.2350],
         [-1.1208, -0.5901,  0.9997,  ..., -0.5682,  0.7813, -0.8019],
         [-1.4163, -0.4276,  0.4333,  ..., -0.5481,  0.5969, -0.6149]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.0699, -1.3332,  0.1624,  ..., -0.6610,  0.7651, -2.3512],
         [-1.0217, -1.8576,  0.2905,  ..., -0.4489,  0.7342, -1.3911],
         [-0.7442, -2.3995,  0.4553,  ..., -0.7150,  1.0486, -0.7917],
         [-0.7889, -1.9451,  0.4231,  ..., -0.5851,  0.5715, -2.1648]],

        [[-0.4326, -1.5588,  0.5843,  ..., -0.9004,  0.6980, -1.8862],
         [-0.9005, -2.2238,  0.5320,  ..., -0.4421, -0.1203, -1.9476],
         [-0.9413, -2.1872,  1.0961,  ..., -0.3746,  0.7038, -1.0902],
         [-0.7192, -2.2825,  0.4982,  ..., -0.7971,  0.8680, -1.4325]],

        [[-1.3568, -1.6965, -0.3725,  ..., -0.6136,  0.4389, -1.8972],
         [-1.2357, -1.9941,  0.4114,  ...,  0.1941,  0.5974, -2.3299],
         [-1.0182, -1.3079,  0.4138,  ..., -0.6122,  0.5594, -1.8545],
         [-1.0752, -1.0245,  0.5026,  ...,  0.0403,  0.5900, -2.1876]],

        ...,

        [[-0.5842, -2.1685,  0.3133,  ..., -0.6874,  0.7481, -1.0642],
         [-1.3770, -2.5104,  0.4436,  ...,  0.0110,  0.7682, -2.4880],
         [-0.8327, -0.9463,  1.0562,  ..., -0.3496,  0.5229, -2.3811],
         [-1.0987, -2.0420,  0.7795,  ..., -0.6814,  0.2177, -2.3711]],

        [[-0.9602, -1.3079,  0.5875,  ..., -0.8137,  0.7287, -1.9073],
         [-1.1574, -1.9876,  0.6767,  ..., -0.8328,  0.5560, -2.5748],
         [-1.4327, -2.3769, -0.4746,  ..., -0.2648,  0.3961, -2.2576],
         [-1.0561, -1.9936,  0.5525,  ..., -0.7274,  0.7505, -2.3896]],

        [[-0.6894, -1.9733, -0.3725,  ..., -0.6797,  0.0438, -2.0985],
         [-0.8938, -0.8762,  0.2007,  ..., -1.1648,  0.3877, -2.0870],
         [-1.0363, -2.1904,  0.6502,  ..., -1.1223,  0.6976, -2.3685],
         [-0.7277, -1.1881, -0.2320,  ..., -0.4734,  0.6454, -2.1675]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.4790, -2.1859,  0.7451,  ..., -0.9705,  0.9328, -0.5897],
         [-0.9579, -2.5603,  0.1014,  ..., -0.6886,  0.8735,  0.0988],
         [-0.7382, -1.7304,  0.3082,  ..., -0.7819,  0.8020, -0.1071],
         [-1.0733, -2.7418,  0.0783,  ..., -0.2842,  0.6319, -0.0810]],

        [[-0.4530, -2.3121,  0.1843,  ..., -0.9852,  0.9895, -0.5816],
         [-0.8824, -2.5561,  0.4440,  ..., -0.1226,  0.7002, -0.6622],
         [-1.0628, -2.9385,  0.7090,  ..., -0.6955,  0.9818, -0.0107],
         [-0.8375, -2.9171,  0.1595,  ..., -0.6591,  0.8535,  0.1881]],

        [[-0.9569, -2.2778,  0.2928,  ..., -0.7390,  0.6500, -0.4188],
         [-1.1493, -2.0591,  0.5402,  ..., -0.1463,  0.9877, -0.3669],
         [-0.6987, -2.4483,  0.3810,  ..., -0.7725,  0.6521, -0.4116],
         [-1.1783, -2.0938,  0.3839,  ..., -0.6278,  0.5360, -0.4109]],

        ...,

        [[-0.6662, -2.7839,  0.3963,  ..., -1.2102,  0.8433, -0.3972],
         [-1.5690, -2.7714,  0.1138,  ..., -0.6170,  0.8549, -0.5817],
         [-0.8847, -1.9475,  0.9920,  ..., -0.4300,  0.8969, -0.9925],
         [-1.1309, -2.6470,  0.5494,  ..., -1.1253,  0.3839, -0.5657]],

        [[-0.9229, -2.3072,  0.4403,  ..., -0.8756,  1.2021, -0.1031],
         [-0.9209, -2.7546,  0.8209,  ..., -0.8050,  0.6750, -0.5299],
         [-1.2210, -2.4387, -0.0944,  ..., -0.4543,  0.9798, -0.9372],
         [-0.7920, -2.5101,  0.2095,  ..., -1.0171,  0.5955, -0.5564]],

        [[-0.3319, -2.4639, -0.0887,  ..., -0.9592,  0.4562,  0.0317],
         [-0.8223, -1.1060,  0.3148,  ..., -1.2089,  0.7143, -0.1912],
         [-0.9757, -2.7044,  0.5375,  ..., -1.2530,  0.5964, -0.5831],
         [-0.9441, -2.1832, -0.1153,  ..., -0.5097,  0.5478, -0.2319]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.0577e+00, -1.3441e+00,  1.4114e-01,  ..., -1.6780e+00,
          -3.0810e-01, -1.3004e+00],
         [-1.8851e+00, -1.3351e+00, -5.1710e-01,  ..., -1.3295e+00,
          -2.0557e-01, -1.7660e-01],
         [-2.0708e+00, -8.9407e-01, -8.1813e-02,  ..., -6.7584e-01,
          -3.2214e-01, -9.5252e-01],
         [-9.6353e-01, -1.5140e+00, -1.6757e-01,  ..., -1.2918e-01,
          -4.8135e-02, -7.3270e-01]],

        [[-2.1041e+00, -1.1850e+00, -1.1547e-01,  ..., -1.3341e+00,
           4.1323e-01, -1.2733e+00],
         [-2.1313e+00, -1.2997e+00,  6.6207e-03,  ..., -9.4249e-01,
          -1.6336e-03, -1.4882e+00],
         [-2.2770e+00, -1.6406e+00, -4.1227e-01,  ..., -1.4854e+00,
          -1.0928e-01, -8.2314e-01],
         [-2.1178e+00, -2.0863e+00, -6.1515e-01,  ..., -1.5077e+00,
           5.1754e-01, -6.5804e-01]],

        [[-2.2025e+00, -1.1892e+00, -6.6580e-01,  ..., -9.3587e-01,
          -4.4141e-01, -9.9053e-01],
         [-2.1110e+00, -1.3507e+00, -7.5129e-02,  ..., -9.5953e-01,
          -5.7931e-02, -3.4792e-01],
         [-2.2185e+00, -1.2821e+00, -7.7589e-02,  ..., -1.5175e+00,
          -5.5770e-01, -8.5182e-01],
         [-1.5529e+00, -1.2737e+00, -1.1655e-01,  ..., -4.3404e-01,
          -3.6946e-01, -9.1016e-01]],

        ...,

        [[-2.2902e+00, -1.4904e+00, -3.6227e-01,  ..., -1.6562e+00,
           7.1641e-02, -1.5161e+00],
         [-1.6522e+00, -1.4689e+00, -3.5560e-01,  ..., -1.2504e+00,
          -6.0052e-01, -4.7296e-01],
         [-2.0016e+00, -1.3013e+00, -3.2118e-03,  ..., -1.1164e+00,
           4.8013e-01, -1.4949e+00],
         [-2.3703e+00, -1.3836e+00, -6.3973e-03,  ..., -1.4979e+00,
          -5.2703e-01, -1.0651e+00]],

        [[-1.6402e+00, -1.2904e+00, -1.3499e-01,  ..., -1.5810e+00,
          -3.0751e-01, -7.3526e-01],
         [-1.4565e+00, -1.8896e+00,  4.3057e-02,  ..., -1.2050e+00,
          -3.1864e-01, -1.1930e+00],
         [-2.3760e+00, -1.3744e+00, -1.8853e-01,  ..., -1.3023e+00,
          -4.4947e-01, -7.2589e-01],
         [-1.9893e+00, -1.2247e+00, -5.2784e-01,  ..., -1.5645e+00,
          -4.4544e-01, -1.1259e+00]],

        [[-1.9163e+00, -1.6016e+00, -4.6938e-01,  ..., -1.5048e+00,
          -4.1385e-01, -6.0833e-01],
         [-1.4198e+00, -5.9727e-01, -2.3458e-01,  ..., -1.2754e+00,
          -6.6132e-01, -6.3893e-01],
         [-1.8076e+00, -1.6827e+00, -1.9627e-01,  ..., -8.2654e-01,
          -4.7445e-02, -4.4732e-01],
         [-2.3912e+00, -1.3345e+00, -7.7494e-02,  ..., -8.6246e-01,
          -4.5276e-01, -3.3765e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([768, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([552, 4, 256]), pos_embed.shape: torch.Size([552, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
encoder start :
	 src.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([552, 4, 256]), k.shape: torch.Size([552, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([552, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([552, 4, 256])
	 (after FFN) src2.shape: torch.Size([552, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([552, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.4790, -1.1549, -0.2753,  ..., -0.8419,  1.6209, -1.6482],
         [-2.2155, -1.1293, -0.4065,  ..., -0.7721,  1.5015, -1.6366],
         [-0.0854, -1.0510, -0.0078,  ..., -0.7082,  1.6531, -1.4702],
         [-2.5271, -1.0282, -0.1067,  ..., -0.7001,  1.7885, -1.2429]],

        [[-2.4850, -1.2685, -0.5018,  ..., -0.6405,  1.8321, -1.4887],
         [-2.1526, -1.2789, -0.0822,  ..., -0.6628,  0.5302, -0.8929],
         [-2.5775, -0.9997,  0.1696,  ..., -0.0485,  1.6915, -0.2604],
         [-2.6746, -0.4387, -0.1871,  ..., -0.3999,  0.4975, -0.8076]],

        [[-0.7667, -0.8171,  0.1871,  ..., -0.7841,  1.8354, -1.3493],
         [-0.7477, -1.3060, -0.6611,  ..., -0.6418,  1.6998, -1.3979],
         [-0.4771, -1.3947, -0.3409,  ..., -0.7112,  1.4921, -0.5246],
         [-2.0189, -1.4544, -0.1294,  ..., -0.8120,  1.7190, -1.4436]],

        ...,

        [[-2.3089, -0.8847, -0.2145,  ..., -0.9623,  1.9076, -1.5178],
         [-2.4614, -1.1641, -0.5035,  ..., -0.6268,  1.6172, -0.7186],
         [-2.2972, -1.2580, -0.3331,  ..., -0.9269,  0.7154, -1.8115],
         [-0.6305, -1.4393, -0.7696,  ..., -0.9767,  1.4945, -1.8764]],

        [[-2.4715, -0.9669, -0.3242,  ..., -0.9250,  1.7050, -1.7332],
         [-2.2656, -1.2044, -0.4224,  ..., -0.3886,  1.7569, -1.3519],
         [-2.6729, -1.2868,  0.3053,  ..., -0.9931,  1.2326, -1.7785],
         [-0.4830, -0.9768, -0.2643,  ..., -0.4581,  0.7312, -1.1027]],

        [[-2.4879, -1.2117, -0.3654,  ..., -0.9533,  1.7748, -1.3855],
         [-2.4837, -0.7250,  0.2238,  ..., -0.6475,  1.4601, -1.2819],
         [-2.3771, -1.1362, -0.3820,  ..., -0.7309,  1.8683, -1.3949],
         [-2.2112, -1.0387, -0.1078,  ..., -0.7194,  1.4059, -1.6868]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4718, -0.6164,  0.2593,  ..., -0.1604,  0.7078, -0.1141],
         [-0.9600, -0.3785,  0.1563,  ..., -0.4271,  0.9530, -0.3577],
         [-0.6300, -1.0849,  0.2514,  ..., -0.4506,  0.7448, -0.5237],
         [-1.5640, -0.3206,  0.6690,  ..., -0.1983,  0.8399, -0.5776]],

        [[-1.1945, -0.6083, -0.2907,  ..., -0.5249,  0.7802, -0.4714],
         [-1.5760, -0.7583,  0.4508,  ..., -0.1562,  0.4239, -0.0739],
         [-1.6693, -0.3648,  0.5197,  ..., -0.2638,  0.7599,  0.4389],
         [-1.5038, -0.7775,  0.5556,  ..., -0.4793,  0.3417, -0.0877]],

        [[-0.9603, -0.3757,  0.7946,  ..., -0.2913,  1.1295, -0.3603],
         [-0.7714, -1.0856,  0.2959,  ..., -0.1928,  1.0312, -0.5531],
         [-0.7023, -1.4723,  0.2073,  ..., -0.3106,  0.7660,  0.0233],
         [-1.2856, -0.9831,  0.4679,  ..., -0.5317,  0.9449, -0.6498]],

        ...,

        [[-1.3586, -0.6040,  0.4644,  ..., -0.4013,  0.7270, -0.2697],
         [-1.3179, -0.5757,  0.3435,  ..., -0.8872,  0.7815, -0.1068],
         [-0.9800, -0.5685,  0.1520,  ..., -0.1441,  0.3807, -0.3626],
         [-1.0901, -0.5518,  0.1314,  ..., -0.4386,  0.6797, -0.6851]],

        [[-1.6616, -0.6307,  0.4086,  ..., -0.3963,  1.0195, -0.6055],
         [-1.5810, -0.5733,  0.4302,  ..., -0.1857,  1.2251, -0.2089],
         [-1.8130, -0.1164,  0.8111,  ..., -0.5386,  0.6961, -0.5621],
         [-0.8440, -0.5092,  0.7011,  ...,  0.0571,  0.2059, -0.2632]],

        [[-1.5216, -0.5396,  0.3479,  ..., -0.5744,  0.6457, -0.4427],
         [-1.6472,  0.1209,  0.4529,  ..., -0.1960,  0.9634, -0.4047],
         [-0.7465, -0.5471, -0.0320,  ..., -0.3793,  0.9365, -0.5436],
         [-1.4982, -0.3834,  0.5491,  ..., -0.2454,  1.1088, -0.4699]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9318, -1.1486,  0.8097,  ..., -0.6385,  0.8081, -2.1164],
         [-0.5287, -1.2122,  0.5259,  ..., -0.3748,  0.8952, -2.1999],
         [-1.1644, -1.5458,  0.5199,  ..., -0.8968,  0.7418, -1.9877],
         [-0.8755, -1.4983,  0.6951,  ..., -0.4110,  0.7456, -2.4602]],

        [[-0.9085, -1.2197,  0.1876,  ..., -0.4403,  0.9506, -2.2463],
         [-1.5333, -2.4714,  0.4956,  ..., -0.3283,  0.5098, -1.8629],
         [-0.9335, -2.1150,  0.4409,  ..., -0.2794,  0.8011, -1.7596],
         [-1.4457, -1.9288,  0.5891,  ..., -0.4939,  0.6141, -1.9575]],

        [[-1.3388, -2.3728,  0.8656,  ..., -0.3800,  0.9814, -2.5940],
         [-0.8091, -2.3466,  0.1803,  ..., -0.3155,  0.0906, -2.4319],
         [-0.6957, -2.7053,  0.3186,  ..., -0.1600,  0.7464, -1.1561],
         [-1.3750, -2.2686,  0.5424,  ..., -0.5785,  0.8490, -1.5961]],

        ...,

        [[-1.1802, -2.0964,  0.4755,  ...,  0.1628,  0.7512, -2.0083],
         [-0.9161, -1.1906, -0.0345,  ..., -0.7089,  0.8248, -1.4860],
         [-0.8216, -2.1865,  0.7362,  ..., -0.1719,  0.6575, -1.9783],
         [-1.0812, -1.2837,  0.2859,  ..., -0.4494,  0.5729, -1.2868]],

        [[-1.5729, -1.9029,  0.3943,  ..., -0.4261,  0.8494, -2.4304],
         [-1.7432, -1.6275,  0.6004,  ..., -0.6124,  1.1689, -2.2596],
         [-1.1402, -1.9000,  0.8790,  ..., -0.4781,  0.9002, -1.3206],
         [-1.0645, -2.0695,  1.0391,  ..., -0.2945,  0.1988, -2.0865]],

        [[-1.2579, -2.1983,  0.7313,  ..., -0.3856,  0.3894, -2.4898],
         [-1.0826, -2.0247,  0.4515,  ..., -0.4419,  0.1932, -2.1699],
         [-0.9860, -1.8684,  0.4373,  ..., -0.4995,  1.0791, -2.1439],
         [-1.0505, -1.8218,  0.4430,  ..., -0.6208,  1.0158, -2.4140]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.3897, -2.3020,  0.4691,  ..., -0.9020,  0.7460, -0.3544],
         [-0.4559, -2.3947,  0.2668,  ..., -0.6971,  0.9589, -0.1961],
         [-1.0539, -2.3288,  0.4050,  ..., -1.0563,  0.7595, -0.3274],
         [-0.9199, -2.6078,  0.6541,  ..., -0.7647,  0.8219, -0.5024]],

        [[-0.9817, -2.4155,  0.4644,  ..., -0.8018,  0.5563, -0.3868],
         [-0.7041, -2.8067,  0.6302,  ..., -0.6275,  0.8184, -0.1130],
         [-0.8349, -2.7921,  0.3819,  ..., -0.3631,  0.9154, -0.2130],
         [-0.9526, -2.7702,  0.7102,  ..., -0.6117,  0.4439, -0.2245]],

        [[-0.9870, -2.6610,  0.6822,  ..., -0.7409,  0.8490, -0.3642],
         [-0.9808, -2.9607,  0.2588,  ..., -0.8083,  0.8428, -0.4597],
         [-0.3173, -2.9965,  0.3158,  ..., -0.3594,  0.6774,  0.1788],
         [-1.3550, -2.6447,  0.3324,  ..., -0.1980,  0.5972,  0.0652]],

        ...,

        [[-1.0922, -2.4313,  0.5483,  ..., -0.5742,  1.1602, -0.3263],
         [-0.8911, -2.0523,  0.1351,  ..., -0.9884,  0.9259, -0.0599],
         [-0.8828, -3.0592,  0.4600,  ..., -0.8246,  0.8402, -0.1164],
         [-1.2807, -2.6173,  0.4334,  ..., -0.9547,  0.6131,  0.0438]],

        [[-1.2723, -2.7128,  0.7482,  ..., -0.6318,  1.0269, -0.3490],
         [-1.1456, -2.4113,  0.5499,  ..., -1.0224,  0.6804, -0.2873],
         [-0.4607, -2.6685,  0.7109,  ..., -0.7253,  0.3613,  0.1390],
         [-1.3846, -2.8809,  0.5678,  ..., -0.9553,  0.5218, -0.1962]],

        [[-1.3330, -2.5824,  0.7004,  ..., -0.5499,  0.8835, -0.3570],
         [-0.7515, -3.0126,  0.2282,  ..., -0.7352,  0.5590,  0.0598],
         [-0.4033, -2.1637,  0.6345,  ..., -0.8985,  0.6513, -0.8714],
         [-0.9153, -2.7929,  0.3640,  ..., -0.2315,  0.9980, -0.5307]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8697e+00, -1.4624e+00,  9.4832e-02,  ..., -1.1265e+00,
          -1.4251e-01, -1.0430e+00],
         [-1.4120e+00, -1.2166e+00, -7.8280e-02,  ..., -1.3838e+00,
          -4.4292e-01, -8.9020e-01],
         [-2.0199e+00, -1.0940e+00, -8.3921e-02,  ..., -1.3974e+00,
          -4.7564e-01, -1.1476e+00],
         [-2.0320e+00, -1.5471e+00, -1.6254e-01,  ..., -1.4503e+00,
           3.9810e-01, -5.5156e-01]],

        [[-2.1629e+00, -1.6015e+00, -3.1103e-02,  ..., -1.6651e+00,
          -8.0405e-02, -1.0169e+00],
         [-2.1039e+00, -1.2617e+00, -3.8788e-05,  ..., -1.2008e+00,
          -2.4804e-01, -6.8660e-01],
         [-2.0945e+00, -1.2517e+00,  3.7547e-02,  ..., -1.0998e+00,
          -3.7277e-01, -8.1804e-01],
         [-1.8363e+00, -1.7042e+00, -1.4734e-01,  ..., -1.0990e+00,
          -1.1885e-01, -1.0014e+00]],

        [[-1.5701e+00, -1.8080e+00,  3.2163e-02,  ..., -1.2429e+00,
          -6.8397e-02, -2.7902e-01],
         [-2.2795e+00, -1.4511e+00, -5.4350e-01,  ..., -1.2939e+00,
          -3.4460e-01, -9.6989e-01],
         [-8.3146e-01, -1.8046e+00, -1.3164e-01,  ..., -4.1149e-01,
          -4.8591e-01, -9.8129e-01],
         [-2.3790e+00, -1.2004e+00, -1.1724e-02,  ..., -1.0791e+00,
          -1.7838e-01, -1.0280e+00]],

        ...,

        [[-1.5385e+00, -1.5022e+00, -1.2240e-01,  ..., -1.4009e+00,
          -2.2792e-01, -9.0100e-01],
         [-1.7655e+00, -1.1547e+00, -3.5124e-01,  ..., -1.3241e+00,
          -3.6058e-01, -8.7366e-01],
         [-1.9298e+00, -1.6656e+00, -2.6714e-03,  ..., -1.4274e+00,
          -2.8586e-01, -1.0390e+00],
         [-2.3886e+00, -1.0484e+00, -6.1586e-03,  ..., -1.5121e+00,
          -1.5946e-01, -9.1598e-01]],

        [[-1.9109e+00, -1.3910e+00,  1.1092e-01,  ..., -1.3333e+00,
           2.0083e-01, -8.3029e-01],
         [-1.1669e+00, -1.4034e+00, -3.3090e-01,  ..., -1.6570e+00,
           3.8709e-01, -1.0317e+00],
         [-1.6851e+00, -1.5012e+00, -3.9395e-02,  ..., -5.0417e-01,
          -5.1068e-01, -6.4260e-01],
         [-2.2063e+00, -1.4645e+00, -2.2347e-01,  ..., -1.5234e+00,
           8.2795e-02, -9.0696e-01]],

        [[-2.1484e+00, -1.3932e+00,  1.3899e-01,  ..., -1.2281e+00,
          -1.8649e-01, -1.1142e+00],
         [-1.9412e+00, -1.6754e+00, -4.5668e-02,  ..., -1.1218e+00,
          -3.4053e-01, -9.3982e-01],
         [-1.8491e+00, -1.4494e+00, -8.4125e-02,  ..., -1.1575e+00,
          -5.5375e-01, -1.5477e+00],
         [-2.3671e+00, -1.7582e+00, -3.2158e-01,  ..., -1.1644e+00,
          -3.8950e-02, -4.7747e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([552, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([660, 4, 256]), pos_embed.shape: torch.Size([660, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1958, -0.9413, -0.3149,  ..., -0.5408,  1.6541, -1.5386],
         [-1.9407, -0.4406,  0.3282,  ..., -1.0562,  1.6570, -1.3664],
         [-2.5759, -1.0781,  0.3170,  ..., -0.0864,  1.9517, -1.5504],
         [-2.2572, -1.1496, -0.3487,  ..., -1.0282,  1.5808, -1.4421]],

        [[-2.0782, -0.9531, -0.1756,  ..., -0.5856,  1.5262, -1.4359],
         [-2.4198, -0.9922, -0.0171,  ..., -0.5228,  1.6544, -1.6076],
         [-2.3894, -1.2324, -0.3609,  ..., -0.6936,  1.8017, -0.8865],
         [-2.5095, -0.4455, -0.1410,  ..., -0.6252,  1.6990, -1.7588]],

        [[-2.3922, -1.1576, -0.1796,  ..., -0.3953,  1.6445, -0.8414],
         [-2.5642,  0.0320, -0.2845,  ..., -0.3945,  1.4812, -1.4434],
         [-2.4699, -1.3813, -0.2402,  ..., -0.7415,  0.6341, -1.4914],
         [-0.8725, -1.1296,  0.1944,  ..., -0.6838,  1.7621, -0.9097]],

        ...,

        [[-1.5300, -1.1600, -0.1055,  ..., -0.8770,  1.4064, -0.8735],
         [-2.1305, -0.4423,  0.0119,  ..., -0.4629,  1.6078, -1.8398],
         [-2.3013, -1.2038, -0.2145,  ..., -0.5085,  1.2627, -1.4186],
         [-2.2303, -1.0460, -0.4768,  ..., -0.6453,  1.8364, -0.1487]],

        [[-2.4453, -1.1261, -0.0777,  ..., -0.5752,  1.4303, -1.1450],
         [-2.0832, -1.2270, -0.1835,  ..., -0.8197,  1.9110, -1.4520],
         [-0.6957, -1.0326, -0.1277,  ..., -0.1971,  1.9209, -1.6857],
         [-2.4764, -1.1192,  0.0804,  ..., -0.5766,  1.5035, -1.4333]],

        [[-2.0567, -1.2966,  0.1181,  ..., -0.9483,  1.3004, -1.2440],
         [-2.4365, -1.1180, -0.1081,  ..., -0.7837,  1.6028, -1.5362],
         [-2.3794, -1.0091, -0.1867,  ..., -0.5428,  1.5754, -1.7443],
         [-2.5231, -1.1873, -0.1952,  ..., -0.7257,  1.7590, -1.6172]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.2856e+00, -2.7985e-01,  3.1691e-01,  ..., -5.3992e-01,
           7.5007e-01, -3.4456e-01],
         [-1.4526e+00, -2.4540e-01,  7.3557e-01,  ..., -1.6076e-01,
           7.9712e-01, -5.5219e-01],
         [-1.5509e+00, -4.2829e-01,  6.0086e-01,  ...,  3.0233e-02,
           1.0121e+00, -1.1643e-01],
         [-1.0025e+00, -5.5250e-01,  1.2522e-01,  ..., -5.2680e-01,
           8.6471e-01, -1.2991e-01]],

        [[-1.1547e+00, -6.0681e-01,  8.1397e-02,  ..., -3.0846e-01,
           8.4422e-01, -5.0062e-01],
         [-1.3093e+00, -5.0654e-01,  7.2770e-01,  ..., -2.5699e-04,
           1.0584e+00, -3.2289e-01],
         [-1.3648e+00, -4.8377e-01,  6.5273e-01,  ..., -4.6989e-01,
           5.2660e-01,  2.6844e-02],
         [-9.6188e-01, -4.3984e-01,  3.9626e-01,  ..., -6.2454e-02,
           7.7940e-01, -3.1791e-01]],

        [[-1.6964e+00, -5.5250e-01,  1.1338e-01,  ..., -2.7614e-01,
           9.0104e-01, -1.4734e-01],
         [-1.9407e+00, -2.3769e-01,  2.0237e-01,  ..., -2.3768e-01,
           7.5931e-01, -2.0415e-01],
         [-1.5143e+00, -4.1439e-01,  8.4044e-01,  ..., -3.9202e-01,
           2.6540e-01, -4.7453e-01],
         [-9.5150e-01, -5.6618e-01,  4.3167e-01,  ..., -3.7827e-01,
           9.6322e-01, -3.1411e-01]],

        ...,

        [[-1.3203e+00, -5.1277e-01,  1.2045e-01,  ..., -5.4475e-01,
           7.2615e-01, -1.6153e-01],
         [-1.7273e+00, -5.4096e-01,  6.5894e-01,  ..., -3.8341e-01,
           9.9069e-01, -7.9111e-01],
         [-1.2162e+00, -5.7441e-01,  4.0907e-01,  ..., -4.8420e-01,
           8.2327e-01, -3.3524e-01],
         [-1.5669e+00, -8.0808e-01,  4.9198e-01,  ..., -4.9409e-02,
           7.3900e-01,  7.9366e-03]],

        [[-1.2291e+00, -9.4764e-01,  6.1008e-01,  ..., -5.8380e-01,
           1.0656e+00,  3.4956e-02],
         [-1.3984e+00, -5.0570e-01,  5.6016e-01,  ..., -3.3084e-01,
           8.6510e-01, -5.4679e-01],
         [-1.0733e+00, -2.5508e-01,  8.5208e-01,  ..., -2.7679e-01,
           8.2910e-01, -5.2570e-01],
         [-1.8826e+00, -5.4022e-01,  7.9127e-01,  ..., -3.6268e-01,
           8.0339e-01, -2.8977e-01]],

        [[-1.6215e+00, -8.9212e-01,  2.2748e-01,  ..., -7.2795e-01,
           6.3747e-01, -3.2473e-01],
         [-1.6712e+00, -1.3869e-01,  8.7204e-01,  ..., -3.4066e-01,
           9.4053e-01, -4.8044e-01],
         [-9.7573e-01, -3.0542e-01,  7.3834e-01,  ..., -3.0572e-01,
           7.6289e-01, -5.6374e-01],
         [-1.5493e+00, -7.0324e-01,  3.2730e-01,  ..., -5.0518e-01,
           6.7813e-01, -1.2105e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6659, -1.9714,  0.2538,  ..., -1.0187,  0.9390, -1.3986],
         [-0.5761, -1.7847,  0.4140,  ..., -0.4117,  0.5639, -1.5731],
         [-1.0313, -1.9018,  0.9865,  ..., -0.1580,  0.1925, -1.1421],
         [-1.0821, -1.9704,  0.0052,  ..., -0.7021,  0.6616, -1.9479]],

        [[-0.9353, -2.2410,  0.4604,  ..., -0.9379,  0.8869, -1.7020],
         [-1.0513, -1.9045,  0.4182,  ..., -0.3605,  0.8639, -2.2264],
         [-0.9938, -2.1431,  0.3778,  ..., -0.3558,  1.0605, -2.0477],
         [-0.9011, -2.1778,  0.6512,  ..., -0.2203,  0.8157, -2.0730]],

        [[-1.2606, -2.0579,  0.3874,  ..., -0.5206,  0.8447, -2.1744],
         [-1.3849, -2.1639,  0.5769,  ..., -0.2653,  0.6124, -2.2256],
         [-1.2965, -2.1788,  0.1269,  ..., -0.5480,  0.6832, -2.2473],
         [-1.0827, -1.2925,  0.9021,  ..., -0.2311,  0.6418, -2.0871]],

        ...,

        [[-1.4636, -2.0587,  0.4113,  ..., -0.6641,  0.8136, -1.6569],
         [-1.3644, -2.2747,  0.4938,  ..., -0.5322,  0.9024, -1.8595],
         [-1.3357, -2.3408,  0.4372,  ..., -0.4826,  1.0368, -1.5524],
         [-1.3343, -2.3145, -0.0534,  ..., -0.6960,  1.0158, -0.9406]],

        [[-1.1197, -1.4418,  0.3939,  ..., -0.1964,  0.8906, -1.0957],
         [-1.0196, -1.8816,  0.5171,  ..., -0.6959,  0.5877, -2.1712],
         [-1.2947, -2.1201, -0.2881,  ..., -0.3029,  0.8739, -2.1746],
         [-1.3536, -1.1681,  0.5981,  ..., -0.3148,  0.9404, -2.2187]],

        [[-1.3574, -1.3828,  0.3319,  ..., -0.4981,  0.9284, -2.2707],
         [-1.5394, -1.8848,  0.4397,  ..., -0.3852,  1.0230, -1.9411],
         [-1.2081, -1.6502,  0.7953,  ..., -0.0499,  1.0041, -2.6238],
         [-1.1076, -2.1556,  0.6916,  ..., -0.7441,  1.0461, -1.0730]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.3938, -1.1415,  0.2167,  ..., -1.1758,  0.8536, -0.3181],
         [-0.8486, -2.6866,  0.3704,  ..., -0.8793,  0.6970,  0.2850],
         [-1.0965, -2.5816,  0.4804,  ..., -0.5432,  0.8379, -0.0143],
         [-1.1466, -2.7377,  0.3816,  ..., -0.9768,  0.9359, -0.3078]],

        [[-0.9462, -2.7126,  0.3345,  ..., -0.9853,  1.1341, -0.4747],
         [-0.9936, -2.6933,  0.4061,  ..., -0.2614,  1.3287, -0.2036],
         [-0.7906, -2.6713,  0.4464,  ..., -0.8116,  1.0664, -0.3294],
         [-1.2529, -3.0095,  0.5597,  ..., -0.6782,  1.0671, -0.5906]],

        [[-1.3309, -2.6756,  0.5193,  ..., -0.7906,  1.2062, -0.7490],
         [-1.2741, -1.7232,  0.7084,  ..., -0.9058,  0.8782, -0.5655],
         [-1.0989, -2.6853,  0.3143,  ..., -0.8325,  0.9217, -0.2095],
         [-1.0761, -2.0891,  0.6523,  ..., -0.9803,  1.2087, -0.2706]],

        ...,

        [[-1.2928, -2.6997,  0.4734,  ..., -1.0709,  1.0335,  0.0182],
         [-1.0918, -2.6989,  0.3058,  ..., -0.8308,  0.8706, -0.1900],
         [-0.9927, -2.8293,  0.4525,  ..., -0.4073,  0.8456, -0.0651],
         [-1.1436, -2.9283,  0.1185,  ..., -0.9947,  1.1284,  0.0677]],

        [[-0.9455, -2.5981,  0.5941,  ..., -0.4870,  1.0191,  0.2676],
         [-1.3535, -2.7630,  0.6668,  ..., -1.1125,  0.7667, -0.3509],
         [-1.1687, -2.0777,  0.3982,  ..., -0.7030,  1.0659, -0.5367],
         [-1.2403, -2.3098,  0.5466,  ..., -0.8420,  1.2674, -0.1931]],

        [[-1.4230, -1.4789,  0.3018,  ..., -0.5692,  1.1546, -0.6972],
         [-1.2354, -2.4279,  0.7186,  ..., -0.2715,  0.6401, -0.3054],
         [-1.2044, -2.4810,  0.3164,  ..., -0.6673,  1.0506, -0.3457],
         [-1.2511, -2.5854,  0.3014,  ..., -0.6102,  0.8154,  0.3770]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.2243e+00, -6.0485e-01,  1.4142e-01,  ..., -1.6878e+00,
           3.5681e-03, -8.5682e-01],
         [-1.9879e+00, -1.5557e+00, -1.5140e-01,  ..., -1.3201e+00,
          -2.5941e-01, -6.3504e-01],
         [-2.0708e+00, -1.3024e+00, -7.9175e-02,  ..., -1.2419e+00,
          -8.6748e-02, -6.1104e-01],
         [-1.9671e+00, -1.5250e+00, -1.5504e-01,  ..., -1.4225e+00,
          -1.5314e-01, -9.7394e-01]],

        [[-1.2713e+00, -1.1585e+00, -4.7400e-02,  ..., -1.6050e+00,
          -2.4626e-01, -9.8091e-01],
         [-1.8805e+00, -1.4333e+00,  3.4306e-01,  ..., -1.1557e+00,
           2.0868e-01, -1.0673e+00],
         [-1.6428e+00, -1.2308e+00,  3.1950e-02,  ..., -1.5469e+00,
          -5.1751e-01, -5.6389e-01],
         [-1.9141e+00, -1.5213e+00, -3.5143e-02,  ..., -1.5144e+00,
           1.1700e-01, -1.1046e+00]],

        [[-1.6177e+00, -1.5769e+00,  1.7905e-02,  ..., -1.5441e+00,
           6.5179e-02, -1.1710e+00],
         [-2.0459e+00, -6.9076e-01, -8.8124e-02,  ..., -1.7137e+00,
          -1.1031e-01, -9.8077e-01],
         [-8.0209e-01, -1.6399e+00,  2.9430e-01,  ..., -1.3025e+00,
          -2.5486e-01, -1.0937e+00],
         [-1.5013e+00, -1.1051e+00, -6.1594e-02,  ..., -1.4825e+00,
          -8.5520e-03, -1.2155e+00]],

        ...,

        [[-2.0918e+00, -1.2001e+00, -2.9444e-01,  ..., -1.2960e+00,
          -1.4110e-01, -8.6386e-01],
         [-1.9002e+00, -1.6263e+00,  4.6509e-02,  ..., -1.3038e+00,
          -2.1267e-03, -8.3893e-01],
         [-2.0923e+00, -1.6032e+00, -1.0310e-01,  ..., -1.1618e+00,
          -2.2517e-02, -5.0114e-01],
         [-1.4053e+00, -1.5264e+00, -1.4575e-01,  ..., -1.4014e+00,
           4.3037e-01, -8.8488e-01]],

        [[-1.1462e+00, -1.4877e+00,  5.8082e-02,  ..., -1.0785e+00,
          -2.2543e-01, -1.1539e-01],
         [-2.3375e+00, -1.3675e+00,  5.8872e-02,  ..., -1.2995e+00,
          -1.1079e-02, -8.4512e-01],
         [-1.9848e+00, -1.1776e+00, -2.0927e-01,  ..., -1.0644e+00,
          -2.1310e-01, -8.9736e-01],
         [-2.1213e+00, -9.5541e-01, -3.9318e-01,  ..., -1.3928e+00,
          -3.6324e-03, -8.0611e-01]],

        [[-2.0064e+00, -8.8776e-01,  3.5752e-03,  ..., -1.0384e+00,
          -2.7743e-01, -7.2326e-01],
         [-1.8643e+00, -1.2804e+00,  1.7385e-02,  ..., -4.5916e-01,
          -3.4531e-01, -1.0581e+00],
         [-1.9406e+00, -1.3636e+00,  6.8020e-02,  ..., -1.3271e+00,
          -1.5761e-01, -8.9982e-01],
         [-2.1316e+00, -1.2587e+00, -6.9787e-02,  ..., -9.1982e-01,
          -4.5745e-01, -9.5959e-02]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([896, 4, 256]), pos_embed.shape: torch.Size([896, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
encoder start :
	 src.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([896, 4, 256]), k.shape: torch.Size([896, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([896, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([896, 4, 256])
	 (after FFN) src2.shape: torch.Size([896, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([896, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.8599, -0.7799, -0.3796,  ..., -0.7766,  1.5859, -1.4995],
         [-2.1835, -1.4878, -0.2975,  ..., -0.4918,  1.2219, -1.0114],
         [-2.4595, -1.2062,  0.0197,  ..., -0.6398,  1.4969, -0.9524],
         [-2.4855, -1.0733, -0.1152,  ..., -0.6540,  1.5905, -1.5601]],

        [[-2.1427, -1.2359, -0.0988,  ..., -1.0752,  1.5957, -0.8030],
         [-2.6632, -0.9133, -0.2345,  ..., -0.6384,  1.4399, -1.6310],
         [-1.9358, -1.2776, -0.1438,  ..., -0.6554,  1.8088, -0.9946],
         [-2.3576, -0.3752,  0.1339,  ..., -0.6221,  1.7782, -1.3073]],

        [[-2.2451, -1.1178, -0.3945,  ..., -0.7898,  1.3229, -1.5980],
         [-2.6748, -1.2832, -0.0075,  ..., -0.5517,  1.4811, -0.8324],
         [-2.4010, -1.0784, -0.1268,  ..., -0.9526,  0.5775, -1.4192],
         [-2.6104, -1.1028,  0.3141,  ..., -0.7723,  1.6040, -1.6347]],

        ...,

        [[-1.9664, -0.9719, -0.1213,  ..., -0.5970,  0.4970, -1.5592],
         [-2.1496, -1.1593, -0.4079,  ..., -0.3558,  1.9413, -1.6473],
         [-2.4213, -1.2922, -0.1753,  ..., -1.0489,  1.3344, -1.7185],
         [-2.4975, -1.0691, -0.5040,  ..., -0.9217,  1.6472, -1.6416]],

        [[-2.4268, -1.4444, -0.1267,  ..., -0.3314,  1.2597, -1.6072],
         [-2.1148, -1.3466,  0.2082,  ..., -0.7537,  0.6636, -1.4140],
         [-2.5340, -0.5874, -0.4026,  ..., -0.6827,  1.4070, -0.9453],
         [-2.3287, -0.3949,  0.1797,  ..., -0.7503,  1.4863, -0.9779]],

        [[-2.5123, -1.1478,  0.0498,  ..., -0.9065,  1.9438, -0.9918],
         [-2.3447, -1.2930,  0.1062,  ..., -0.8044,  0.2850, -0.8978],
         [-2.2356, -1.0493,  0.0192,  ..., -0.7385,  0.6866, -0.9506],
         [-2.1143, -0.0111,  0.0260,  ..., -0.5092,  1.6576, -2.0785]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9166, -0.6118,  0.1264,  ..., -0.7466,  0.9316, -0.0993],
         [-1.6082, -0.9415,  0.0780,  ..., -0.5276,  0.7214, -0.1387],
         [-1.0209, -0.5297,  0.5830,  ..., -0.8817,  0.7487, -0.0533],
         [-1.8640, -0.3242,  0.1915,  ..., -0.2390,  0.7696, -0.8230]],

        [[-1.4122, -0.6146,  0.5083,  ..., -0.5598,  0.5994, -0.1335],
         [-1.2190, -0.7180,  0.4873,  ..., -0.4867,  0.8342, -0.3993],
         [-1.5166, -0.7741,  0.6847,  ..., -0.0199,  0.7261, -0.2105],
         [-1.3813, -0.2095,  0.5759,  ..., -0.0252,  0.8914, -0.2225]],

        [[-1.6725, -0.7991,  0.3677,  ..., -0.1504,  0.6563, -0.5129],
         [-1.4844, -0.6978,  0.3511,  ..., -0.2639,  0.5420, -0.0430],
         [-1.4878, -0.7757,  0.6446,  ..., -0.0937,  0.3063, -0.2680],
         [-1.6002, -0.5471,  0.6611,  ..., -0.5620,  0.7190, -0.2912]],

        ...,

        [[-1.0830, -0.6112,  0.5723,  ..., -0.2877,  0.4479, -0.4528],
         [-1.4401, -0.4314,  0.5696,  ..., -0.4187,  0.8802, -0.4819],
         [-1.8265, -0.8655,  0.0886,  ..., -0.6690,  0.6553, -0.2064],
         [-1.2827, -0.5956,  0.2974,  ..., -0.3503,  0.6191, -0.3639]],

        [[-1.3888, -0.8765,  0.1748,  ..., -0.1918,  0.5692, -0.4112],
         [-1.7376, -0.5459,  0.7639,  ..., -0.5824,  0.2916, -0.4717],
         [-1.7889, -0.2260,  0.0095,  ..., -0.3162,  0.6306, -0.2328],
         [-1.6226, -0.8893,  0.8290,  ..., -0.3318,  0.5105, -0.3324]],

        [[-1.0777, -0.2767,  0.5765,  ..., -0.5721,  1.0012, -0.2710],
         [-1.0702, -0.0965,  0.6469,  ..., -0.1558,  0.1133, -0.0276],
         [-1.4106, -0.4764,  0.3325,  ..., -0.7066,  0.1737, -0.4128],
         [-1.7972, -0.4415,  0.3794,  ..., -0.3303,  1.0394, -0.4594]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4080, -2.3034,  0.3277,  ..., -0.0110,  1.0670, -1.5576],
         [-1.4909, -1.8928,  0.2911,  ..., -0.6964,  0.2773, -1.8435],
         [-0.7603, -2.1470,  0.4599,  ..., -0.5828,  0.9364, -2.1731],
         [-1.2677, -2.1116,  0.2512,  ..., -0.3020,  0.8596, -2.0066]],

        [[-1.5253, -1.7339,  0.5857,  ..., -0.3596,  0.9427, -2.3267],
         [-1.5339, -2.0356,  0.6442,  ..., -0.3451,  1.1315, -1.2590],
         [-1.2776, -2.3061,  0.5248,  ..., -0.1050,  0.9858, -2.0138],
         [-0.9122, -2.2985, -0.0988,  ..., -0.6286,  0.8613, -2.1885]],

        [[-0.8816, -2.2107,  0.3958,  ..., -0.5473,  1.0197, -1.9786],
         [-1.1444, -1.3161,  0.5883,  ..., -0.9321,  0.6497, -2.1410],
         [-1.8078, -2.4392,  0.4981,  ..., -0.1766,  0.6650, -2.0690],
         [-1.3977, -1.8825,  0.4392,  ..., -0.5200,  0.8403, -1.3442]],

        ...,

        [[-1.2954, -2.0265,  0.4003,  ..., -0.4636, -0.1051, -1.5407],
         [-1.2987, -1.8414,  0.4031,  ..., -0.6409,  1.0449, -2.2133],
         [-1.1650, -2.3960,  0.7441,  ..., -1.2423,  1.0568, -2.0979],
         [-1.2081, -1.9351, -0.1577,  ..., -0.5853,  0.8769, -2.1062]],

        [[-1.3067, -2.5101,  0.8018,  ..., -0.3643,  0.9601, -2.1118],
         [-1.2081, -1.8304,  0.5739,  ..., -0.4592,  0.4842, -2.3169],
         [-1.2967, -1.1381,  0.0291,  ..., -0.5997,  0.8206, -1.1647],
         [-1.7082, -2.0679,  0.6602,  ..., -0.7366,  0.7846, -2.1438]],

        [[-0.8423, -1.6582,  0.7526,  ..., -0.7056,  0.7734, -2.0987],
         [-1.2261, -1.7588,  0.5660,  ..., -0.9992,  0.6030, -2.2149],
         [-1.0776, -1.9828,  0.2130,  ..., -0.6954,  0.6087, -2.0425],
         [-1.5195, -1.1719,  0.0464,  ..., -0.6069,  1.1818, -2.1440]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.4731, -2.9852,  0.5411,  ..., -0.0801,  1.4618,  0.0752],
         [-1.4379, -2.3965,  0.3074,  ..., -1.0891,  0.7838, -0.4774],
         [-1.0761, -1.7650,  0.4264,  ..., -0.9215,  1.2615, -0.2549],
         [-1.3965, -2.7550,  0.7497,  ..., -0.8896,  0.9715, -0.1718]],

        [[-1.1362, -2.3266,  0.3264,  ..., -0.2494,  1.2706, -0.6675],
         [-0.5392, -2.5645,  0.5746,  ..., -0.6425,  1.2604, -0.0419],
         [-1.2079, -2.6774,  0.5872,  ..., -0.6757,  0.9374, -0.5916],
         [-1.2500, -2.8501,  0.4568,  ..., -0.9424,  1.4125, -0.4548]],

        [[-0.9323, -2.6915,  0.3166,  ..., -0.8747,  1.5074, -0.3375],
         [-1.4106, -2.2435,  0.5643,  ..., -1.0227,  1.0233,  0.0114],
         [-1.4941, -2.4184,  0.5409,  ..., -0.9387,  1.0465, -0.8105],
         [-1.3450, -2.6866,  0.4875,  ..., -1.2446,  1.2532, -0.2135]],

        ...,

        [[-1.0754, -2.9014,  0.3800,  ..., -0.2764,  0.0776, -0.1368],
         [-0.9610, -2.4968,  0.1658,  ..., -1.0908,  1.0729, -0.1953],
         [-1.1606, -2.6521,  0.6477,  ..., -1.0864,  0.7231, -0.2607],
         [-1.1274, -2.5099,  0.0400,  ..., -0.9238,  1.2362, -0.2091]],

        [[-0.4160, -2.5007,  0.5468,  ..., -0.5028,  1.0806, -0.0065],
         [-1.3632, -2.6553,  0.4113,  ..., -0.6286,  0.3579, -0.7032],
         [-1.0919, -1.2576,  0.5992,  ..., -0.7961,  1.3369, -0.1957],
         [-1.1105, -2.7073,  0.7076,  ..., -0.9012,  1.1094, -0.9536]],

        [[-1.1845, -1.5028,  0.5540,  ..., -0.7512,  0.6178, -0.3479],
         [-0.9241, -2.8416,  0.5381,  ..., -0.9204,  1.1154, -0.2075],
         [-1.3319, -2.7158,  0.0090,  ..., -0.8204,  1.2077, -0.5132],
         [-1.3488, -2.2270,  0.3114,  ..., -0.3510,  1.1774, -0.5380]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.6506, -1.3246,  0.2611,  ..., -1.3523, -0.0116, -0.9442],
         [-1.8186, -1.2853, -0.3025,  ..., -1.3133, -0.3521, -1.1860],
         [-1.7791, -0.9692, -0.3142,  ..., -1.4187,  0.3788, -0.9019],
         [-1.3606, -1.3084,  0.4979,  ..., -1.5600, -0.3195, -0.8122]],

        [[-1.6562, -1.7935,  0.0292,  ..., -1.0148,  1.0095, -1.5960],
         [-1.8374, -1.1361, -0.0640,  ..., -1.5428, -0.1268, -0.3483],
         [-2.0394, -1.6676,  0.3011,  ..., -1.3068,  0.0523, -1.1163],
         [-2.3680, -1.2307, -0.1933,  ..., -1.2975,  0.1107, -0.4390]],

        [[-2.0303, -1.3090, -0.2103,  ..., -1.2390,  0.0701, -0.9635],
         [-1.9516, -1.2756, -0.1639,  ..., -1.6790,  0.1006, -1.0412],
         [-2.1769, -1.4745, -0.1820,  ..., -1.3675,  0.0848, -1.0797],
         [-2.0219, -1.1789,  0.1046,  ..., -1.6020,  0.3267, -1.0662]],

        ...,

        [[-2.0761, -1.5640,  0.1546,  ..., -1.1568, -0.4131, -0.3347],
         [-1.7145, -1.1041,  0.0357,  ..., -1.6521, -0.0835, -0.3922],
         [-1.7322, -1.1773, -0.1429,  ..., -1.2139, -0.5141, -0.9830],
         [-1.9538, -1.2131,  0.2245,  ..., -1.6161, -0.0202, -0.9172]],

        [[-1.5696, -1.3642,  0.1319,  ..., -1.4857,  0.1752, -0.8792],
         [-1.8201, -1.2723, -0.1496,  ..., -1.5274, -0.2914, -1.2895],
         [-1.9282, -0.6834,  0.2943,  ..., -1.2813, -0.2340, -1.1231],
         [-1.6140, -1.2731,  0.0971,  ..., -1.5765,  0.1108, -1.2160]],

        [[-1.7331, -0.8842,  0.1085,  ..., -1.1420, -0.2665, -1.0380],
         [-1.6297, -1.5775,  0.1873,  ..., -1.1702, -0.4105, -0.8150],
         [-2.0550, -1.1768,  0.0153,  ..., -1.4471, -0.2063, -0.5163],
         [-1.3238, -1.1213, -0.1895,  ..., -1.1966, -0.2013, -1.0076]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([896, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([950, 4, 256]), pos_embed.shape: torch.Size([950, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
encoder start :
	 src.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([950, 4, 256]), k.shape: torch.Size([950, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([950, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([950, 4, 256])
	 (after FFN) src2.shape: torch.Size([950, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([950, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.4745e+00, -1.0724e+00, -2.2936e-01,  ..., -6.8547e-01,
           6.3893e-01, -6.6155e-01],
         [-2.4422e+00, -7.9324e-01, -7.1097e-01,  ..., -6.8840e-01,
           1.5008e+00, -1.0173e+00],
         [-2.3216e+00, -1.1530e+00,  3.7933e-01,  ..., -8.6042e-01,
           1.9272e+00, -1.3854e+00],
         [-2.4701e+00, -1.2201e+00,  2.5200e-01,  ..., -7.8157e-01,
           1.2879e+00, -1.4590e+00]],

        [[-2.4657e+00, -1.2806e+00,  2.2980e-03,  ..., -9.7218e-01,
           1.5014e+00, -9.3357e-01],
         [-1.6733e+00, -1.3087e+00, -1.0653e-01,  ..., -7.1484e-01,
           1.4692e+00, -1.8075e+00],
         [-8.6362e-01, -1.0424e+00,  3.4737e-01,  ..., -8.8803e-01,
           1.4122e+00, -1.4053e+00],
         [-2.3814e+00, -1.2356e+00, -5.4782e-01,  ..., -9.3227e-01,
           1.2149e+00, -1.6317e+00]],

        [[-1.5452e+00, -9.9524e-01,  8.0358e-02,  ..., -9.4549e-01,
           1.2794e+00, -1.4839e+00],
         [-2.4783e+00, -1.1150e+00,  4.7165e-03,  ..., -6.4962e-01,
           1.5046e+00, -1.4870e+00],
         [-2.0817e+00, -1.4505e+00,  1.1339e-01,  ..., -6.5370e-01,
           1.5050e+00, -9.2736e-01],
         [-2.2431e+00, -1.3063e+00,  2.4696e-01,  ..., -8.7974e-01,
           1.4793e+00, -1.6272e+00]],

        ...,

        [[-2.0057e+00, -1.2052e+00,  3.2572e-01,  ..., -1.0460e+00,
           1.5299e+00, -8.8956e-01],
         [-1.5362e+00, -1.0253e+00, -1.1382e-01,  ..., -7.1386e-01,
           1.4933e+00, -6.7195e-01],
         [-2.3580e+00, -1.1449e+00, -2.9925e-01,  ..., -8.0372e-01,
           1.4061e+00, -1.6319e+00],
         [-2.1244e+00, -9.8831e-01, -2.9336e-01,  ..., -9.4736e-01,
           1.3182e+00, -1.1928e+00]],

        [[-2.3993e+00, -1.0086e+00, -1.9678e-01,  ..., -7.7036e-01,
           1.6649e+00, -1.3590e+00],
         [-2.3591e+00, -1.3345e+00,  3.7450e-01,  ..., -7.0061e-01,
           1.3276e+00, -1.5107e+00],
         [-2.4804e+00, -3.9411e-01, -1.1151e-01,  ..., -4.1880e-01,
           1.5958e+00, -1.2004e+00],
         [-2.1994e+00, -1.0335e+00,  4.6699e-01,  ..., -6.3293e-01,
           1.6192e+00, -1.2275e+00]],

        [[-6.4488e-01, -1.0489e+00, -2.1742e-01,  ..., -9.2505e-01,
           1.5924e+00, -1.7406e+00],
         [-2.2949e+00, -4.2122e-01,  4.0940e-01,  ..., -6.3936e-01,
           1.2445e+00, -1.8686e+00],
         [-2.4721e+00, -1.1693e+00, -1.2355e-01,  ..., -8.5353e-01,
           1.2331e+00, -1.5606e+00],
         [-2.3046e+00, -1.2065e+00,  3.9900e-01,  ..., -5.3309e-01,
           1.5969e+00, -8.9693e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7347e+00, -3.7592e-01,  2.3424e-01,  ..., -6.7483e-01,
           2.2591e-01,  1.6344e-01],
         [-1.6808e+00, -9.2872e-01,  2.4075e-01,  ..., -3.7038e-01,
           5.3379e-01, -2.4248e-01],
         [-1.7265e+00, -4.0355e-01,  6.0915e-01,  ...,  9.8585e-02,
           9.8716e-01, -6.4165e-01],
         [-1.4001e+00, -3.2191e-01,  7.5355e-01,  ..., -4.2605e-01,
           4.9665e-01, -5.6202e-01]],

        [[-1.7577e+00, -3.7581e-01,  3.9947e-01,  ..., -8.1468e-01,
           1.0712e+00, -5.8213e-02],
         [-1.4479e+00, -8.2950e-01,  4.8241e-01,  ..., -9.7978e-02,
           5.3447e-01, -5.6145e-01],
         [-1.7042e-01, -4.4840e-01,  4.8544e-01,  ..., -1.7670e-01,
           5.7447e-01, -2.0582e-01],
         [-1.6095e+00, -7.2352e-01,  5.8359e-01,  ..., -5.2130e-01,
           6.5627e-01, -7.5394e-01]],

        [[-1.1242e+00, -7.6916e-01,  6.4792e-01,  ..., -5.7671e-01,
           7.2892e-01, -2.9703e-01],
         [-1.9156e+00, -3.2489e-01,  5.1063e-01,  ..., -3.9904e-01,
           7.6259e-01, -3.2176e-01],
         [-1.4986e+00, -1.1086e+00,  4.1764e-01,  ..., -1.1275e-03,
           6.8202e-01, -5.8509e-02],
         [-1.6880e+00, -8.4409e-01,  7.5203e-01,  ..., -4.7579e-01,
           5.6982e-01, -4.0399e-01]],

        ...,

        [[-1.8051e+00, -7.7881e-01,  5.6429e-01,  ..., -5.2082e-01,
           5.2736e-01, -2.0287e-01],
         [-1.4879e+00, -7.1623e-01,  4.1198e-01,  ..., -2.7631e-01,
           3.2418e-01, -9.7291e-02],
         [-1.5610e+00, -4.1077e-01,  4.6874e-01,  ..., -4.6783e-01,
           3.8805e-01, -6.5789e-01],
         [-1.3236e+00, -4.0831e-01,  4.5532e-01,  ..., -4.8261e-01,
           4.4307e-01, -3.3282e-01]],

        [[-1.6091e+00, -6.0348e-01,  4.8526e-01,  ..., -3.6468e-01,
           6.9378e-01, -1.3931e-01],
         [-1.6383e+00, -8.7993e-01,  6.8274e-01,  ..., -2.4600e-01,
           6.9076e-01, -3.9603e-01],
         [-1.3580e+00, -5.1015e-01,  5.7951e-01,  ..., -5.2901e-01,
           6.1738e-01, -3.0948e-01],
         [-1.5114e+00, -8.1535e-01,  7.3421e-01,  ..., -1.9810e-01,
           8.0511e-01, -3.0138e-01]],

        [[-1.0997e+00, -4.7956e-01,  4.3506e-01,  ..., -1.9848e-01,
           8.6782e-01, -5.1217e-01],
         [-1.5035e+00, -2.5800e-01,  8.8035e-01,  ..., -3.0111e-01,
           5.6229e-01, -5.2717e-01],
         [-1.7441e+00, -4.2507e-01,  6.6101e-01,  ..., -7.5855e-01,
           4.8258e-01, -8.2615e-01],
         [-1.6298e+00, -5.3248e-01,  6.6605e-01,  ..., -5.1133e-01,
           6.2875e-01,  9.6851e-02]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5838, -1.8427,  0.3036,  ..., -0.6070,  0.9150, -1.6250],
         [-1.0311, -2.1086,  0.3632,  ..., -0.3364,  1.1089, -2.1257],
         [-1.7712, -1.3160,  0.8393,  ..., -0.1970,  0.8812, -2.3366],
         [-1.3507, -2.0605,  0.2596,  ..., -0.5992,  0.8382, -1.8016]],

        [[-1.5139, -2.2975,  0.4402,  ..., -0.5799,  1.0666, -2.1030],
         [-1.4735, -2.1662,  0.1409,  ..., -0.1868,  0.4196, -1.3812],
         [-0.7880, -1.8827,  0.6419,  ..., -0.6877,  0.6046, -2.1289],
         [-1.6354, -2.0331,  0.1506,  ..., -0.4186,  0.9645, -2.4467]],

        [[-1.2550, -2.0722,  0.4746,  ..., -0.6484,  0.7944, -2.1309],
         [-1.9108, -1.2008, -0.0170,  ...,  0.2549,  0.8221, -2.1628],
         [-1.0608, -2.1387,  0.5910,  ..., -0.4083,  0.8382, -1.9384],
         [-1.4088, -2.1629,  0.3252,  ..., -0.6620,  0.8378, -2.3624]],

        ...,

        [[-1.3728, -2.0645,  0.2590,  ...,  0.0252,  0.7607, -1.5357],
         [-1.1313, -2.3571, -0.0030,  ...,  0.0850,  0.3886, -1.9142],
         [-1.4013, -2.3100,  0.4040,  ..., -0.3036, -0.2598, -2.0690],
         [-1.3450, -1.8821,  0.0671,  ..., -0.6565,  0.0873, -2.3923]],

        [[-1.6078, -2.1565,  0.2685,  ..., -0.4110,  0.9447, -1.5304],
         [-1.3028, -2.2321,  0.2355,  ..., -0.4965,  0.9595, -1.6252],
         [-1.1645, -1.8010,  0.3394,  ..., -0.4005,  0.9504, -1.6107],
         [-1.3403, -1.2769,  0.3634,  ..., -0.7654,  0.9168, -2.1822]],

        [[-1.3448, -1.0006,  0.8522,  ..., -0.3444,  0.7319, -2.3412],
         [-1.6766, -1.9385,  0.9868,  ..., -0.3763,  1.0659, -2.2115],
         [-1.0521, -1.2282,  0.4406,  ..., -0.7444,  0.9539, -0.9451],
         [-1.4693, -2.0137,  0.9330,  ...,  0.0288,  0.5728, -2.1084]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.0964, -2.5887,  0.2415,  ..., -0.5736,  1.5065, -0.2727],
         [-1.3488, -2.8353,  0.3769,  ..., -0.7599,  1.2488, -0.2915],
         [-1.7596, -2.1940,  0.4491,  ..., -0.5991,  0.9614, -0.3105],
         [-1.3420, -2.2486,  0.4193,  ..., -0.8097,  1.1418, -0.4118]],

        [[-1.4483, -2.6013,  0.3704,  ..., -0.8840,  1.0437, -0.4358],
         [-0.9292, -2.3741,  0.3729,  ..., -0.7785,  1.2602, -0.4471],
         [-1.2670, -2.2830,  0.3811,  ..., -0.9376,  1.1125, -0.1891],
         [-1.7468, -2.6887,  0.2277,  ..., -0.8446,  0.4788, -0.5023]],

        [[-1.6569, -1.2022,  0.2688,  ..., -0.9433,  1.0433, -0.4158],
         [-1.6734, -2.2890,  0.3648,  ..., -0.0168,  1.4346, -0.3332],
         [-1.2715, -2.6656,  0.3341,  ..., -0.9160,  1.0119, -0.6221],
         [-1.4033, -2.7665,  0.2902,  ..., -0.4094,  1.0038, -0.5039]],

        ...,

        [[-1.0894, -2.8061,  0.2842,  ..., -0.6534,  0.6316, -0.1137],
         [-1.1182, -2.8393,  0.1150,  ..., -0.5417,  1.1205, -0.5214],
         [-1.3395, -2.7529,  0.6144,  ..., -0.7266,  0.2298, -0.2673],
         [-0.7244, -2.1936,  0.1093,  ..., -0.9501,  0.8429, -0.3684]],

        [[-1.3632, -2.6164,  0.4150,  ..., -0.5808,  1.4415, -0.1763],
         [-1.6548, -2.8705,  0.1061,  ..., -0.7901,  1.4796, -0.2100],
         [-1.1469, -2.6928,  0.3555,  ..., -0.7851,  1.4798, -0.5098],
         [-1.2722, -2.3816,  0.3605,  ..., -1.0777,  1.1759, -0.5407]],

        [[-0.7258, -2.5504,  0.8413,  ..., -0.9176,  1.1978, -0.3336],
         [-1.3090, -2.5754,  0.6754,  ..., -1.0368,  1.4800, -0.4157],
         [-1.2837, -2.5634,  0.1669,  ..., -0.7183,  0.9343, -0.4726],
         [-0.7511, -2.2124,  0.5326,  ..., -0.7732,  1.1986, -0.6071]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9219e+00, -1.0236e+00, -1.1267e-01,  ..., -1.3246e+00,
           7.0583e-02, -1.2818e+00],
         [-2.2394e+00, -1.4271e+00, -1.5158e-01,  ..., -1.5989e+00,
           1.7118e-01, -5.7003e-01],
         [-1.6084e+00, -9.5056e-01,  5.2444e-03,  ..., -1.5110e+00,
           1.6019e-01, -1.2455e+00],
         [-1.6889e+00, -1.3183e+00, -1.3409e-01,  ..., -1.4098e+00,
           1.2332e-01, -1.0727e+00]],

        [[-2.1686e+00, -1.4544e+00, -1.1521e-04,  ..., -1.5756e+00,
          -1.7560e-01, -1.2591e+00],
         [-1.8834e+00, -1.1663e+00, -1.9054e-01,  ..., -1.4320e+00,
           8.4992e-02, -4.7100e-01],
         [-1.9901e+00, -1.2185e+00, -2.3436e-01,  ..., -1.5603e+00,
           8.0499e-01, -7.7964e-01],
         [-1.4387e+00, -1.3080e+00,  3.2929e-01,  ..., -1.7346e+00,
          -1.4460e-01, -1.1347e+00]],

        [[-2.1006e+00, -6.1792e-01, -1.2857e-01,  ..., -1.3465e+00,
          -3.4207e-02, -1.1441e+00],
         [-2.0860e+00, -1.1803e+00, -2.7008e-01,  ..., -9.2096e-01,
           2.3473e-01, -4.6412e-01],
         [-9.9359e-01, -1.4527e+00, -2.3373e-01,  ..., -1.3849e+00,
          -1.4923e-01, -1.3303e+00],
         [-1.8519e+00, -1.3515e+00, -2.2341e-01,  ..., -1.2978e+00,
           2.3844e-01, -1.5133e+00]],

        ...,

        [[-1.8158e+00, -1.5642e+00, -3.3724e-01,  ..., -1.0357e+00,
          -1.0682e-01, -1.1596e+00],
         [-1.0448e+00, -1.3673e+00, -2.0667e-01,  ..., -1.3519e+00,
           4.8474e-03, -8.9976e-01],
         [-2.1078e+00, -1.2572e+00,  2.4586e-02,  ..., -1.3927e+00,
          -2.3791e-01, -5.7987e-01],
         [-8.1770e-01, -9.1909e-01, -3.7701e-02,  ..., -1.6165e+00,
          -6.3230e-02, -1.1810e+00]],

        [[-1.8287e+00, -1.2996e+00, -1.6849e-01,  ..., -1.3236e+00,
           4.3093e-01, -9.1745e-01],
         [-1.4264e+00, -1.5555e+00,  3.2098e-01,  ..., -1.4811e+00,
           2.4194e-01, -9.9068e-01],
         [-2.0787e+00, -1.3633e+00,  3.8891e-01,  ..., -1.5844e+00,
           5.1235e-02, -1.3025e+00],
         [-7.5022e-01, -1.2057e+00,  1.2665e-01,  ..., -1.6811e+00,
          -2.8390e-02, -1.1863e+00]],

        [[-1.7238e+00, -1.2398e+00,  6.6091e-02,  ..., -1.1017e+00,
          -6.9340e-02, -4.0999e-01],
         [-1.7003e+00, -8.3047e-01,  1.6034e-01,  ..., -1.5392e+00,
           3.8630e-01, -1.1758e+00],
         [-1.3196e+00, -1.2307e+00, -5.1266e-02,  ..., -1.4647e+00,
          -2.2774e-01, -1.3746e+00],
         [-1.6791e+00, -1.2090e+00,  3.9734e-01,  ..., -1.1591e+00,
           3.8836e-02, -1.4765e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([950, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([690, 4, 256]), pos_embed.shape: torch.Size([690, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
encoder start :
	 src.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([690, 4, 256]), k.shape: torch.Size([690, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([690, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([690, 4, 256])
	 (after FFN) src2.shape: torch.Size([690, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([690, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.2957e+00, -4.1969e-01, -3.6653e-01,  ..., -6.9687e-01,
           1.1990e+00, -1.8095e+00],
         [-1.9921e+00, -1.3231e+00, -3.8155e-01,  ..., -7.3735e-01,
           1.1967e+00, -1.4688e+00],
         [-2.3754e+00, -9.8335e-01,  1.4321e-01,  ..., -9.0822e-01,
           3.7327e-01, -1.4807e+00],
         [-2.4303e+00, -1.3468e+00,  3.8563e-01,  ..., -7.2798e-01,
           1.5250e+00, -1.5543e+00]],

        [[-2.0983e+00, -1.1300e+00, -3.0530e-02,  ..., -7.3739e-01,
           1.2669e+00, -1.3425e+00],
         [-2.7438e+00, -8.7793e-01,  1.6969e-02,  ..., -9.0984e-01,
           1.4902e+00, -1.2262e+00],
         [-2.5715e+00, -1.3252e+00, -3.3730e-01,  ..., -9.3891e-01,
           1.4393e+00, -1.0585e+00],
         [-2.5219e+00, -9.9809e-01, -1.1207e-01,  ..., -8.1714e-01,
           1.5356e+00, -1.6962e+00]],

        [[-2.0238e+00, -1.1328e+00, -3.0935e-01,  ..., -9.7848e-01,
           1.6902e+00, -1.6548e+00],
         [-2.0828e+00, -3.6112e-01, -3.2427e-01,  ..., -9.2686e-01,
           1.4761e+00, -1.9266e+00],
         [-2.3667e+00, -1.2571e+00, -3.0456e-04,  ..., -8.1659e-01,
           1.4363e+00, -1.3591e+00],
         [-7.6312e-01, -1.5359e+00,  3.5027e-01,  ..., -7.7389e-01,
           1.5703e+00, -1.5615e+00]],

        ...,

        [[-2.2782e+00, -1.3393e+00, -2.6314e-01,  ..., -8.5360e-01,
           1.4456e+00, -1.8198e+00],
         [-2.5779e+00, -1.2938e+00, -7.8233e-03,  ..., -9.2858e-01,
           1.6128e+00, -1.5198e+00],
         [-1.3622e+00, -1.0154e+00,  5.7794e-02,  ..., -9.4601e-01,
           1.3046e+00, -8.3264e-01],
         [-2.1641e+00, -1.2195e+00,  3.9468e-01,  ..., -6.6977e-01,
           1.2828e+00, -1.6428e+00]],

        [[-2.2689e+00, -1.2049e+00, -3.1924e-01,  ..., -1.5128e-01,
           1.5309e+00, -1.4431e+00],
         [-1.9239e+00, -3.9480e-01,  6.7818e-02,  ..., -6.6015e-01,
           1.3856e+00, -1.6705e+00],
         [-2.3652e+00, -3.9176e-01, -1.4746e-01,  ..., -6.9539e-01,
           3.8880e-01, -2.0431e-01],
         [-2.2244e+00, -1.3402e+00, -3.6862e-02,  ..., -1.1537e+00,
           1.2619e+00, -1.6764e+00]],

        [[-2.1849e+00, -8.6935e-01, -2.1390e-01,  ..., -7.6710e-01,
           1.4849e+00, -1.6967e+00],
         [-1.9743e+00, -1.3164e+00, -4.1796e-01,  ..., -8.0071e-01,
           1.6530e+00, -1.6856e+00],
         [-2.2326e+00, -7.6671e-01, -1.1833e-03,  ..., -4.2523e-01,
           1.4425e+00, -1.4529e+00],
         [-2.2100e+00, -1.4302e+00,  1.1424e-01,  ..., -1.0385e+00,
           1.2120e+00, -1.3098e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5815, -0.3004, -0.0354,  ..., -0.3998,  0.6051, -0.5867],
         [-1.4394, -0.5006,  0.1132,  ..., -0.5223,  1.0584, -0.1120],
         [-1.0649, -0.7863,  0.6897,  ..., -0.8895,  0.4512,  0.0403],
         [-1.6130, -0.6456,  0.7892,  ..., -0.4696,  0.8241, -0.4285]],

        [[-1.5361, -0.5886,  0.1814,  ..., -0.2753,  1.0295, -0.1307],
         [-1.9682, -0.6067,  0.2561,  ..., -0.4113,  0.6722, -0.2416],
         [-1.7170, -0.8064,  0.4608,  ..., -0.4175,  0.5518, -0.1887],
         [-1.9127, -0.5642,  0.5388,  ..., -0.7608,  0.7503, -0.3426]],

        [[-1.7702, -0.9230,  0.0233,  ..., -0.4850,  1.1274, -0.6907],
         [-1.2749, -0.2389,  0.0294,  ..., -0.5620,  0.6907, -0.6509],
         [-1.8672, -0.9083,  0.0941,  ..., -0.3558,  0.6607, -0.4867],
         [-0.4059, -1.0700,  0.6323,  ..., -0.4188,  1.0913, -0.6359]],

        ...,

        [[-1.3266, -0.7445,  0.7154,  ..., -0.3934,  0.7733, -0.4974],
         [-1.7753, -0.7918,  0.6268,  ..., -0.1425,  0.7010, -0.5535],
         [-1.4299, -0.3692,  0.5292,  ..., -0.1591,  0.6116, -0.0772],
         [-1.6440, -0.6295,  0.8357,  ..., -0.3149,  0.7284, -0.4040]],

        [[-1.4644, -0.6759,  0.2178,  ..., -0.1661,  0.8139, -0.5299],
         [-1.6386, -0.6730,  0.4482,  ..., -0.4102,  0.7286, -0.3332],
         [-1.7101, -0.3715,  0.3723,  ..., -0.4621,  0.1175,  0.5957],
         [-1.6913, -0.7460,  0.6285,  ..., -0.3418,  0.7680, -0.0436]],

        [[-1.9199, -0.2615,  0.5666,  ..., -0.5363,  0.7814, -0.1842],
         [-1.4570, -0.6452,  0.2947,  ..., -0.2175,  0.6905, -0.5104],
         [-1.9468, -0.2702,  0.4660,  ...,  0.0543,  0.5927, -0.3547],
         [-1.3698, -0.7643,  0.8722,  ..., -0.8422,  0.8629, -0.2166]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.1227, -1.8372,  0.2509,  ..., -0.7487,  0.4850, -2.3760],
         [-1.3417, -2.1666,  0.0879,  ..., -0.4025,  0.1908, -1.4634],
         [-1.5847, -2.1690,  0.5202,  ..., -0.6394,  0.9682, -1.7823],
         [-1.5828, -1.8761,  0.8610,  ..., -1.0717,  0.6476, -2.2234]],

        [[-1.5233, -1.8598,  0.3897,  ..., -0.1760,  1.0494, -1.9840],
         [-1.2656, -2.1395,  0.3845,  ..., -0.5382,  0.8114, -2.2682],
         [-1.6699, -1.3949,  0.2666,  ..., -0.9262,  0.8515, -1.6237],
         [-1.5388, -2.1895,  0.4729,  ..., -0.8525,  1.0120, -2.3143]],

        [[-1.5672, -2.4580,  0.3127,  ..., -0.9721,  1.2176, -2.2235],
         [-1.3185, -1.9438, -0.0195,  ..., -0.9613,  0.9629, -2.4051],
         [-1.7508, -2.4842,  0.0726,  ..., -0.6973,  1.0279, -1.5862],
         [-1.1010, -2.3463,  0.3195,  ..., -0.8672,  1.2982, -2.2673]],

        ...,

        [[-1.5650, -1.3795,  0.4498,  ..., -0.5755,  0.9018, -1.3125],
         [-1.2660, -2.1710,  0.4604,  ..., -0.5799,  0.9161, -1.9215],
         [-1.5028, -1.9458,  0.4177,  ..., -0.1026,  0.9146, -1.9513],
         [-1.7499, -1.9723,  0.6785,  ..., -0.5359,  0.8217, -2.1294]],

        [[-1.0589, -2.1341,  0.1416,  ..., -0.3460,  1.0247, -2.3366],
         [-1.5876, -2.2964, -0.0690,  ..., -0.7443,  0.6859, -1.5283],
         [-1.4714, -1.9470,  0.4672,  ..., -0.6424,  0.5678, -1.8780],
         [-1.3967, -2.1763,  0.8867,  ..., -0.7918,  1.1065, -2.3772]],

        [[-1.1702, -1.1129,  0.2820,  ..., -0.3365,  0.9824, -2.1959],
         [-1.5863, -1.2347,  0.2822,  ..., -0.6106,  1.0794, -2.2546],
         [-1.6796, -1.9890,  0.1885,  ..., -0.3158,  1.0035, -2.4044],
         [-1.1359, -1.1802,  0.1369,  ..., -1.2196,  0.7137, -2.0669]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.6858, -2.6753,  0.0402,  ..., -1.3817,  1.1836, -0.4301],
         [-1.4368, -2.9185,  0.1220,  ..., -0.6992,  0.6985, -0.1736],
         [-1.5967, -2.2659,  0.3886,  ..., -1.3143,  1.0349, -0.3272],
         [-1.5586, -2.6435,  0.4634,  ..., -0.9235,  1.1447, -0.5598]],

        [[-1.4898, -2.4737,  0.3150,  ..., -0.9344,  1.0337, -0.1147],
         [-1.3654, -2.6445,  0.3886,  ..., -1.0348,  1.1499, -0.2389],
         [-1.7559, -2.4750,  0.6593,  ..., -0.4813,  1.2397, -0.0034],
         [-1.2452, -2.7813,  0.2868,  ..., -0.7992,  1.1113, -0.1739]],

        [[-1.4475, -2.9234,  0.2938,  ..., -1.3170,  0.4926, -0.8086],
         [-1.2295, -2.5516, -0.1295,  ..., -1.1090,  1.0356, -0.7970],
         [-0.9288, -1.9128,  0.0931,  ..., -1.2119,  0.6733, -0.2915],
         [-1.3488, -2.7556,  0.6074,  ..., -1.1913,  0.6067, -0.6622]],

        ...,

        [[-1.6203, -2.3613,  0.3257,  ..., -1.2562,  1.6270, -0.2986],
         [-1.5208, -2.2457,  0.3708,  ..., -0.9875,  0.8052, -0.3449],
         [-1.5644, -2.5153,  0.4081,  ..., -0.7076,  1.2572, -0.3108],
         [-1.7280, -2.7878,  0.6052,  ..., -1.2941,  1.3131, -0.4731]],

        [[-1.1047, -2.9816,  0.2388,  ..., -0.8876,  1.4064, -0.5843],
         [-1.5520, -2.6497,  0.4119,  ..., -0.4965,  0.9239, -0.0256],
         [-1.3271, -2.7706,  0.5663,  ..., -1.0860,  0.4306, -0.1740],
         [-1.5106, -2.5424,  0.5517,  ..., -1.1548,  1.1098, -0.2815]],

        [[-1.5750, -2.3753,  0.1939,  ..., -0.8465,  1.0647, -0.3538],
         [-1.7740, -2.2541, -0.0272,  ..., -0.8513,  1.6370, -0.5658],
         [-1.7442, -2.7088,  0.4183,  ..., -0.7909,  1.5914, -0.2948],
         [-0.6028, -2.3087,  0.1875,  ..., -1.4272,  0.9921, -0.6125]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7372, -1.5714, -0.3792,  ..., -1.6053,  0.3985, -1.1495],
         [-1.0697, -1.3116, -0.3890,  ..., -1.2544, -0.1584, -1.1903],
         [-1.7697, -0.9876, -0.3595,  ..., -1.9388, -0.1471, -1.1640],
         [-1.8656, -1.2210, -0.0916,  ..., -1.4812,  0.2846, -1.3694]],

        [[-2.3111, -1.1713,  0.3969,  ..., -1.3360,  0.2156, -1.1789],
         [-1.8505, -1.4330, -0.3135,  ..., -1.4414,  0.3272, -1.5987],
         [-1.1928, -1.0774, -0.1893,  ..., -1.5742,  0.4123, -1.3832],
         [-2.1130, -1.1599, -0.2960,  ..., -1.4643,  0.3312, -1.1924]],

        [[-2.0996, -1.1086, -0.1779,  ..., -1.3754, -0.0503, -1.4155],
         [-1.7849, -1.4553, -0.4580,  ..., -1.3435,  0.4857, -1.2069],
         [-1.8233, -0.8521, -0.3616,  ..., -1.7276, -0.0288, -1.2553],
         [-1.1451, -1.2137, -0.3401,  ..., -1.5367, -0.0600, -1.3304]],

        ...,

        [[-1.9990, -0.8964, -0.0115,  ..., -0.6475,  0.0159, -1.2144],
         [-1.9507, -1.1990, -0.3075,  ..., -1.0484, -0.0828, -0.4248],
         [-2.1320, -1.0669,  0.3936,  ..., -1.3006,  0.3735, -0.4278],
         [-1.5558, -1.6485, -0.3662,  ..., -1.7446,  0.6648, -1.1872]],

        [[-1.2900, -1.4925, -0.1874,  ..., -1.4182,  0.3871, -1.2738],
         [-2.1816, -1.4642,  0.4202,  ..., -1.3798, -0.1807, -0.8489],
         [-2.1358, -1.6600, -0.2590,  ..., -1.5840, -0.3090, -1.2852],
         [-1.6193, -1.2206,  0.4116,  ..., -1.6899,  0.2870, -1.1132]],

        [[-1.5278, -1.2449, -0.5291,  ..., -1.6746, -0.1128, -0.6323],
         [-2.1193, -0.8937,  0.2214,  ..., -1.6251,  0.1010, -0.7467],
         [-2.3699, -1.3798,  0.0091,  ..., -1.2793,  0.5881, -1.2595],
         [-1.7413, -1.1075, -0.2418,  ..., -1.6558,  0.0506, -1.2803]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([690, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([672, 4, 256]), pos_embed.shape: torch.Size([672, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
encoder start :
	 src.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([672, 4, 256]), k.shape: torch.Size([672, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([672, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([672, 4, 256])
	 (after FFN) src2.shape: torch.Size([672, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([672, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1619, -0.9498,  0.4534,  ..., -0.9020,  1.1825, -0.6661],
         [-2.3718, -1.3014, -0.3309,  ..., -0.9200,  1.1194, -1.7218],
         [-2.2288, -1.6067, -0.1563,  ..., -0.3260,  1.2298, -0.9000],
         [-2.0774, -0.4774, -0.1298,  ..., -0.8616,  1.3270, -1.5215]],

        [[-2.3326, -1.5130,  0.2587,  ..., -1.0828,  0.3444, -1.4843],
         [-2.0099, -1.1724, -0.4521,  ..., -0.9801,  1.1571, -1.5538],
         [-1.9518, -1.4143,  0.3559,  ..., -0.8092,  1.3091, -1.6993],
         [-2.2784, -1.4624,  0.1406,  ..., -0.7121,  1.4358, -1.4718]],

        [[-0.7470, -0.8327, -0.2064,  ..., -0.9845, -0.0110, -1.6617],
         [-2.3379, -1.0881,  0.0121,  ..., -0.8971,  1.3079, -1.3725],
         [-2.1801, -1.2540, -0.1017,  ..., -0.3507,  1.1062, -1.8089],
         [-2.1837, -0.8917, -0.0463,  ..., -0.9416,  1.3444, -1.5634]],

        ...,

        [[-2.3107, -0.4304, -0.1567,  ..., -0.9324,  1.3068, -0.4646],
         [-2.3272, -1.1396,  0.4052,  ..., -0.8118,  1.1908, -1.7475],
         [-2.1863, -1.1777, -0.1174,  ..., -1.0712,  1.3075, -0.5691],
         [-1.8547, -0.7691,  0.0221,  ..., -0.6385,  1.1803, -1.6276]],

        [[-2.2328, -1.2049,  0.1551,  ..., -1.1442,  1.1106, -1.3122],
         [-2.1530, -0.4146,  0.0159,  ..., -0.8431,  1.1255, -0.5971],
         [-2.0618, -0.9610,  0.3795,  ..., -0.9289,  1.4176, -1.5826],
         [-2.0099, -1.2287,  0.2752,  ..., -0.3811,  1.4239, -1.4786]],

        [[-2.1564, -0.4608,  0.2346,  ..., -0.2386,  1.2543, -1.5928],
         [-2.2871, -1.0521, -0.1622,  ..., -0.6577,  1.1670, -1.6359],
         [-2.2557, -1.1505,  0.0799,  ..., -1.1236,  0.5558, -1.2189],
         [-2.4246, -0.8279, -0.1295,  ..., -0.9062,  1.3685, -1.6738]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8484, -0.6921,  0.9141,  ..., -0.3853,  0.3368, -0.0049],
         [-1.5619, -0.6223,  0.1107,  ..., -0.4749,  0.5441, -0.5100],
         [-1.6843, -0.7124,  0.5224,  ..., -0.0765,  0.6132, -0.1209],
         [-1.7696, -0.4474,  0.8129,  ...,  0.0395,  0.4860, -0.3788]],

        [[-1.8564, -1.1252,  0.6180,  ..., -0.3709,  0.4210, -0.0352],
         [-1.4013, -0.6545,  0.3318,  ..., -0.5795,  0.6378, -0.3348],
         [-1.3629, -0.6885,  0.3156,  ..., -0.4068,  0.6984, -0.5597],
         [-1.5433, -0.9245,  0.5528,  ..., -0.1343,  0.6421, -0.1832]],

        [[-1.2097, -0.6291,  0.5452,  ..., -0.5404,  0.1002, -0.0516],
         [-1.7761, -0.3595,  0.2080,  ..., -0.3948,  0.5009, -0.4885],
         [-1.3056, -0.3734,  0.5499,  ..., -0.2153,  0.4053, -0.5132],
         [-1.5659, -0.6487,  0.5234,  ..., -0.5212,  0.7789, -0.2040]],

        ...,

        [[-1.7548, -0.6221,  0.4373,  ..., -0.5793,  0.7428, -0.1096],
         [-1.9898, -0.9523,  0.9916,  ..., -0.1799,  0.6649, -0.7067],
         [-1.5457, -0.7832,  0.7162,  ..., -0.4402,  0.7196,  0.0503],
         [-1.7645, -0.2803,  0.2023,  ..., -0.3130,  0.8053, -0.5496]],

        [[-1.6888, -0.8540,  0.8742,  ..., -0.6327,  0.3595, -0.1953],
         [-0.9714, -0.6015,  0.4576,  ..., -0.7127,  0.4779, -0.2592],
         [-1.7556, -0.8358,  0.5915,  ..., -0.7281,  0.7103,  0.0805],
         [-1.6784, -0.7872,  0.6656,  ..., -0.2408,  0.9644, -0.7757]],

        [[-1.2476, -0.4124,  0.9406,  ..., -0.2029,  0.7022, -0.4906],
         [-1.6664, -0.7247,  0.3544,  ..., -0.2857,  0.8996, -0.3547],
         [-1.6073, -0.4633,  1.0035,  ..., -0.5540,  0.2498, -0.4611],
         [-1.1673, -0.2960,  0.6439,  ..., -0.6706,  0.7647, -0.6336]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4854, -1.2546,  0.3782,  ..., -0.7824,  0.9838, -1.3473],
         [-1.4932, -2.2357,  0.5968,  ..., -0.6808,  0.9126, -2.2141],
         [-1.5198, -2.1637,  0.1368,  ..., -0.7049,  1.1366, -1.5571],
         [-1.6839, -2.2639,  0.3818,  ...,  0.0323,  0.7145, -1.8783]],

        [[-1.2029, -2.5017,  0.4676,  ..., -0.4404,  0.9574, -1.4416],
         [-1.4833, -1.9806, -0.2655,  ..., -0.5295,  0.9441, -2.0171],
         [-1.7542, -2.3362, -0.2975,  ..., -0.6809,  0.8641, -2.3838],
         [-1.6794, -1.4154,  0.4953,  ..., -0.1036,  0.2447, -1.7670]],

        [[-1.4973, -2.0755,  0.3931,  ..., -0.9965,  0.8927, -2.1176],
         [-1.3927, -1.8626,  0.6520,  ..., -0.0129,  0.8519, -1.5139],
         [-1.1779, -2.0914,  0.3032,  ...,  0.3136,  0.6812, -1.5135],
         [-1.6231, -2.2236,  0.2037,  ..., -0.3336,  0.9347, -1.9478]],

        ...,

        [[-1.6715, -2.0887,  0.4864,  ..., -0.9282,  0.8771, -1.5419],
         [-1.6209, -2.2480,  0.4298,  ..., -0.6633,  0.9502, -2.0676],
         [-0.8420, -2.1398,  0.2604,  ..., -0.6733,  0.1075, -2.3943],
         [-1.5195, -1.9254,  0.3012,  ..., -0.4714,  1.2252, -2.1704]],

        [[-1.6360, -2.1782,  0.4607,  ..., -0.5288,  0.8456, -1.9979],
         [-1.1708, -2.0695,  0.1736,  ..., -0.8688,  0.6935, -2.1821],
         [-1.4482, -2.0927,  0.3616,  ..., -0.7175,  0.9468, -1.4315],
         [-1.6892, -2.1211,  0.4462,  ...,  0.0610,  0.8870, -2.1178]],

        [[-1.7413, -1.1617, -0.1240,  ..., -0.6097,  1.0079, -1.3371],
         [-1.6751, -2.1416,  0.2894,  ..., -0.6792,  1.0004, -2.2059],
         [-1.5942, -1.1457,  0.3789,  ..., -0.0386,  0.7757, -1.6399],
         [-1.5274, -0.2374,  0.2877,  ..., -0.9624,  0.9798, -2.0589]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3257, -2.5978,  0.2357,  ..., -1.3360,  1.4547, -0.0277],
         [-1.6595, -2.5404,  0.4688,  ..., -1.2224,  1.2498, -0.4026],
         [-1.6151, -2.7739,  0.1642,  ..., -1.2425,  1.3369, -0.3595],
         [-1.3904, -2.3070,  0.1872,  ..., -1.0845, -0.0897, -0.0971]],

        [[-1.3158, -3.0772,  0.1675,  ..., -1.0339,  1.5354,  0.2554],
         [-1.5834, -2.5952,  0.3120,  ..., -1.0516,  0.2814, -0.1828],
         [-1.6427, -2.3037,  0.5295,  ..., -0.6953,  0.5433, -0.4923],
         [-1.8088, -2.5391,  0.5506,  ..., -0.2892,  0.9205, -0.4334]],

        [[-1.8189, -2.7510,  0.3941,  ..., -1.0271,  1.1607, -0.1162],
         [-1.3371, -2.1969,  0.2302,  ..., -1.1107,  1.1700, -0.5488],
         [-1.6314, -2.6885,  0.3032,  ..., -0.6892,  0.7666, -0.0624],
         [-1.5750, -2.6009,  0.2342,  ..., -0.8085,  1.0698, -0.3298]],

        ...,

        [[-1.4151, -1.3649,  0.4609,  ..., -1.1145,  1.0329,  0.0136],
         [-1.5896, -2.2340,  0.2182,  ..., -1.2275,  0.3797, -0.5645],
         [-1.3402, -2.3375,  0.1344,  ..., -0.7579,  1.0728, -0.3026],
         [-1.3754, -1.6675,  0.4882,  ..., -0.4473,  1.2731, -0.2994]],

        [[-1.4697, -1.7951,  0.5407,  ..., -1.0530,  0.5919,  0.0162],
         [-1.2001, -2.5737,  0.1244,  ..., -0.5818,  0.5944, -0.3642],
         [-1.4108, -2.7357,  0.2302,  ..., -1.0467,  1.1951, -0.1053],
         [-1.8984, -2.5871,  0.6300,  ..., -0.9909,  1.1477, -0.5804]],

        [[-1.6880, -2.2941,  0.1366,  ..., -1.2671,  1.5231,  0.2150],
         [-1.6066, -2.8850,  0.2999,  ..., -1.3092,  1.5676, -0.2354],
         [-1.5855, -2.3068,  0.5601,  ..., -1.0969,  0.7071, -0.2019],
         [-1.4870, -1.8747,  0.1956,  ..., -1.1217,  1.0355, -0.4615]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9485e+00, -1.0032e+00, -3.7954e-01,  ..., -1.8460e+00,
           7.2721e-01, -9.5638e-01],
         [-2.0512e+00, -1.4874e+00, -2.3796e-01,  ..., -1.6534e+00,
           4.6612e-01, -6.4565e-01],
         [-2.0436e+00, -1.1675e+00, -2.4930e-02,  ..., -1.6525e+00,
           1.7985e-01, -1.4044e+00],
         [-9.6821e-01, -1.0895e+00, -3.8883e-01,  ..., -1.6905e+00,
          -1.8501e-01, -9.2436e-01]],

        [[-1.9422e+00, -1.4476e+00, -4.6117e-01,  ..., -9.3714e-01,
           1.8583e-01, -8.9350e-01],
         [-1.7621e+00, -7.8724e-01, -4.8072e-01,  ..., -1.3271e+00,
           5.3714e-02, -1.1240e+00],
         [-1.1799e+00, -1.3090e+00, -8.3489e-01,  ..., -1.4428e+00,
          -4.5844e-02, -9.9482e-01],
         [-1.8175e+00, -1.0618e+00, -1.1966e-01,  ..., -1.4974e+00,
          -2.2588e-01, -1.3132e+00]],

        [[-1.9635e+00, -1.3571e+00, -1.9158e-01,  ..., -9.2142e-01,
           1.0262e-01, -9.1061e-01],
         [-2.1335e+00, -1.1104e+00, -4.4439e-01,  ..., -1.2649e+00,
           1.1002e-01, -1.0070e+00],
         [-2.2442e+00, -1.0214e+00, -3.5247e-01,  ..., -5.3979e-01,
           1.0100e-03, -1.0322e+00],
         [-2.1279e+00, -1.2503e+00, -1.5859e-01,  ..., -1.5022e+00,
          -1.1653e-02, -1.1150e+00]],

        ...,

        [[-1.1483e+00, -7.1036e-01, -3.5779e-01,  ..., -1.7679e+00,
           2.1151e-01, -9.8051e-01],
         [-1.7819e+00, -1.1186e+00, -1.6804e-01,  ..., -1.6681e+00,
          -1.9129e-01, -8.2022e-01],
         [-2.0331e+00, -1.0974e+00, -2.6730e-01,  ..., -1.4732e+00,
           1.6467e-01, -1.2263e+00],
         [-1.7702e+00, -8.5646e-01, -1.4489e-01,  ..., -1.2844e+00,
           6.2501e-01, -1.3950e+00]],

        [[-1.9336e+00, -8.3399e-01,  3.0335e-01,  ..., -1.6443e+00,
           2.5515e-01, -9.5157e-01],
         [-1.8711e+00, -1.3976e+00, -2.0044e-01,  ..., -1.3115e+00,
           4.5621e-03, -8.8253e-01],
         [-2.1677e+00, -1.1773e+00, -5.7457e-01,  ..., -1.4775e+00,
          -1.3338e-01, -1.2270e+00],
         [-2.2611e+00, -8.2097e-01, -4.5237e-01,  ..., -1.1509e+00,
           1.8749e-01, -9.9279e-01]],

        [[-2.4918e+00, -9.2841e-01, -2.3024e-01,  ..., -1.8699e+00,
           5.1226e-01, -1.1346e+00],
         [-2.1727e+00, -1.3851e+00, -4.9091e-01,  ..., -8.8206e-01,
           3.2047e-01, -3.5114e-01],
         [-1.8949e+00, -1.1104e+00, -3.4219e-01,  ..., -1.6384e+00,
          -2.7320e-01, -8.2015e-01],
         [-1.2941e+00, -7.1058e-01,  2.6288e-01,  ..., -1.3546e+00,
           2.9234e-01, -1.3375e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([672, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([792, 4, 256]), pos_embed.shape: torch.Size([792, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
encoder start :
	 src.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([792, 4, 256]), k.shape: torch.Size([792, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([792, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([792, 4, 256])
	 (after FFN) src2.shape: torch.Size([792, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([792, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9893, -1.0301, -0.1457,  ..., -0.5489,  1.1787, -1.5076],
         [-2.1726, -1.2041, -0.0973,  ..., -0.9085,  1.2803, -1.4387],
         [-2.2923, -0.4885,  0.0702,  ..., -0.6928,  1.2548, -1.5930],
         [-1.9453, -1.2033,  0.2035,  ..., -0.6583,  1.0347, -1.1977]],

        [[-1.9948, -0.9627, -0.1598,  ..., -0.7287,  1.1805, -1.6610],
         [-2.3283, -1.4598, -0.0478,  ..., -0.9334,  1.2247, -1.4696],
         [-2.2317, -1.3714, -0.0027,  ..., -1.0302,  1.1139, -0.8793],
         [-2.3850, -1.2792, -0.1889,  ..., -0.1730,  1.1933, -1.7221]],

        [[-1.9292, -1.4656, -0.0248,  ..., -0.7253,  1.2696, -1.3773],
         [-2.3741, -1.1991,  0.0960,  ..., -0.8913,  1.3485, -0.8507],
         [-2.2802, -1.3927, -0.0376,  ..., -0.6531,  0.4124, -1.2387],
         [-2.0994, -1.5452, -0.1698,  ..., -1.0570,  1.2745, -0.8462]],

        ...,

        [[-2.0525, -0.8920, -0.4960,  ..., -0.4805,  1.0206, -1.3641],
         [-2.4534, -0.9715, -0.2071,  ..., -0.7072,  1.0599, -1.5101],
         [-2.3074, -1.4167,  0.0278,  ..., -1.0083,  0.9957, -1.4579],
         [-0.5962, -1.1191, -0.2617,  ..., -1.0323,  1.4663, -1.4138]],

        [[-0.7331, -1.3583, -0.2365,  ..., -0.7479,  1.2656, -1.2283],
         [-1.9406, -1.4703, -0.1832,  ..., -0.8190,  1.3025, -1.2867],
         [-2.1081, -1.3824,  0.2245,  ..., -0.6916,  1.4855, -1.2528],
         [-2.1339, -1.3481,  0.0643,  ..., -0.9695,  0.9453, -1.2127]],

        [[-1.9680, -0.8952,  0.0665,  ..., -0.6404,  1.1392, -1.5106],
         [-2.1090, -1.1917, -0.5120,  ..., -0.9452,  1.2885, -1.5699],
         [-2.5043, -1.2555,  0.1872,  ..., -0.9615,  1.5930, -1.4670],
         [-2.5951, -0.5151, -0.0977,  ..., -0.8402,  1.0209, -1.6609]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7796, -0.6807,  0.2197,  ..., -0.3204,  0.6945, -0.2890],
         [-0.8431, -0.8838,  0.5318,  ..., -0.3363,  0.7961, -0.3853],
         [-1.7466, -0.2100,  0.5253,  ..., -0.4508,  0.5005, -0.1519],
         [-1.5200, -1.2072,  0.7202,  ..., -0.1614,  0.5778, -0.2419]],

        [[-1.5034, -0.4070,  0.0201,  ..., -0.4372,  0.7444, -0.4170],
         [-1.7594, -0.9229,  0.4698,  ..., -0.5410,  0.6859, -0.3861],
         [-2.0334, -0.8690,  0.5095,  ..., -0.5934,  0.8751, -0.1367],
         [-1.9069, -0.6515,  0.5165,  ...,  0.0169,  0.3111, -0.5739]],

        [[-1.5342, -0.8749,  0.0572,  ..., -0.0158,  0.5352, -0.1851],
         [-1.6386, -0.7782,  0.6676,  ..., -0.5894,  0.9398, -0.5293],
         [-1.6089, -0.7051,  0.5094,  ..., -0.4021,  0.1112, -0.2986],
         [-1.7352, -1.0136,  0.0816,  ..., -0.5724,  0.7519, -0.0814]],

        ...,

        [[-0.9741, -0.8197,  0.2499,  ..., -0.4399,  0.1471, -0.1625],
         [-1.4958, -0.9241,  0.3496,  ..., -0.3619,  0.8634, -0.2272],
         [-1.0669, -0.6209,  0.5580,  ..., -0.7365,  0.5346, -0.2693],
         [-1.1845, -0.6959,  0.5626,  ..., -0.1448,  0.6586, -0.1193]],

        [[-1.1866, -0.6862,  0.1256,  ...,  0.1699,  0.4670, -0.5710],
         [-1.4763, -1.1949, -0.0707,  ..., -0.5858,  0.5709,  0.0822],
         [-1.4504, -0.8811,  0.6885,  ..., -0.0596,  0.7562, -0.2546],
         [-1.5179, -0.6831,  0.3384,  ..., -0.1239,  0.2058, -0.1477]],

        [[-1.7058, -0.3428,  0.4258,  ..., -0.3689,  0.5206, -0.2410],
         [-1.4005, -0.7195,  0.0280,  ..., -0.1990,  0.5763, -0.2814],
         [-1.8014, -0.7924,  0.4467,  ..., -0.2403,  0.8523, -0.3028],
         [-1.7546, -0.3795,  0.5170,  ..., -0.4391,  0.7244, -0.0430]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.0129e+00, -2.0637e+00,  2.1618e-01,  ..., -3.7380e-01,
           6.2273e-01, -1.8105e+00],
         [-1.5404e+00, -1.8923e+00,  3.9535e-01,  ..., -3.5842e-01,
           1.1513e+00, -2.0119e+00],
         [-1.0119e+00, -2.0712e+00,  5.7113e-01,  ..., -5.6018e-01,
           8.7274e-01, -1.7423e+00],
         [-1.7349e+00, -2.2959e+00,  8.8858e-01,  ..., -7.3068e-01,
           8.2085e-01, -1.9563e+00]],

        [[-1.6347e+00, -1.9520e+00,  5.5786e-02,  ..., -8.4700e-01,
           2.8461e-01, -2.2571e+00],
         [-1.7388e+00, -2.4515e+00,  1.7059e-02,  ..., -4.8944e-01,
           6.2678e-01, -2.0124e+00],
         [-1.7219e+00, -2.2691e+00,  5.5256e-01,  ..., -7.0084e-01,
           1.0494e+00, -1.9403e+00],
         [-1.8359e+00, -1.8686e+00,  4.4115e-01,  ..., -4.0444e-01,
           7.7456e-01, -1.7139e+00]],

        [[-1.2563e+00, -2.1557e+00, -1.2244e-01,  ..., -1.5579e-01,
           8.0603e-01, -1.4471e+00],
         [-1.1774e+00, -2.3730e+00,  5.1595e-01,  ..., -1.5241e-02,
           1.1541e+00, -1.6581e+00],
         [-1.2208e+00, -1.7894e+00, -3.1416e-01,  ..., -5.8173e-01,
           4.6304e-01, -2.0888e+00],
         [-1.2894e+00, -2.2015e+00, -1.4278e-01,  ..., -1.3247e+00,
           8.9171e-01, -1.4962e+00]],

        ...,

        [[-1.3589e+00, -2.2014e+00,  7.2347e-01,  ..., -5.2674e-01,
           9.8798e-01, -1.9366e+00],
         [-9.1872e-01, -2.4192e+00,  5.2327e-01,  ..., -7.0695e-01,
           1.1106e+00, -2.3224e+00],
         [-7.2788e-01, -2.1030e+00,  6.3727e-02,  ..., -9.1209e-01,
           6.8261e-01, -2.2498e+00],
         [-1.4260e+00, -1.9606e+00,  8.1555e-01,  ..., -4.5911e-01,
           1.0085e+00, -2.1511e+00]],

        [[-1.5195e+00, -1.2422e+00,  2.0384e-01,  ..., -3.9972e-01,
           6.4382e-01, -2.3207e+00],
         [-1.6266e+00, -2.5903e+00,  1.7932e-01,  ...,  3.6303e-04,
           9.1157e-01, -2.0231e+00],
         [-1.3908e+00, -2.0584e+00,  5.2957e-01,  ..., -5.8003e-01,
           1.0228e+00, -1.7982e+00],
         [-1.5198e+00, -1.9968e+00,  3.1948e-02,  ..., -4.7966e-01,
           5.8483e-01, -1.9776e+00]],

        [[-1.6886e+00, -2.4834e+00,  5.1413e-01,  ..., -3.6073e-01,
           8.5161e-01, -1.9442e+00],
         [-1.4352e+00, -2.1292e+00,  3.1768e-02,  ..., -6.5877e-01,
           1.0002e+00, -1.5408e+00],
         [-1.4306e+00, -1.1467e+00,  2.7552e-01,  ..., -4.4524e-01,
           9.2578e-01, -2.0553e+00],
         [-1.6580e+00, -2.3258e+00,  2.9581e-01,  ..., -5.4002e-01,
           9.8552e-01, -1.8104e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-8.7860e-01, -2.8752e+00,  1.2433e-01,  ..., -1.0035e+00,
           1.3847e+00,  4.8077e-02],
         [-1.7569e+00, -2.8587e+00,  1.4439e-01,  ..., -9.1072e-01,
           1.3416e+00, -1.2188e-01],
         [-1.4639e+00, -2.6620e+00,  3.0418e-01,  ..., -1.2945e+00,
           7.3192e-01, -1.1932e-01],
         [-7.8114e-01, -2.6642e+00,  4.9189e-01,  ..., -1.3923e+00,
           1.2238e+00,  1.5551e-02]],

        [[-1.8480e+00, -2.6811e+00,  2.3340e-01,  ..., -1.3738e+00,
           7.7038e-01, -1.0263e-01],
         [-1.8395e+00, -3.1664e+00,  2.3255e-01,  ..., -9.8989e-01,
           1.3020e+00,  1.0069e-01],
         [-1.7302e+00, -1.8686e+00,  3.3113e-01,  ..., -1.1932e+00,
           1.0430e+00,  2.1881e-01],
         [-1.4541e+00, -2.6205e+00,  3.2549e-01,  ..., -1.0969e+00,
           1.0283e+00, -7.8290e-02]],

        [[-7.8660e-01, -2.4692e+00, -3.1522e-01,  ..., -1.1969e+00,
           9.8331e-01,  2.4826e-01],
         [-1.7039e+00, -2.8444e+00,  3.5963e-01,  ..., -6.2388e-01,
           1.0968e+00,  3.9650e-01],
         [-6.2060e-01, -2.4607e+00,  7.1804e-02,  ..., -1.1685e+00,
           1.1919e-01, -7.9359e-01],
         [-1.5702e+00, -2.5904e+00, -1.2503e-01,  ..., -1.4225e+00,
           1.1217e+00, -3.2515e-01]],

        ...,

        [[-1.7799e+00, -2.9059e+00,  5.9616e-01,  ..., -8.2426e-01,
           4.2489e-01, -6.9970e-02],
         [-1.2650e+00, -2.8133e+00,  1.2594e-01,  ..., -9.3759e-01,
           1.3365e+00, -2.8679e-01],
         [-8.8798e-01, -2.4743e+00,  9.8387e-02,  ..., -1.2590e+00,
           7.0500e-01, -1.9916e-01],
         [-1.0953e+00, -2.7789e+00,  2.4867e-01,  ..., -7.0439e-01,
           1.2299e+00, -1.9966e-01]],

        [[-1.4729e+00, -2.4242e+00,  3.2165e-01,  ..., -1.1685e+00,
           7.7824e-01, -2.7153e-01],
         [-1.6414e+00, -2.6698e+00,  5.2354e-01,  ..., -9.0005e-01,
           1.3340e+00,  9.5579e-02],
         [-1.4368e+00, -2.3373e+00,  5.0466e-01,  ..., -1.2019e+00,
           5.9024e-01, -9.7895e-02],
         [-1.3518e+00, -2.6509e+00,  2.7997e-01,  ..., -1.0772e+00,
           1.1725e+00, -2.7463e-01]],

        [[-1.4041e+00, -2.8246e+00,  5.8824e-01,  ..., -7.5000e-01,
           9.1357e-01,  3.3689e-02],
         [-1.4745e+00, -2.8406e+00,  2.0029e-01,  ..., -1.2491e+00,
           1.2559e+00,  2.0520e-01],
         [-1.3716e+00, -1.8201e+00, -6.1501e-04,  ..., -7.3196e-01,
           1.5480e+00, -2.8327e-01],
         [-1.7503e+00, -2.9503e+00,  1.5742e-01,  ..., -9.2760e-01,
           9.4039e-01,  7.5631e-02]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.1380, -1.2473, -0.3302,  ..., -1.5059,  0.3542, -1.1529],
         [-1.8247, -1.0264, -0.6854,  ..., -1.5121,  0.3510, -1.2095],
         [-1.8626, -1.2609, -0.4293,  ..., -1.5080, -0.0913, -0.8435],
         [-1.5301, -1.4427, -0.0869,  ..., -1.9420,  0.0201, -1.1553]],

        [[-2.0680, -0.8433, -0.2656,  ..., -1.5527,  0.4479, -0.2266],
         [-2.0046, -1.3749, -0.3297,  ..., -1.1664,  0.0828, -0.9486],
         [-1.1922, -0.8950, -0.5646,  ..., -1.7865,  0.2750, -0.6123],
         [-2.0732, -1.4473, -0.4366,  ..., -1.4269,  0.2491, -0.8913]],

        [[-1.4238, -1.2587, -0.4336,  ..., -1.6034,  0.0894, -0.7177],
         [-2.0583, -1.2853, -0.2662,  ..., -1.4724,  0.4720, -0.8111],
         [-1.2660, -1.1576, -0.4444,  ..., -1.6954, -0.1940, -1.2243],
         [-2.1042, -1.0335, -0.7577,  ..., -1.3677,  0.1500, -0.9439]],

        ...,

        [[-1.0794, -1.3291, -0.1806,  ..., -1.6567, -0.4341, -0.8833],
         [-1.6460, -1.2673, -0.4100,  ..., -1.4809,  0.2604, -0.9364],
         [-1.9178, -0.6655, -0.6202,  ..., -1.9018, -0.2246, -0.9420],
         [-1.8838, -0.9916, -0.2822,  ..., -1.4464,  0.2964, -0.8647]],

        [[-2.1274, -1.2812, -0.1663,  ..., -1.5160, -0.0430, -1.1585],
         [-1.7606, -1.4788, -0.3711,  ..., -1.5204,  0.1917, -1.2425],
         [-1.4591, -1.3901, -0.2514,  ..., -1.8972, -0.2107, -0.9891],
         [-2.1276, -0.9220, -0.3023,  ..., -1.6004, -0.2384, -0.4088]],

        [[-1.5916, -1.3735, -0.2538,  ..., -1.4159,  0.1083, -0.9985],
         [-2.0671, -1.4894, -0.0223,  ..., -1.6316,  0.3053, -0.7409],
         [-1.9617, -0.7984, -0.1653,  ..., -0.5862,  0.0619, -0.8374],
         [-2.2560, -1.2278, -0.4094,  ..., -1.5170,  0.6478, -0.9059]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([792, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([816, 4, 256]), pos_embed.shape: torch.Size([816, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
encoder start :
	 src.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([816, 4, 256]), k.shape: torch.Size([816, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([816, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([816, 4, 256])
	 (after FFN) src2.shape: torch.Size([816, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([816, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1372, -1.2921,  0.3192,  ..., -0.5834,  0.8625, -1.5817],
         [-1.9693, -1.4791, -0.0697,  ..., -0.7064,  0.9770, -1.6292],
         [-1.9547, -1.2934, -0.0134,  ..., -1.0513,  1.2770, -1.6252],
         [-0.7208, -1.2966, -0.1116,  ..., -0.9103,  1.0007, -1.4616]],

        [[-2.0314, -1.0765,  0.0703,  ..., -0.9075,  0.3323, -1.5457],
         [-2.1544, -1.5208, -0.2105,  ..., -0.8535,  1.0826, -1.3747],
         [-2.1880, -1.0563,  0.1916,  ..., -0.3145,  1.1283, -1.6934],
         [-2.4527, -1.0966, -0.0549,  ..., -1.0124,  1.3761, -0.8929]],

        [[-1.9473, -0.4607, -0.2192,  ..., -0.8604,  0.8763, -1.4892],
         [-2.0086, -1.1635, -0.2582,  ..., -0.8205,  0.9252, -1.1983],
         [-2.6385, -1.2351, -0.0311,  ..., -0.5027,  1.2342, -1.3832],
         [-2.4593, -1.4579,  0.3301,  ..., -0.7089,  0.5464, -1.1439]],

        ...,

        [[-2.1897, -1.1503, -0.0078,  ..., -0.6675,  1.3345, -1.6378],
         [-2.2460, -1.3650, -0.2869,  ..., -0.5455,  1.2903, -1.1937],
         [-2.3373, -1.3239, -0.2744,  ..., -0.6727,  1.0007, -1.5651],
         [-0.7306, -1.5033, -0.4070,  ..., -1.0719,  1.4555, -1.7444]],

        [[-2.3654, -1.3054,  0.3377,  ..., -1.0145,  1.0091, -0.6599],
         [-1.6021, -1.4189, -0.0695,  ..., -0.7436,  1.0012, -1.4784],
         [-1.9686, -1.3262, -0.0753,  ..., -0.9683,  1.1654, -0.9173],
         [-2.2938, -1.4266,  0.0153,  ..., -0.4892,  1.2637, -1.2794]],

        [[-2.2603, -0.4815,  0.1124,  ..., -0.9153,  0.9323, -1.1916],
         [-2.3282, -1.3436, -0.0199,  ..., -0.7572,  1.0314, -0.2122],
         [-2.1721, -0.4546, -0.1328,  ..., -0.5952,  1.2158, -1.5508],
         [-2.3743, -1.1693, -0.0046,  ..., -0.7471,  1.1621, -0.5545]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3055, -1.0193,  0.5430,  ..., -0.2633,  0.6813, -0.3066],
         [-1.4025, -0.7243,  0.6566,  ..., -0.0325,  0.1520, -0.3970],
         [-1.6252, -0.6779,  0.5946,  ..., -0.3800,  1.0608, -0.3241],
         [-0.9828, -0.4584,  0.5404,  ..., -0.4739,  0.1451, -0.4315]],

        [[-1.7114, -0.4095,  0.6521,  ..., -0.4172,  0.4048, -0.2555],
         [-1.7103, -1.0610,  0.2704,  ..., -0.3450,  0.7411, -0.4809],
         [-1.9271, -0.6230,  0.3449,  ...,  0.2366,  0.5411, -0.1015],
         [-1.6831, -0.7593,  0.4218,  ..., -0.5848,  0.7912, -0.0260]],

        [[-1.7829, -0.1047,  0.4091,  ..., -0.3414,  0.6895, -0.2350],
         [-1.7342, -0.5551,  0.1732,  ...,  0.2930,  0.6243, -0.2557],
         [-1.7277, -0.6296,  0.4514,  ..., -0.1960,  0.4319, -0.3821],
         [-2.0052, -0.7859,  0.3490,  ..., -0.3111,  0.7003, -0.5094]],

        ...,

        [[-1.6990, -0.5011,  0.2323,  ..., -0.4511,  0.6080, -0.3879],
         [-1.6531, -0.3713,  0.3781,  ..., -0.1070,  0.7586, -0.0462],
         [-1.0935, -0.5546,  0.3351,  ..., -0.3939,  0.5758, -0.3705],
         [-1.1574, -0.8308,  0.2802,  ..., -0.3577,  0.6043, -0.5137]],

        [[-1.7877, -1.2587,  0.5756,  ..., -0.3954,  0.5183,  0.2177],
         [-1.4343, -0.5602,  0.4152,  ..., -0.3607,  0.5915, -0.6997],
         [-1.5769, -0.8861,  0.8347,  ..., -0.4555,  0.2185,  0.1342],
         [-1.6809, -0.7786,  0.5874,  ...,  0.2738,  0.2752, -0.1759]],

        [[-1.5744, -0.3712,  0.7640,  ..., -0.5784,  0.5103, -0.3418],
         [-1.8436, -0.7156,  0.3577,  ..., -0.2313,  0.6326,  0.3488],
         [-1.7572, -0.3087,  0.7381,  ..., -0.1816,  0.7105, -0.0667],
         [-1.5510, -0.7950,  0.3876,  ..., -0.3196,  0.8159,  0.0663]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4778, -1.9107,  0.4305,  ...,  0.0884,  0.8713, -1.9802],
         [-1.4345, -2.1114,  0.2960,  ..., -0.4532,  0.6768, -2.2449],
         [-1.4807, -1.4407,  0.1075,  ..., -0.4333,  0.9291, -1.9801],
         [-1.5042, -1.2111,  0.2256,  ..., -0.8944,  0.8108, -1.9430]],

        [[-1.5434, -2.0029,  0.2716,  ..., -0.8271,  0.6653, -2.2855],
         [-1.1420, -2.1824,  0.3439,  ..., -0.4001,  0.7303, -2.0046],
         [-1.9131, -2.1279,  0.4625,  ..., -0.1940,  0.9663, -0.9995],
         [-0.9324, -1.2948,  0.7593,  ..., -0.5879,  1.2661, -1.1044]],

        [[-1.4816, -1.7228,  0.5429,  ...,  0.2621,  1.0439, -2.0207],
         [-1.7188, -2.3004,  0.1281,  ..., -0.3327,  0.5749, -1.7670],
         [-1.4066, -1.8579,  0.3836,  ..., -0.6650,  0.9402, -2.1065],
         [-1.6397, -1.5988,  0.1433,  ..., -0.3074,  0.8878, -1.7412]],

        ...,

        [[-1.6857, -2.1557, -0.0739,  ..., -0.2939,  0.8550, -1.5332],
         [-1.7655, -1.0786,  0.3360,  ..., -0.5655,  0.8206, -1.4343],
         [-1.4576, -2.1846, -0.4193,  ..., -0.4545,  0.5547, -2.0655],
         [-1.5509, -2.0225, -0.5062,  ..., -0.2762,  0.5509, -1.8618]],

        [[-1.5131, -1.6576, -0.2149,  ..., -0.7718,  0.3521, -1.3260],
         [-1.6430, -2.0926,  0.3300,  ..., -0.6865,  0.8346, -2.3671],
         [-1.1887, -1.2478,  0.5100,  ..., -0.9593,  0.9184, -1.0085],
         [-1.3165, -2.1276,  0.0618,  ..., -0.1711,  0.6100, -1.2691]],

        [[-1.5300, -2.2035,  0.9435,  ..., -0.4103,  0.8254, -1.9767],
         [-1.6441, -2.1336,  0.1162,  ..., -0.5268,  1.0213, -1.4803],
         [-0.8148, -0.9797,  0.2151,  ..., -0.4649, -0.1846, -2.0374],
         [-1.2951, -2.3002,  0.3695,  ..., -0.1606,  1.1247, -1.7053]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.6113, -2.8177,  0.3758,  ..., -0.9160,  1.1372, -0.3295],
         [-0.7316, -2.0479,  0.3591,  ..., -1.0322,  0.9113, -0.4930],
         [-0.7776, -2.0427,  0.1177,  ..., -1.2610,  0.4698,  0.0631],
         [-1.8577, -2.1444,  0.2056,  ..., -1.2837,  1.2264, -0.7996]],

        [[-1.6256, -2.5288,  0.3587,  ..., -1.0739,  1.1919, -0.3381],
         [-1.4080, -2.6141,  0.2187,  ..., -0.4388,  1.0091,  0.0248],
         [-1.9260, -2.9591,  0.3413,  ..., -0.9572,  1.2200,  0.3249],
         [-1.0876, -1.7782,  0.3488,  ..., -1.1830,  1.3697,  0.2577]],

        [[-1.6222, -2.6289,  0.1412,  ..., -0.6271,  1.4321, -0.0246],
         [-1.8579, -2.7859, -0.0401,  ..., -1.0930,  0.8230, -0.1530],
         [-1.4280, -2.0344,  0.4018,  ..., -1.5278,  1.1213, -0.6635],
         [-1.6403, -2.5059,  0.0187,  ..., -0.9877,  1.2039,  0.4229]],

        ...,

        [[-0.9821, -2.5533, -0.1626,  ..., -1.2139,  1.1339, -0.1993],
         [-2.0328, -2.3561,  0.1649,  ..., -0.9430,  1.4818, -0.3581],
         [-1.9044, -2.5764, -0.1211,  ..., -1.2323,  1.0324, -0.1785],
         [-1.3726, -2.5954, -0.0232,  ..., -1.1275,  1.0613, -0.1429]],

        [[-1.4377, -2.6350,  0.0433,  ..., -1.3119,  1.0590, -0.3025],
         [-1.5233, -2.6315,  0.0690,  ..., -1.2710,  0.4445, -0.1966],
         [-1.4811, -2.1685,  0.2949,  ..., -0.3049,  0.2479,  0.0057],
         [-1.4612, -2.6242,  0.1436,  ..., -0.9271,  0.8219,  0.1719]],

        [[-1.5421, -2.3341,  0.5564,  ..., -1.0742,  1.0918, -0.2613],
         [-1.4675, -3.0773,  0.1627,  ..., -1.3278,  0.4756,  0.0426],
         [-0.5197, -1.9752,  0.1438,  ..., -1.1556, -0.0565, -0.1279],
         [-1.6585, -2.9592,  0.4518,  ..., -0.9967,  1.4055,  0.1469]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9503, -1.2916, -0.4791,  ..., -1.5299,  0.2360, -0.3668],
         [-1.6854, -0.7942, -0.1247,  ..., -1.6870, -0.1624, -1.1252],
         [-1.4362, -0.9111,  0.1181,  ..., -1.5824, -0.1732, -0.1875],
         [-2.3843, -0.8408, -0.1928,  ..., -1.3870,  0.1706, -1.2895]],

        [[-2.0961, -1.5397, -0.4278,  ..., -1.6152, -0.1030, -1.3093],
         [-1.8950, -1.3570, -0.3767,  ..., -1.2937,  0.0771, -0.5232],
         [-2.1013, -1.3142, -0.4349,  ..., -0.7727,  0.1401, -0.6663],
         [-1.8534, -0.7219, -0.4244,  ..., -1.4764,  0.4464, -0.8771]],

        [[-2.0555, -1.0643, -0.0229,  ..., -1.4487,  0.2044, -1.0620],
         [-2.1461, -1.3357, -0.4512,  ..., -1.7700,  0.0771, -1.0648],
         [-1.6739, -0.9565, -0.1462,  ..., -1.7202,  0.1440, -1.1332],
         [-1.7336, -0.9986, -0.6035,  ..., -1.4906, -0.0087, -0.8146]],

        ...,

        [[-1.7572, -1.1041, -0.8138,  ..., -1.6424,  0.3874, -1.0057],
         [-2.3546, -0.8562, -0.1825,  ..., -1.6084,  0.7779, -0.3902],
         [-1.4965, -0.9264, -0.5254,  ..., -1.6535,  0.1458, -1.0079],
         [-2.0821, -1.1917, -0.3089,  ..., -1.1266, -0.0449, -1.0706]],

        [[-1.8280, -0.9535, -0.4614,  ..., -1.6575,  0.0654, -1.0328],
         [-1.6679, -1.2379, -0.1783,  ..., -0.6977, -0.1871, -1.0653],
         [-1.3160, -1.1406, -0.3920,  ..., -1.3239, -0.4252, -0.6893],
         [-1.7314, -1.0873, -0.5350,  ..., -1.8347,  0.2157, -0.6373]],

        [[-1.9570, -0.9448, -0.2529,  ..., -1.7825, -0.0475, -1.1348],
         [-1.8751, -1.3099, -0.4634,  ..., -1.8246,  0.1109, -0.7448],
         [-1.5623, -0.8545, -0.4924,  ..., -1.6395, -0.3899, -0.9834],
         [-1.8487, -1.2104, -0.1930,  ..., -1.3594,  0.5767, -0.4350]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([816, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
Epoch: [0]  [   20/29571]  eta: 1:41:45  lr: 0.000100  class_error: 100.00  loss: 53.9123 (54.7976)  loss_ce: 1.9982 (2.2479)  loss_bbox: 4.5199 (4.5461)  loss_giou: 2.3019 (2.3109)  loss_ce_0: 1.9524 (2.2604)  loss_bbox_0: 4.5714 (4.5943)  loss_giou_0: 2.2737 (2.3026)  loss_ce_1: 1.9369 (2.2117)  loss_bbox_1: 4.6005 (4.6143)  loss_giou_1: 2.2689 (2.2932)  loss_ce_2: 1.9730 (2.2425)  loss_bbox_2: 4.5555 (4.5844)  loss_giou_2: 2.2801 (2.2985)  loss_ce_3: 1.9633 (2.2902)  loss_bbox_3: 4.4557 (4.5594)  loss_giou_3: 2.2938 (2.3068)  loss_ce_4: 2.0172 (2.2809)  loss_bbox_4: 4.4769 (4.5448)  loss_giou_4: 2.3127 (2.3088)  loss_ce_unscaled: 1.9982 (2.2479)  class_error_unscaled: 100.0000 (99.1962)  loss_bbox_unscaled: 0.9040 (0.9092)  loss_giou_unscaled: 1.1510 (1.1554)  cardinality_error_unscaled: 5.0000 (10.3095)  loss_ce_0_unscaled: 1.9524 (2.2604)  loss_bbox_0_unscaled: 0.9143 (0.9189)  loss_giou_0_unscaled: 1.1369 (1.1513)  cardinality_error_0_unscaled: 5.0000 (10.1667)  loss_ce_1_unscaled: 1.9369 (2.2117)  loss_bbox_1_unscaled: 0.9201 (0.9229)  loss_giou_1_unscaled: 1.1344 (1.1466)  cardinality_error_1_unscaled: 5.0000 (10.5119)  loss_ce_2_unscaled: 1.9730 (2.2425)  loss_bbox_2_unscaled: 0.9111 (0.9169)  loss_giou_2_unscaled: 1.1401 (1.1492)  cardinality_error_2_unscaled: 5.0000 (11.8214)  loss_ce_3_unscaled: 1.9633 (2.2902)  loss_bbox_3_unscaled: 0.8911 (0.9119)  loss_giou_3_unscaled: 1.1469 (1.1534)  cardinality_error_3_unscaled: 5.0000 (10.1310)  loss_ce_4_unscaled: 2.0172 (2.2809)  loss_bbox_4_unscaled: 0.8954 (0.9090)  loss_giou_4_unscaled: 1.1564 (1.1544)  cardinality_error_4_unscaled: 5.0000 (10.3810)  time: 0.1523  data: 0.0088  max mem: 7055
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([864, 4, 256]), pos_embed.shape: torch.Size([864, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
encoder start :
	 src.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([864, 4, 256]), k.shape: torch.Size([864, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([864, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([864, 4, 256])
	 (after FFN) src2.shape: torch.Size([864, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([864, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.5314, -1.4639, -0.1562,  ..., -0.7515,  1.0105, -1.5551],
         [-2.2007, -1.3059,  0.0414,  ..., -0.1550,  0.9177, -1.5536],
         [-2.4059, -1.3517, -0.0065,  ..., -0.7405,  1.0192, -1.3622],
         [-2.1175, -1.6442, -0.1583,  ..., -1.0334,  0.9252, -1.6233]],

        [[-2.3011, -1.3747, -0.0963,  ..., -0.9434,  0.5074, -0.6109],
         [-0.9152, -1.3865,  0.1217,  ..., -1.0931,  1.3845, -1.2613],
         [-1.9785, -1.0624, -0.2655,  ..., -0.5900,  1.2008, -0.9414],
         [-0.9909, -1.4341, -0.2020,  ..., -0.2757,  0.9167, -1.3959]],

        [[-2.0667, -1.5132,  0.2726,  ..., -0.1239,  1.0728, -1.5614],
         [-2.4012, -1.5039,  0.3519,  ..., -0.3964,  1.0777, -1.4519],
         [-2.4363, -1.2346, -0.0070,  ..., -0.7204,  1.0559, -1.5711],
         [-2.2161, -1.5233,  0.0857,  ..., -0.8264,  0.9845, -1.5161]],

        ...,

        [[-0.6138, -1.4502, -0.1274,  ..., -0.6212,  1.0081, -0.9029],
         [-1.9567, -1.2353, -0.2453,  ..., -1.0019,  1.3720, -1.6828],
         [-2.4670, -1.6515, -0.0150,  ..., -0.9973,  1.3836, -0.9451],
         [-1.6915, -1.3421,  0.1536,  ..., -0.7387,  1.1760, -1.5082]],

        [[-2.1279, -1.3574,  0.0635,  ..., -1.2085,  0.9511, -1.4839],
         [-0.7279, -1.3736,  0.3175,  ..., -0.8771,  0.9727, -1.4332],
         [-0.9970, -1.3765,  0.0444,  ..., -0.8703,  0.2372, -1.3538],
         [-1.7412, -1.4211,  0.2736,  ..., -0.7274,  1.2202, -1.2751]],

        [[-2.2735, -1.1788, -0.1335,  ..., -0.7554,  0.7175, -1.6873],
         [-1.8942, -1.7560, -0.0374,  ..., -0.7575,  1.0935, -1.2721],
         [-1.6991, -0.1434, -0.1020,  ..., -0.5288,  1.1003, -1.6329],
         [-2.1912, -1.3075,  0.0903,  ..., -1.0840,  0.9312, -0.6469]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3400, -0.9552,  0.7036,  ..., -0.2956,  0.5889, -0.1315],
         [-1.5830, -0.9684,  0.4315,  ..., -0.2404,  0.4183, -0.3899],
         [-1.7317, -0.3664,  0.1809,  ..., -0.6886,  0.6230, -0.5885],
         [-1.6393, -0.9628,  0.3366,  ..., -0.4723,  0.7393, -0.2650]],

        [[-1.9855, -0.5263,  0.5956,  ..., -0.6451,  0.3075,  0.3069],
         [-1.4630, -0.9079,  0.4106,  ..., -0.2224,  0.9254, -0.1086],
         [-1.8530, -0.8150,  0.7979,  ...,  0.1655,  0.5873, -0.0546],
         [-1.1143, -0.8858,  0.4687,  ..., -0.1164,  0.4038, -0.2694]],

        [[-1.5497, -0.6560,  0.6377,  ...,  0.2096,  0.5030, -0.1769],
         [-1.8103, -0.7331,  0.5658,  ..., -0.2483,  0.3055, -0.1205],
         [-1.5691, -0.9663, -0.0623,  ..., -0.4666,  0.8922, -0.4127],
         [-1.9431, -0.6028,  0.7013,  ..., -0.5789,  0.5926, -0.2545]],

        ...,

        [[-1.0089, -0.6254,  0.2273,  ..., -0.1015,  0.5545,  0.1392],
         [-1.5148, -0.7900,  0.7817,  ..., -0.4455,  0.5698, -0.3389],
         [-1.6431, -0.5276,  0.8290,  ..., -0.3060,  0.6305, -0.5033],
         [-1.6872, -0.9307,  0.6412,  ..., -0.2227,  0.5467, -0.2990]],

        [[-1.5172, -0.9597,  0.4671,  ..., -0.4745,  0.6006, -0.4858],
         [-1.0867, -1.3398,  0.4083,  ..., -0.4055,  0.4875, -0.3954],
         [-1.1799, -0.5467,  0.5042,  ..., -0.3336,  0.0645, -0.1619],
         [-1.7136, -0.5836,  0.7825,  ..., -0.3848,  0.6420, -0.0868]],

        [[-1.6400, -0.5122,  0.3003,  ..., -0.3807,  0.7236, -0.2820],
         [-0.8702, -1.1833,  0.0807,  ..., -0.3266,  0.5361, -0.2496],
         [-1.3593, -0.6425,  0.2847,  ..., -0.3042,  0.6670, -0.5123],
         [-1.6587, -1.3568,  0.8113,  ..., -0.2320,  0.5547,  0.0657]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3178, -2.1666,  0.8113,  ..., -0.4511,  0.2852, -1.9760],
         [-1.8144, -2.1109,  0.2967,  ..., -0.2812,  0.7272, -1.8415],
         [-1.7088, -1.9358,  0.3099,  ..., -0.5865,  0.0552, -1.8801],
         [-1.6228, -0.4342,  0.1817,  ..., -0.6804,  0.9556, -1.8671]],

        [[-1.6978, -2.1320,  0.2826,  ..., -0.7248,  0.9026, -1.8395],
         [-1.8175, -2.2088,  0.6155,  ..., -0.6883,  1.0203, -1.9743],
         [-1.9722, -2.1583,  0.2474,  ..., -0.2509,  1.0101, -0.8433],
         [-1.2084, -1.9310,  0.2252,  ..., -0.8125,  0.8636, -1.6476]],

        [[-1.9236, -2.4106,  0.6282,  ..., -0.0819,  0.9895, -1.8618],
         [-2.0843, -2.3750,  0.2193,  ..., -0.3966,  0.9373, -1.9841],
         [-1.8233, -2.2810,  0.0351,  ..., -0.2777,  0.7879, -1.8811],
         [-1.8088, -2.0089, -0.3971,  ..., -1.2018,  1.0377, -1.9184]],

        ...,

        [[-1.5984, -1.9530,  0.1992,  ..., -0.4968,  0.9114, -1.9084],
         [-1.4953, -2.1675,  0.0942,  ..., -0.5728,  0.8479, -1.8383],
         [-1.7543, -2.1196,  0.4287,  ..., -0.6253,  0.8369, -1.9775],
         [-1.7334, -2.0930,  0.2394,  ..., -0.4415,  0.1970, -1.8642]],

        [[-1.6512, -1.1524,  0.4573,  ..., -0.6444,  1.1198, -1.8913],
         [-1.3254, -2.2128,  0.4929,  ..., -0.3931,  0.7732, -0.6313],
         [-1.3787, -1.8283,  0.1023,  ..., -0.4657,  0.4304, -2.0174],
         [-1.8909, -2.2489,  0.1763,  ..., -0.2663,  0.8031, -1.5951]],

        [[-1.6709, -1.8889,  0.3932,  ..., -0.4769,  1.3647, -2.1422],
         [-1.1362, -2.3430,  0.0955,  ..., -0.1790,  0.7817, -1.8503],
         [-1.3663, -2.2021,  0.2071,  ..., -0.2975,  0.6778, -1.6673],
         [-1.4972, -2.1177, -0.1083,  ..., -0.4587,  0.5736, -1.8011]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7698e+00, -2.1314e+00,  6.2528e-01,  ..., -1.2035e+00,
           1.0350e+00,  2.8679e-02],
         [-1.2427e+00, -2.7781e+00,  2.4261e-01,  ..., -1.2299e+00,
           9.4771e-01, -2.6074e-01],
         [-1.6518e+00, -1.6981e+00,  1.4181e-01,  ..., -1.1127e+00,
           2.1238e-01, -6.7072e-02],
         [-1.6932e+00, -2.1738e+00,  1.7105e-01,  ..., -1.1323e+00,
           1.3728e+00, -3.3570e-01]],

        [[-2.0057e+00, -2.2225e+00,  7.8122e-02,  ..., -1.1468e+00,
           1.1300e+00, -4.8667e-01],
         [-1.5108e+00, -2.6795e+00,  4.1127e-01,  ..., -4.9707e-01,
           1.3287e+00, -3.9034e-03],
         [-1.6538e+00, -2.9619e+00,  4.6488e-01,  ..., -1.0196e+00,
           9.7981e-01,  5.7252e-01],
         [-6.1875e-01, -2.4058e+00,  1.8215e-01,  ..., -7.5898e-01,
           4.7026e-01,  2.4044e-01]],

        [[-1.7553e+00, -1.8407e+00,  5.5185e-01,  ..., -9.9626e-01,
           6.9437e-01,  1.6615e-01],
         [-1.7161e+00, -2.2627e+00,  2.1040e-01,  ..., -1.2480e+00,
           1.5805e+00, -4.1930e-02],
         [-1.5766e+00, -2.3596e+00,  3.9528e-02,  ..., -8.3741e-01,
           1.1986e+00, -5.7587e-03],
         [-1.7866e+00, -2.2404e+00,  1.4827e-01,  ..., -1.2367e+00,
           1.1947e+00,  2.1007e-03]],

        ...,

        [[-1.5201e+00, -1.7607e+00,  9.4007e-02,  ..., -1.1214e+00,
           1.0435e+00, -4.7048e-01],
         [-1.2064e+00, -2.6168e+00,  8.6324e-02,  ..., -1.7510e+00,
           9.2160e-01,  8.9773e-02],
         [-8.0405e-01, -2.4525e+00, -8.7523e-02,  ..., -1.3190e+00,
           1.0655e+00, -2.7087e-02],
         [-1.8599e+00, -2.4889e+00, -2.0760e-01,  ..., -4.1320e-01,
           9.4036e-01, -9.1084e-02]],

        [[-1.5888e+00, -1.3598e+00,  3.2321e-01,  ..., -8.7045e-01,
           1.4409e+00, -2.8541e-01],
         [-7.3318e-01, -2.7787e+00,  1.6846e-01,  ..., -1.0778e+00,
           2.3578e-01,  1.8634e-01],
         [-1.7253e+00, -1.6770e+00,  7.4994e-02,  ..., -9.4002e-01,
           7.7625e-01,  1.1387e-01],
         [-1.6009e+00, -2.9669e+00,  1.4802e-01,  ..., -1.0699e+00,
           1.4775e+00,  2.2167e-01]],

        [[-1.7688e+00, -2.3551e+00,  3.6321e-01,  ..., -1.1141e+00,
           6.2440e-01, -1.6315e-01],
         [-1.6567e+00, -2.8207e+00, -8.9443e-02,  ..., -1.1872e+00,
           1.0612e+00, -2.0955e-01],
         [-1.4531e+00, -2.8207e+00,  2.9364e-01,  ..., -3.5077e-01,
           2.1124e-01,  4.5286e-01],
         [-1.5012e+00, -2.5308e+00,  1.5569e-01,  ..., -1.2387e+00,
           1.2423e+00, -3.6491e-03]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1497, -1.0006,  0.4433,  ..., -1.6446,  0.2963, -1.0718],
         [-1.8807, -1.2499, -0.3996,  ..., -1.7086,  0.1219, -1.5585],
         [-2.0825, -0.7430, -0.4446,  ..., -1.2672, -0.3861, -1.2661],
         [-2.1998, -0.9825, -0.0188,  ..., -1.5756,  0.4633, -0.3768]],

        [[-2.1245, -1.1113, -0.2562,  ..., -1.6244,  0.0498, -1.1761],
         [-2.1041, -1.2418, -0.2356,  ..., -1.0482,  0.0170, -0.5076],
         [-2.0047, -1.2786, -0.3847,  ..., -1.3922,  0.2955, -0.8373],
         [-1.4116, -0.9235, -0.1228,  ..., -1.3397, -0.4628, -0.9326]],

        [[-2.0036, -0.8271,  0.4276,  ..., -1.1989, -0.2208, -0.7894],
         [-2.1580, -1.0189,  0.3297,  ..., -1.5223,  0.1315, -1.2793],
         [-2.0159, -1.2656, -0.1722,  ..., -1.7120,  0.1183, -1.0075],
         [-2.3465, -1.2198, -0.2122,  ..., -1.8009,  0.3129, -0.9589]],

        ...,

        [[-1.8390, -0.9052, -0.4148,  ..., -1.9156,  0.7119, -0.5362],
         [-1.9854, -1.0729, -0.5708,  ..., -1.7835,  0.1896, -1.2120],
         [-1.7547, -1.0437, -0.5155,  ..., -1.8117,  0.4213, -0.9789],
         [-2.0953, -0.9730, -0.3945,  ..., -1.2253,  0.0464, -0.9960]],

        [[-2.3800, -0.5894, -0.5917,  ..., -1.1567,  0.1613, -0.5575],
         [-1.7769, -1.3606,  0.2857,  ..., -1.7978, -0.3219, -0.8681],
         [-1.8907, -1.0058, -0.4312,  ..., -1.3431, -0.1662, -0.2572],
         [-2.3162, -1.1762, -0.6073,  ..., -1.2766,  0.3940, -1.1220]],

        [[-2.3181, -1.1257, -0.2610,  ..., -0.9622, -0.0118, -0.6639],
         [-2.0191, -1.2636, -0.6941,  ..., -0.8802,  0.0261, -1.1149],
         [-1.2117, -1.1202, -0.3096,  ..., -1.2684,  0.1276, -1.0443],
         [-1.9638, -1.5144, -0.3731,  ..., -1.6511,  0.2891, -0.3155]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([864, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([675, 4, 256]), pos_embed.shape: torch.Size([675, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
encoder start :
	 src.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([675, 4, 256]), k.shape: torch.Size([675, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([675, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([675, 4, 256])
	 (after FFN) src2.shape: torch.Size([675, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([675, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.2695e+00, -1.5018e+00, -1.7694e-01,  ..., -7.0175e-01,
           1.2977e+00, -1.5325e+00],
         [-2.2754e+00, -1.3025e+00,  1.1446e-01,  ..., -7.9795e-01,
           1.1255e+00, -1.4885e+00],
         [-2.4194e+00, -1.2132e+00, -1.4730e-02,  ..., -7.2621e-01,
           8.4830e-01, -1.6542e+00],
         [-2.1217e+00, -1.1405e+00, -1.6245e-01,  ..., -8.9732e-01,
           8.0805e-01, -1.3662e+00]],

        [[-2.3820e+00, -1.4027e+00,  8.6497e-03,  ..., -9.5935e-01,
           3.7615e-02, -5.9821e-01],
         [-2.1792e+00, -1.1040e+00, -4.4268e-03,  ..., -6.9737e-01,
           1.2512e+00, -1.7533e+00],
         [-2.0908e+00, -1.4240e+00, -2.4937e-01,  ..., -8.2081e-01,
           8.9291e-01, -1.6377e+00],
         [-2.1055e+00, -1.4303e+00,  1.2486e-03,  ..., -8.4960e-01,
           1.0317e+00, -1.0317e+00]],

        [[-2.0890e+00, -1.5954e+00, -4.1860e-02,  ..., -8.9786e-01,
           1.0411e+00, -1.2620e+00],
         [-2.2205e+00, -1.4263e+00, -1.8335e-01,  ..., -9.6350e-01,
           1.2881e+00, -5.8491e-01],
         [-2.1778e+00, -1.3636e+00, -1.7288e-01,  ..., -9.0089e-01,
           2.6305e-01, -1.5074e+00],
         [-2.1251e+00, -1.4536e+00,  1.1951e-01,  ..., -8.0430e-01,
           1.2658e+00, -9.5378e-01]],

        ...,

        [[-1.0011e+00, -1.4540e+00,  7.0591e-02,  ..., -7.8902e-01,
           9.9034e-01, -1.4705e+00],
         [-6.1725e-01, -6.0352e-01, -1.8295e-01,  ..., -1.0536e+00,
           1.0678e+00, -1.4576e+00],
         [-7.4780e-01, -1.6213e+00, -1.4585e-01,  ..., -8.1666e-01,
           1.1268e+00, -1.6650e+00],
         [-2.1552e+00, -1.3009e+00,  1.8077e-02,  ..., -8.8300e-01,
           9.8191e-01, -1.3496e+00]],

        [[-1.9075e+00, -1.3436e+00, -1.7761e-01,  ..., -6.2285e-01,
           1.9307e-01, -1.5956e+00],
         [-2.2983e+00, -5.5039e-01,  3.0892e-01,  ..., -6.8186e-01,
           1.1739e+00, -6.1817e-01],
         [-1.7071e+00, -1.6261e+00, -3.0670e-01,  ..., -9.3353e-01,
           8.4640e-01, -1.3839e+00],
         [-2.4179e+00, -1.4352e+00, -1.1509e-01,  ..., -6.1595e-01,
           9.9818e-01, -1.5074e+00]],

        [[-2.2174e+00, -1.3122e+00, -1.3019e-01,  ..., -5.1281e-01,
           6.7388e-01, -1.4423e+00],
         [-2.0356e+00, -1.2558e+00, -5.6208e-02,  ..., -7.5942e-01,
           8.7553e-01, -1.4534e+00],
         [-2.3480e+00, -1.3896e+00, -2.1121e-01,  ..., -8.9310e-01,
           5.9156e-01, -1.6770e+00],
         [-2.1808e+00, -1.5587e+00, -3.7878e-01,  ..., -6.0648e-01,
           1.1296e+00, -1.5581e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1780, -1.0433,  0.2602,  ..., -0.2558,  0.6115, -0.3455],
         [-1.5870, -0.8373,  0.4952,  ..., -0.4441,  0.7418, -0.3153],
         [-1.7695, -0.5286,  0.2134,  ..., -0.1304,  0.4559, -0.5821],
         [-1.9054, -1.0992,  0.4922,  ..., -0.3857,  0.8628, -0.3196]],

        [[-2.0249, -1.0818,  0.5392,  ..., -0.1722,  0.1805,  0.2422],
         [-1.7783, -0.7954,  0.2333,  ..., -0.6320,  0.9517, -0.7215],
         [-1.6990, -0.9809,  0.3923,  ..., -0.4695,  0.6242, -0.3857],
         [-1.8864, -1.0438,  0.4843,  ..., -0.4745,  0.4825, -0.1386]],

        [[-1.0853, -0.6957,  0.2844,  ..., -0.4631,  0.5604,  0.0667],
         [-1.9947, -1.3183,  0.6523,  ..., -0.5983,  0.4854, -0.0268],
         [-1.8012, -0.8652,  0.3841,  ..., -0.4162,  0.2388, -0.1290],
         [-2.0196, -0.8739,  0.8061,  ..., -0.4313,  0.8883, -0.1920]],

        ...,

        [[-1.3853, -0.9725,  0.6342,  ..., -0.3911,  0.4004,  0.0855],
         [-1.3930, -0.7716,  0.6957,  ..., -0.4392,  0.6463, -0.4357],
         [-1.2875, -0.8847,  0.4464,  ..., -0.1410,  0.8238, -0.3362],
         [-1.6442, -1.3266,  0.5456,  ..., -0.2528,  0.6122, -0.5101]],

        [[-1.6834, -1.0811,  0.6419,  ..., -0.4174,  0.2111, -0.2866],
         [-1.7025, -0.6381,  0.9860,  ..., -0.0122,  0.6069, -0.1421],
         [-1.5630, -0.9151,  0.6002,  ..., -0.4496,  0.6149, -0.2629],
         [-1.7346, -0.6212,  0.4280,  ...,  0.0342,  0.4414, -0.2371]],

        [[-1.8963, -0.7557,  0.3832,  ..., -0.1278,  0.6176, -0.4339],
         [-1.4820, -0.9751,  0.3430,  ..., -0.4670,  0.2037, -0.0111],
         [-1.8008, -0.8759,  0.7619,  ..., -0.2685,  0.5475, -0.3986],
         [-1.7108, -0.8523,  0.3286,  ..., -0.0480,  0.5047, -0.0442]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3019, -1.3514,  0.4335,  ..., -0.3641,  0.8617, -1.3569],
         [-1.7481, -2.2833,  0.5746,  ..., -0.4217,  0.0299, -2.1897],
         [-1.1701, -1.9020,  0.4834,  ..., -0.0766,  0.4401, -2.2430],
         [-1.6917, -2.2814,  0.4961,  ...,  0.1578,  1.1818, -1.8638]],

        [[-1.7296, -2.3903,  0.3113,  ...,  0.2132,  0.5855, -1.7552],
         [-1.7441, -1.2224,  0.5017,  ..., -0.0429,  1.1892, -0.5417],
         [-1.0362, -1.6097,  0.3761,  ..., -0.2891,  1.1906, -2.0973],
         [-1.4733, -1.5777,  0.1998,  ..., -0.9957,  0.9006, -1.9616]],

        [[-1.5739, -1.8441, -0.3354,  ..., -0.3527,  0.8302, -1.7358],
         [-1.5589, -2.1788,  0.3801,  ..., -0.4157,  0.9835, -1.9759],
         [-1.7846, -2.3096,  0.2419,  ..., -0.4405,  0.5107, -2.0077],
         [-1.8235, -1.2183,  0.5324,  ..., -0.7182,  0.8265, -1.9897]],

        ...,

        [[-1.5652, -2.0996,  0.3905,  ..., -0.1823,  0.7675, -0.8882],
         [-1.5499, -2.0920,  0.6136,  ..., -0.9125,  0.8378, -1.8599],
         [-1.1523, -2.2248,  0.0809,  ..., -0.3568,  0.6975, -2.0909],
         [-2.0610, -2.1716,  0.6996,  ...,  0.1915,  0.8023, -2.0621]],

        [[-1.9067, -1.3715,  0.4153,  ..., -0.2040,  0.6386, -1.7930],
         [-1.9160, -1.9182,  1.0465,  ..., -0.2939,  0.9537, -1.5014],
         [-1.1069, -2.4122,  0.3643,  ..., -0.3355,  1.2917, -0.8621],
         [-1.5973, -1.9347,  0.5097,  ..., -0.8778,  0.9195, -0.8069]],

        [[-1.7279, -2.1714,  0.4306,  ..., -0.3505,  1.0024, -1.8063],
         [-1.4033, -2.0915,  0.2205,  ..., -0.5128,  0.8951, -1.7350],
         [-1.6469, -1.3333,  0.3227,  ..., -0.2723,  0.7524, -2.2133],
         [-1.4437, -2.2324, -0.0830,  ..., -0.2249,  0.9026, -1.7910]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7405, -2.2588,  0.2879,  ..., -0.7653,  0.9639, -0.2236],
         [-1.9427, -2.3836,  0.5006,  ..., -0.7647,  0.8641, -0.0409],
         [-1.5366, -2.3352,  0.3093,  ..., -1.0039,  1.1254,  0.0486],
         [-1.9473, -2.6155,  0.2391,  ..., -0.8701,  1.3755, -0.0248]],

        [[-1.5910, -1.8799,  0.3585,  ..., -0.6902,  1.0518, -0.5639],
         [-1.9178, -2.3758,  0.1840,  ..., -1.0634,  1.1467,  0.1145],
         [-1.3579, -2.3453,  0.1062,  ..., -0.8040,  1.3881,  0.1954],
         [-1.6650, -1.5103,  0.2344,  ..., -1.3538,  1.0848, -0.0346]],

        [[-1.4457, -2.7448, -0.1104,  ..., -0.3429,  1.0685,  0.3068],
         [-1.5138, -2.3045,  0.1083,  ..., -0.9950,  1.6691, -0.0289],
         [-1.1721, -2.7020,  0.3291,  ..., -0.9989,  0.5313,  0.1805],
         [-1.8410, -1.2848,  0.2766,  ..., -1.1544,  1.3704, -0.5670]],

        ...,

        [[-1.4516, -2.8233,  0.3203,  ..., -0.9808,  1.2985,  0.6530],
         [-1.2534, -2.8822,  0.4489,  ..., -1.2340,  1.5707,  0.2173],
         [-1.3125, -2.7535,  0.1180,  ..., -1.2109,  1.1873, -0.6056],
         [-1.9310, -2.6195,  0.4366,  ..., -0.1395,  1.0848, -0.5830]],

        [[-2.1212, -2.4081,  0.2457,  ..., -0.2927,  1.3420,  0.1190],
         [-1.7266, -2.8473,  0.5035,  ..., -0.3796,  1.1367,  0.1050],
         [-1.2423, -1.5293,  0.3265,  ..., -0.9821,  1.4451,  0.2586],
         [-1.6652, -2.0907,  0.2488,  ..., -0.2343,  1.3450,  0.1725]],

        [[-1.4258, -2.8510,  0.2526,  ..., -1.3334,  1.2119, -0.2637],
         [-1.7216, -2.4509,  0.4078,  ..., -1.0506,  1.1910, -0.1381],
         [-1.8701, -2.2903,  0.2304,  ..., -0.0146,  0.5961, -0.7347],
         [-1.2391, -2.6106, -0.0120,  ..., -0.7790,  1.1447,  0.2219]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.3591e+00, -1.0939e+00,  1.3812e-01,  ..., -1.4771e+00,
          -9.0483e-02, -8.7629e-01],
         [-2.2427e+00, -9.3342e-01,  5.3220e-01,  ..., -1.2921e+00,
           2.8206e-01, -1.2087e+00],
         [-2.2273e+00, -1.0231e+00, -5.2771e-01,  ..., -1.4924e+00,
           2.2704e-01, -9.1265e-01],
         [-2.2404e+00, -1.3047e+00, -7.8284e-01,  ..., -1.3684e+00,
           2.9435e-01, -9.3427e-01]],

        [[-2.2122e+00, -1.1062e+00, -1.1049e-01,  ..., -1.2532e+00,
          -3.3888e-04, -1.1571e+00],
         [-2.3495e+00, -1.1211e+00, -4.4994e-01,  ..., -1.4626e+00,
           6.1761e-02, -8.7028e-01],
         [-1.5849e+00, -1.1383e+00, -4.1264e-01,  ..., -1.1637e+00,
          -3.4498e-01, -3.4928e-01],
         [-2.3508e+00, -8.6816e-01, -2.7578e-01,  ..., -1.1528e+00,
           9.7389e-03, -1.2028e+00]],

        [[-1.7341e+00, -1.1601e+00, -2.5210e-01,  ..., -1.2003e+00,
           3.5031e-01, -1.5207e-01],
         [-2.1486e+00, -1.1964e+00, -5.8756e-01,  ..., -1.6229e+00,
           6.1036e-01, -1.1906e+00],
         [-2.1537e+00, -1.1619e+00, -2.0243e-01,  ..., -1.3560e+00,
          -8.6940e-02, -8.7691e-01],
         [-1.7494e+00, -6.3170e-01, -2.8286e-01,  ..., -1.4556e+00,
           2.0399e-01, -4.8627e-01]],

        ...,

        [[-1.2362e+00, -1.1618e+00, -7.0683e-01,  ..., -1.4560e+00,
           3.1167e-01, -2.5399e-02],
         [-1.9856e+00, -1.0946e+00, -9.6509e-02,  ..., -1.4921e+00,
           3.6401e-01, -1.0936e+00],
         [-2.1931e+00, -1.1675e+00, -3.0800e-01,  ..., -1.5107e+00,
           1.6844e-01, -1.2686e+00],
         [-1.1013e+00, -1.0635e+00,  6.0107e-03,  ..., -1.2120e+00,
          -1.5652e-02, -1.3298e+00]],

        [[-2.0928e+00, -9.5530e-01, -4.9392e-01,  ..., -1.3097e+00,
          -4.9527e-02, -1.2125e+00],
         [-1.6526e+00, -1.1704e+00, -4.6211e-02,  ..., -1.3720e+00,
           1.2791e-01, -7.6151e-01],
         [-1.1327e+00, -5.9719e-01, -4.2311e-01,  ..., -1.4608e+00,
           4.0245e-02, -9.5741e-01],
         [-1.9494e+00, -9.8502e-01, -5.4797e-02,  ..., -9.4583e-01,
           2.9793e-01, -1.0695e+00]],

        [[-1.6452e+00, -1.3818e+00,  4.3420e-01,  ..., -1.7484e+00,
           5.9113e-03, -1.2149e+00],
         [-1.7927e+00, -1.2951e+00, -4.9405e-01,  ..., -1.6064e+00,
           3.4529e-01, -6.4540e-01],
         [-2.3520e+00, -1.1943e+00, -2.8967e-01,  ..., -1.5251e-01,
          -2.9918e-01, -1.0955e+00],
         [-1.6688e+00, -1.1873e+00, -8.2964e-01,  ..., -1.4993e+00,
           5.3160e-02, -7.5168e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([675, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([840, 4, 256]), pos_embed.shape: torch.Size([840, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
encoder start :
	 src.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([840, 4, 256]), k.shape: torch.Size([840, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([840, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([840, 4, 256])
	 (after FFN) src2.shape: torch.Size([840, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([840, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9818e+00, -1.3742e+00,  2.4318e-01,  ..., -7.5896e-01,
           8.9965e-01, -8.3945e-01],
         [-2.3822e+00, -1.2817e+00, -2.3134e-02,  ..., -6.7334e-01,
           3.4191e-01, -1.4072e+00],
         [-1.8171e+00, -1.7438e+00,  9.4178e-02,  ..., -7.2208e-01,
           1.1811e-01, -1.0130e+00],
         [-2.1142e+00, -1.4203e+00,  1.2844e-01,  ..., -7.0028e-01,
           9.0324e-01, -1.6779e+00]],

        [[-2.2264e+00, -1.5456e+00,  1.1052e-01,  ..., -4.5473e-01,
           7.6429e-01, -1.9094e+00],
         [-2.3144e+00, -1.2767e+00,  3.0954e-01,  ..., -6.9077e-01,
           1.0262e+00, -1.6874e+00],
         [-2.1015e+00, -5.8705e-01, -2.1277e-01,  ..., -9.9887e-01,
           7.0069e-01, -1.6367e+00],
         [-4.6657e-01, -1.6055e+00,  5.5018e-02,  ..., -1.0167e+00,
           8.4348e-01, -1.4844e+00]],

        [[-2.1037e+00, -1.0828e+00,  2.5020e-01,  ..., -5.7790e-01,
           7.2600e-01, -1.7617e+00],
         [-2.4830e+00, -5.4811e-01,  3.0803e-01,  ..., -9.1695e-01,
           8.8204e-01, -1.2911e+00],
         [-1.9664e+00, -6.1741e-01,  3.3922e-01,  ..., -8.0720e-01,
           8.8455e-01, -1.7021e+00],
         [-2.0531e+00, -1.5460e+00,  5.3989e-01,  ..., -7.4727e-01,
           8.5462e-01, -1.7771e+00]],

        ...,

        [[-2.1269e+00, -1.4396e+00, -5.2429e-02,  ..., -8.0620e-01,
           8.0466e-01, -1.5757e+00],
         [-2.2333e+00, -1.4581e+00,  2.0982e-01,  ..., -6.8221e-01,
           1.1451e+00, -8.0361e-01],
         [-9.3063e-01, -1.3890e+00, -2.2837e-02,  ..., -7.0225e-01,
           1.0068e+00, -1.0066e+00],
         [-2.1915e+00, -6.3707e-01,  1.8111e-03,  ..., -7.7340e-01,
           1.1105e+00, -1.6657e+00]],

        [[-2.4008e+00, -1.2016e+00,  2.4705e-01,  ..., -7.2050e-01,
           8.1413e-01, -1.7654e+00],
         [-2.0185e+00, -1.5877e+00,  3.0167e-01,  ..., -7.6144e-01,
           1.2452e+00, -1.5172e+00],
         [-1.9531e+00, -1.3206e+00,  1.6514e-01,  ..., -5.6533e-01,
           1.2699e+00, -1.6971e+00],
         [-2.1316e+00, -1.1919e+00, -3.3038e-01,  ..., -8.6869e-01,
           1.2179e+00, -1.5179e+00]],

        [[-7.4957e-01, -1.7142e+00,  3.7364e-01,  ..., -8.1675e-01,
           8.6327e-01, -1.5279e+00],
         [-5.2345e-01, -1.3974e+00, -2.4096e-01,  ..., -9.0926e-01,
           1.0993e+00, -8.4148e-01],
         [-2.0608e+00, -1.4232e+00,  3.8840e-03,  ..., -2.1916e-02,
           4.5592e-01, -8.1909e-01],
         [-2.1315e+00, -1.5768e+00,  4.4759e-02,  ..., -8.6569e-01,
           1.2137e+00, -1.4956e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8904, -0.6215,  0.6853,  ..., -0.1078,  0.9183, -0.2690],
         [-1.6896, -1.2261,  0.2037,  ..., -0.5381,  0.3533, -0.3802],
         [-0.9399, -1.1606,  0.6443,  ..., -0.4418,  0.2958, -0.5479],
         [-1.7749, -0.8285,  0.7234,  ..., -0.4473,  0.2480, -0.2205]],

        [[-1.8099, -0.7115,  0.4102,  ..., -0.1469,  0.6140, -0.5642],
         [-1.5453, -0.6249,  0.5731,  ..., -0.2871,  0.9754, -0.3124],
         [-1.8082, -0.3458,  0.6244,  ...,  0.4650,  0.6020, -0.5287],
         [-1.0009, -0.6917,  0.4003,  ..., -0.0906,  0.5888, -0.0611]],

        [[-1.6567, -0.6959,  0.4261,  ...,  0.1984,  0.6987, -0.4063],
         [-1.4970, -0.2762,  0.5832,  ..., -0.3992,  0.5540, -0.3145],
         [-1.7545, -0.8412,  0.6357,  ..., -0.1409,  0.4925, -0.4056],
         [-1.9688, -0.4034,  0.8674,  ..., -0.4417,  0.5135, -0.3467]],

        ...,

        [[-1.9068, -1.1520,  0.2101,  ..., -0.2125,  0.4669, -0.4379],
         [-1.7659, -0.7118,  0.2290,  ..., -0.1355,  0.7133, -0.2185],
         [-1.4596, -1.1292,  0.6057,  ..., -0.1421,  0.4133, -0.0396],
         [-1.6374, -0.7536,  0.3196,  ..., -0.5295,  0.6100, -0.3243]],

        [[-1.9461, -0.6480,  0.6251,  ..., -0.1435,  0.5318, -0.3694],
         [-1.9482, -1.1253,  0.6905,  ..., -0.5866,  0.9129, -0.4004],
         [-1.7333, -0.6259, -0.0143,  ..., -0.0024,  0.6328, -0.4257],
         [-1.1319, -1.1463,  0.1197,  ..., -0.1356,  0.9510, -0.5046]],

        [[-1.1326, -1.1558,  0.6471,  ..., -0.1653,  0.5468,  0.0608],
         [-1.2812, -0.9771,  0.5730,  ..., -0.2324,  0.8065, -0.3066],
         [-2.1429, -0.8427,  0.6771,  ..., -0.2022,  0.7060, -0.0394],
         [-1.5178, -0.7776,  0.6270,  ..., -0.5028,  0.3798, -0.2817]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.6516, -2.0663,  0.5953,  ..., -0.1441,  1.0284, -1.0853],
         [-1.7264, -2.4215,  0.2768,  ..., -0.4967,  0.7252, -1.7795],
         [-1.2623, -2.1114,  0.5450,  ..., -0.6539,  0.7979, -2.2971],
         [-1.6844, -1.5380,  0.3867,  ..., -0.6863,  0.6641, -2.1075]],

        [[-1.6710, -2.0499,  0.4236,  ..., -0.2343,  0.8410, -1.8666],
         [-1.6556, -2.1697,  0.6258,  ..., -0.5981,  1.3519, -1.2953],
         [-1.7489, -2.2884,  0.4631,  ..., -0.0635,  0.8053, -2.2122],
         [-1.4028, -2.1446,  0.5022,  ..., -0.3494,  0.8538, -1.6390]],

        [[-1.6614, -1.9756,  0.7222,  ..., -0.3797,  1.2294, -2.3565],
         [-1.6382, -2.0498,  0.7270,  ..., -0.4315,  0.0316, -2.1274],
         [-1.8227, -2.3609,  0.1299,  ...,  0.0239,  0.7041, -1.8520],
         [-1.8306, -1.0077,  0.7421,  ..., -0.2015,  0.9851, -2.0748]],

        ...,

        [[-1.4520, -2.1410, -0.2787,  ..., -0.3567,  0.1339, -1.8104],
         [-1.9693, -2.2523,  0.5449,  ..., -0.3957,  1.0073, -1.6644],
         [-1.9037, -0.4404,  0.0940,  ..., -0.1236,  0.9424, -1.9142],
         [-1.7487, -1.1575,  0.5157,  ..., -0.6163,  0.9020, -1.3032]],

        [[-1.2965, -2.3026,  0.7872,  ..., -0.1680,  0.7514, -1.3894],
         [-1.8506, -2.2707,  0.7642,  ..., -0.3476,  1.0988, -2.0846],
         [-1.7852, -1.6966,  0.2331,  ..., -0.2668,  0.9112, -2.0536],
         [-2.0539, -2.1064,  0.3965,  ..., -0.3099,  1.0292, -1.9054]],

        [[-1.6298, -2.3654,  0.4398,  ..., -0.0634,  1.0941, -0.5096],
         [-1.4001, -2.2792,  0.6321,  ..., -0.0411,  1.1125, -2.0707],
         [-1.4856, -2.0486,  0.5947,  ..., -0.2770,  0.6689, -2.0844],
         [-1.7681, -1.9554,  0.3215,  ..., -0.4968,  0.9302, -1.9917]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8309, -3.1152,  0.2630,  ..., -0.2730,  1.0780,  0.1885],
         [-1.8096, -2.4864,  0.1195,  ..., -0.7951,  1.0762,  0.1141],
         [-1.5606, -2.6064,  0.6167,  ..., -1.0462,  0.4869, -0.5136],
         [-0.6263, -2.7006,  0.4191,  ..., -1.2056,  1.1986, -0.2744]],

        [[-1.9698, -2.7575,  0.3140,  ..., -0.7085,  0.9911,  0.2634],
         [-1.9123, -2.4787, -0.0786,  ..., -1.2866,  1.3572,  0.1200],
         [-0.8554, -2.7817,  0.2479,  ..., -1.0020,  1.3207,  0.0830],
         [-1.7097, -1.7011,  0.4252,  ..., -1.1082,  1.4278,  0.0276]],

        [[-1.6553, -2.7095,  0.5849,  ..., -0.4802,  1.4496,  0.0389],
         [-1.6906, -3.0852,  0.7801,  ..., -1.0661,  0.9566, -0.1325],
         [-1.4900, -2.6711,  0.2971,  ..., -0.8761,  1.1072,  0.1789],
         [-1.9299, -2.2490,  0.2473,  ..., -0.8808,  1.1956, -0.0362]],

        ...,

        [[-1.5852, -1.7363,  0.0892,  ..., -1.0843,  0.7962,  0.1955],
         [-1.7501, -2.5759,  0.3399,  ..., -0.8868,  1.1057, -0.1343],
         [-1.6282, -2.1980,  0.1753,  ..., -1.0445,  1.2050,  0.0387],
         [-1.7806, -2.2889,  0.5297,  ..., -1.1883,  1.1590,  0.6436]],

        [[-1.5944, -2.8851,  0.4579,  ..., -1.1209,  1.0550,  0.3475],
         [-2.1806, -2.7193,  0.4220,  ..., -0.8522,  1.2276,  0.2078],
         [-0.8491, -2.9172,  0.5876,  ..., -0.8738,  0.9753,  0.0521],
         [-1.9557, -2.7330, -0.0514,  ..., -0.9192,  1.0826, -0.1315]],

        [[-1.5417, -2.8257,  0.4703,  ..., -0.8415,  1.2878,  0.7413],
         [-1.8921, -2.7642,  0.3120,  ..., -0.8806,  1.5785,  0.1084],
         [-1.1943, -2.5864,  0.3650,  ..., -1.0737,  0.8904, -0.0961],
         [-1.9311, -2.6523,  0.4842,  ..., -1.2493,  1.3883, -0.0191]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.0002, -1.5425,  0.4319,  ..., -0.4165,  0.3556, -0.6849],
         [-1.9320, -1.3776,  0.0672,  ..., -1.4581,  0.0920, -1.3837],
         [-1.7801, -0.7440, -0.3742,  ..., -1.3886, -0.1372, -1.2018],
         [-1.4611, -1.0804, -0.0302,  ..., -1.6190,  0.0173, -1.4177]],

        [[-2.3014, -0.9215, -0.1680,  ..., -1.0460, -0.1154, -1.0232],
         [-1.8773, -1.0917, -0.3881,  ..., -1.2089,  0.5143, -1.0881],
         [-1.1533, -0.9668, -0.1911,  ..., -1.3595,  0.0436, -0.7828],
         [-2.2906, -0.7632, -0.0418,  ..., -1.4717,  0.2832, -1.1407]],

        [[-1.8381, -1.3371, -0.2931,  ..., -1.1140,  0.2958, -1.0385],
         [-2.1828, -1.1014, -0.0712,  ..., -1.5642,  0.2465, -0.9938],
         [-2.3436, -1.3954, -0.2153,  ..., -1.4429,  0.2986, -0.1968],
         [-1.9014, -1.1024, -0.3811,  ..., -1.5864,  0.0596, -1.1447]],

        ...,

        [[-1.9486, -0.7638, -0.1891,  ..., -0.9969, -0.0040, -0.7780],
         [-2.0978, -1.0210, -0.1209,  ..., -1.4801,  0.0439, -1.1373],
         [-1.9147, -1.0926, -0.1390,  ..., -1.4759,  0.5709, -1.0077],
         [-1.7222, -0.8677, -0.0338,  ..., -1.5031,  0.5460, -0.0361]],

        [[-1.9710, -1.2500, -0.1772,  ..., -0.8480,  0.0332, -0.9834],
         [-2.3472, -1.0662, -0.4278,  ..., -1.3017,  0.0317, -1.1833],
         [-1.8628, -1.1039, -0.2606,  ..., -1.4091, -0.1424, -1.0458],
         [-1.9791, -1.0647,  0.0975,  ..., -1.3798,  0.0815, -1.1711]],

        [[-1.8566, -1.4212, -0.2236,  ..., -1.4391,  0.0101, -0.6418],
         [-2.2271, -1.1296, -0.1263,  ..., -1.4343,  0.4932, -0.1924],
         [-1.8965, -1.2251, -0.3974,  ..., -1.4132,  0.1688, -0.9895],
         [-2.1611, -1.0816, -0.0834,  ..., -1.4158,  0.4411, -0.5891]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([840, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([812, 4, 256]), pos_embed.shape: torch.Size([812, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
encoder start :
	 src.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([812, 4, 256]), k.shape: torch.Size([812, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([812, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([812, 4, 256])
	 (after FFN) src2.shape: torch.Size([812, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([812, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.4203, -1.7735,  0.0833,  ..., -0.8126,  0.9270, -1.8471],
         [-0.6763, -1.0459,  0.2415,  ..., -0.7307,  0.9730, -1.7039],
         [-2.3029, -1.5121, -0.0765,  ..., -0.6023,  0.8206, -1.6191],
         [-2.1428, -1.5426, -0.0437,  ..., -0.6472,  0.8916, -0.8120]],

        [[-2.3245, -1.6641,  0.2136,  ..., -0.9425,  0.9718, -1.6137],
         [-2.2017, -1.6577,  0.2613,  ..., -0.9431,  0.8896, -1.6697],
         [-2.0745, -1.3559,  0.2187,  ..., -0.7393,  0.9416, -1.8074],
         [-2.2450, -1.5328, -0.4326,  ..., -0.6906,  0.8956, -1.6146]],

        [[-0.8557, -1.4115, -0.2727,  ..., -0.8440,  0.7972, -1.6509],
         [-2.2051, -1.5002, -0.0546,  ..., -0.3545,  0.9156, -0.8699],
         [-2.2458, -1.3509,  0.1317,  ..., -1.0268,  0.3437, -1.8683],
         [-2.2366, -1.5859,  0.2255,  ..., -0.9430,  0.0386, -1.4982]],

        ...,

        [[-1.7881, -1.3206,  0.2172,  ..., -0.3878,  0.7571, -1.6511],
         [-2.3270, -1.3066, -0.0438,  ..., -0.8870,  0.7869, -1.8481],
         [-2.1565, -1.2584, -0.1687,  ..., -0.7558,  0.9482, -1.7797],
         [-2.2048, -0.5853, -0.1691,  ..., -0.9256,  0.8739, -1.8643]],

        [[-2.3060, -1.5342,  0.0175,  ..., -0.9034,  0.5860, -1.6908],
         [-0.5643, -1.5240, -0.0607,  ..., -0.7212,  0.8934, -1.8030],
         [-2.1084, -0.1632, -0.2052,  ..., -0.6638,  0.0714, -1.6731],
         [-1.9882, -1.4800,  0.0986,  ..., -0.9505,  1.0742, -0.7951]],

        [[-2.1967, -1.3897, -0.0728,  ..., -0.8046,  0.8485, -1.5035],
         [-1.7524, -1.6151,  0.1258,  ..., -1.0774,  0.7965, -0.2436],
         [-0.0260, -1.3239,  0.1338,  ..., -0.8048,  0.9419, -1.6382],
         [-2.0378, -1.3384, -0.1532,  ..., -0.7849,  0.7858, -1.6892]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9391, -1.0754,  0.6441,  ..., -0.2847,  0.6157, -0.5982],
         [-1.0667, -0.7001,  0.5987,  ...,  0.0858,  0.8773, -0.2325],
         [-1.8746, -0.6046,  0.5095,  ...,  0.1269,  0.4822, -0.5592],
         [-1.9347, -0.8605,  0.2335,  ..., -0.3355,  0.5496, -0.2995]],

        [[-1.7964, -1.0848,  0.1907,  ..., -0.1543,  0.5580, -0.3974],
         [-1.3545, -0.7574,  0.7805,  ..., -0.0630,  0.5453, -1.0990],
         [-1.5537, -1.4393,  0.5831,  ..., -0.1954,  0.7990, -0.1936],
         [-1.6666, -0.9062,  0.4605,  ..., -0.0026,  0.6996, -0.6454]],

        [[-0.6179, -1.1058,  0.4405,  ...,  0.0153,  0.6500, -0.1988],
         [-1.5045, -1.0503,  0.4158,  ..., -0.2241,  0.7096,  0.1137],
         [-1.1327, -0.9460,  0.5628,  ..., -0.4643,  0.3656, -0.3512],
         [-2.0753, -0.7209,  0.3970,  ..., -0.1651,  0.6709, -0.2830]],

        ...,

        [[-1.3580, -0.9271,  0.6030,  ...,  0.1164,  0.7161, -0.5555],
         [-1.1647, -1.1421,  0.3268,  ..., -0.5462,  0.9809, -0.4730],
         [-1.5618, -0.8047,  0.1148,  ..., -0.3159,  0.5319, -0.2728],
         [-1.6939, -0.2087,  0.0404,  ..., -0.2924,  0.7778, -0.3421]],

        [[-1.5948, -0.9611,  0.4822,  ..., -0.2420,  0.5012, -0.3366],
         [-0.9296, -1.4737,  0.5322,  ...,  0.3502,  0.7087, -0.5277],
         [-1.7844, -0.3062,  0.4402,  ..., -0.0488,  0.3503, -0.4010],
         [-1.7389, -0.9492,  0.5615,  ..., -0.5455,  0.4886,  0.1693]],

        [[-1.8429, -0.7462,  0.3669,  ..., -0.4270,  0.7391, -0.2708],
         [-1.4794, -1.2788,  0.4109,  ..., -0.2681,  0.6622,  0.0326],
         [-0.8230, -0.8866,  0.3751,  ..., -0.4115,  0.6389, -0.2081],
         [-1.7508, -1.1856,  0.3681,  ...,  0.0251,  0.3547, -0.3507]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8114, -1.2173,  0.4005,  ...,  0.1910,  0.8493, -1.9074],
         [-1.2975, -1.8243,  0.8691,  ..., -0.2063,  1.1105, -2.1467],
         [-1.7223, -1.1191,  0.5033,  ...,  0.3349,  0.9674, -2.0871],
         [-1.8514, -2.0753,  0.2612,  ..., -0.4524,  0.8490, -0.9220]],

        [[-1.6636, -2.2224,  0.5224,  ..., -0.1543,  0.9635, -1.3426],
         [-1.7314, -2.0103,  0.7374,  ..., -0.7123,  0.5828, -2.2630],
         [-1.6875, -2.1387,  0.6381,  ...,  0.0342,  1.1586, -1.4880],
         [-1.8059, -2.2276,  0.8432,  ..., -0.2573,  0.3006, -0.7698]],

        [[-1.3961, -2.1697,  0.0945,  ..., -0.4791,  1.0710, -1.5473],
         [-1.4196, -1.2757, -0.0458,  ..., -0.0553,  1.1369, -2.0784],
         [-1.5155, -2.1242,  0.6652,  ..., -0.3690,  0.9813, -2.1828],
         [-1.6176, -1.2047,  0.5861,  ..., -0.3044,  1.0283, -1.4869]],

        ...,

        [[-1.2441, -2.4805,  0.5124,  ..., -0.0400,  0.8305, -2.3094],
         [-1.4644, -2.4156,  0.6125,  ..., -0.3714,  1.0283, -2.0441],
         [-1.7566, -2.3609,  0.3562,  ..., -0.8086,  0.9846, -1.9589],
         [-1.5281, -2.0311,  0.1079,  ..., -0.1567,  0.8800, -1.7429]],

        [[-1.9705, -2.2141,  0.7373,  ..., -0.1348,  0.8319, -1.5818],
         [-1.7346, -1.3953,  0.6883,  ..., -0.2165,  1.0779, -2.2397],
         [-1.7003, -1.7264, -0.2840,  ..., -0.1008,  1.0407, -2.0853],
         [-1.6470, -1.5387,  0.6351,  ..., -0.6679,  0.8447, -2.0653]],

        [[-1.8086, -1.9062,  0.6251,  ..., -0.2032,  1.2891, -1.8303],
         [-1.3788, -1.3310,  0.5138,  ..., -0.3084,  0.7753, -1.7348],
         [-1.1229, -2.2316,  0.2083,  ...,  0.1249,  0.5460, -1.9093],
         [-1.4802, -1.3558,  0.8835,  ..., -0.4221,  0.8606, -1.8957]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8897, -2.5365,  0.0527,  ..., -0.6003,  1.2158,  0.2669],
         [-1.2385, -2.5436,  0.7680,  ..., -0.9046,  1.5689,  0.0772],
         [-1.7949, -1.9251,  0.3932,  ..., -0.5697,  1.4264, -0.3130],
         [-2.0460, -2.4235,  0.3015,  ..., -0.3471,  1.3885,  0.4286]],

        [[-1.7816, -2.5286,  0.2468,  ..., -0.7900,  0.2745, -0.0239],
         [-1.6855, -2.7053,  0.4304,  ..., -1.1905,  1.3073,  0.0674],
         [-1.9862, -2.5101,  0.5093,  ..., -0.7887,  1.1363,  0.3708],
         [-1.7111, -2.7098,  1.0310,  ..., -1.1153,  1.1624,  0.3679]],

        [[-1.7948, -2.7784,  0.4768,  ..., -1.2695,  1.2052,  0.2801],
         [-1.6457, -2.1222,  0.1170,  ..., -0.7566,  1.5596, -0.1559],
         [-1.6789, -2.4977,  0.2852,  ..., -0.9518,  1.3537,  0.2233],
         [-1.8969, -1.6825,  0.5543,  ..., -0.9780, -0.1946,  0.1696]],

        ...,

        [[-1.5463, -2.8163,  0.4139,  ..., -0.9415,  0.3484, -0.1964],
         [-1.8847, -2.6286,  0.5719,  ..., -1.1035,  1.0627, -0.1077],
         [-1.3628, -2.8349,  0.3018,  ..., -0.9829,  0.9501,  0.1634],
         [-0.8303, -2.7882,  0.2407,  ..., -0.2873,  1.5060,  0.0230]],

        [[-1.9042, -2.3788,  0.4924,  ..., -0.8476,  1.5528,  0.2608],
         [-1.6408, -2.5734,  0.3768,  ..., -1.2504,  1.4923, -0.5454],
         [-1.8754, -2.5235,  0.0947,  ..., -0.9073,  1.0514, -0.1651],
         [-0.7945, -2.4089,  0.4149,  ..., -1.2194,  1.1931, -0.1914]],

        [[-1.5052, -2.5723,  0.8165,  ..., -0.7973,  1.6970,  0.1142],
         [-1.7668, -2.3513,  0.4662,  ..., -0.8856,  1.2825,  0.1175],
         [-1.7963, -1.4263,  0.4677,  ..., -0.8206,  0.9679, -0.0195],
         [-1.5288, -2.6082,  0.8473,  ..., -0.9894,  1.1360, -0.1740]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9075e+00, -1.2220e+00,  2.8572e-01,  ..., -1.2822e+00,
           1.0284e-01, -9.3688e-01],
         [-1.6021e+00, -1.0162e+00, -9.3921e-02,  ..., -1.3739e+00,
           6.2083e-01, -9.6699e-01],
         [-2.1309e+00, -8.2584e-01, -1.2848e-01,  ..., -1.2984e+00,
           4.3510e-01, -1.3485e+00],
         [-2.4087e+00, -8.3133e-01, -5.5277e-01,  ..., -1.2313e+00,
           1.9788e-01, -1.1226e+00]],

        [[-2.0985e+00, -1.1816e+00,  1.6620e-02,  ..., -1.4637e+00,
          -5.0061e-02, -1.1262e+00],
         [-2.2478e+00, -1.1073e+00, -1.9534e-01,  ..., -1.1242e+00,
           4.6003e-01, -1.0707e+00],
         [-2.3562e+00, -1.2351e+00, -2.8142e-01,  ..., -1.1612e+00,
           3.1475e-01, -9.7069e-01],
         [-2.2554e+00, -1.1551e+00,  7.6885e-01,  ..., -1.5932e+00,
           3.3604e-01, -7.7953e-01]],

        [[-1.0766e+00, -1.3663e+00, -7.3817e-02,  ..., -1.1012e+00,
           5.2626e-01, -1.1045e+00],
         [-2.3059e+00, -6.6775e-01, -1.9992e-01,  ..., -1.4900e+00,
           6.4599e-01, -8.0034e-01],
         [-1.5911e+00, -1.2222e+00, -1.9501e-01,  ..., -1.2392e+00,
           3.6088e-01, -8.7082e-01],
         [-1.9616e+00, -1.0496e+00, -1.2511e-01,  ..., -1.3023e+00,
          -4.5531e-01, -1.0219e+00]],

        ...,

        [[-2.0243e+00, -1.2050e+00, -2.1629e-01,  ..., -1.2562e+00,
           7.5579e-02, -8.9246e-01],
         [-1.6187e+00, -9.2984e-01, -3.2061e-01,  ..., -1.4504e+00,
           4.3806e-01, -1.0441e+00],
         [-1.5065e+00, -9.9570e-01, -2.9776e-01,  ..., -1.7246e+00,
           1.9314e-01, -9.3052e-01],
         [-1.9206e+00, -1.1028e+00,  5.0423e-01,  ..., -1.1293e+00,
           2.8990e-01, -1.2050e+00]],

        [[-2.2566e+00, -7.2388e-01,  7.3211e-02,  ..., -1.3192e+00,
           1.4364e-01, -9.4959e-01],
         [-1.7196e+00, -1.2329e+00, -1.3002e-01,  ..., -1.2407e+00,
           4.0708e-01, -1.2943e+00],
         [-2.2875e+00, -1.1301e+00, -3.9155e-01,  ..., -1.3147e+00,
           5.7930e-02, -1.3110e+00],
         [-1.7756e+00, -8.8246e-01, -9.9204e-02,  ..., -1.4989e+00,
           4.0153e-01, -1.0645e+00]],

        [[-1.6266e+00, -9.6358e-01, -4.6267e-02,  ..., -1.2241e+00,
           7.7981e-02, -8.7668e-01],
         [-2.1125e+00, -9.8492e-01, -1.1504e-01,  ..., -1.3644e+00,
           3.5420e-01, -1.1673e+00],
         [-1.2288e+00, -6.1750e-01, -1.4096e-01,  ..., -1.2587e+00,
           4.7137e-02, -9.1373e-01],
         [-2.0721e+00, -1.0630e+00, -2.4694e-01,  ..., -1.5047e+00,
           8.5524e-05, -1.0627e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([812, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([660, 4, 256]), pos_embed.shape: torch.Size([660, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1370, -1.2143,  0.0136,  ..., -1.0804,  0.7457, -0.8879],
         [-1.9712, -1.0585,  0.0709,  ..., -0.5066,  0.5103, -1.8312],
         [-2.2840, -1.5539,  0.2391,  ..., -0.1413,  0.0118, -1.6244],
         [-1.5460, -1.6564,  0.1315,  ..., -0.9022,  1.1394, -1.5082]],

        [[-2.1509, -1.7612, -0.1863,  ..., -0.7413,  0.8885, -1.7613],
         [-0.4812, -1.8086,  0.4636,  ..., -0.7413,  0.7254, -1.7443],
         [-2.1407, -1.5523,  0.0298,  ..., -0.3087,  0.7635, -1.6133],
         [-1.9152, -1.1304,  0.1167,  ..., -0.6612,  1.1691, -1.5769]],

        [[-2.3121, -1.6003,  0.2398,  ..., -0.7320,  0.8937, -0.8418],
         [-1.9577, -1.1850, -0.0452,  ..., -0.9694,  0.5527, -1.9618],
         [-2.1764, -0.5821, -0.0088,  ..., -0.9314,  1.1328, -1.7064],
         [-2.2121, -1.2667, -0.1257,  ..., -0.7895,  0.7189, -1.9959]],

        ...,

        [[-2.2583, -1.3669,  0.0542,  ..., -0.8103,  0.8440, -1.7303],
         [-2.1882, -1.5458,  0.2448,  ..., -0.7027,  0.7767, -1.6755],
         [-0.6813, -1.6609,  0.0176,  ..., -0.9283,  0.9395, -1.6298],
         [-2.1859, -1.3672,  0.0119,  ..., -0.8569,  0.8057, -1.5811]],

        [[-2.3309, -1.5536, -0.2038,  ..., -0.3008,  0.9859, -1.8324],
         [-2.1022, -1.7442,  0.0141,  ..., -1.0468,  0.7902, -1.0146],
         [-1.6036, -0.6159,  0.3936,  ..., -0.9214,  0.7971, -1.7243],
         [-1.9603, -0.1729,  0.2838,  ..., -0.8457,  0.8383, -1.7767]],

        [[-2.2630, -0.5986, -0.1350,  ..., -0.7972,  0.9920, -0.8605],
         [-2.3830, -1.7279,  0.1445,  ..., -0.6214,  0.1231, -1.6342],
         [-2.1123, -0.5724,  0.2591,  ..., -0.7013,  0.8082, -1.6738],
         [-1.9612, -1.2411,  0.0310,  ..., -0.5958,  0.7628, -1.6834]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5581, -0.7868,  0.6469,  ..., -0.1729,  0.6199,  0.0984],
         [-1.6313, -0.9109,  0.1580,  ..., -0.1394,  0.5703, -0.5908],
         [-1.9951, -0.9227,  0.6936,  ..., -0.1361,  0.2971, -0.0520],
         [-1.6153, -1.0777,  0.4918,  ..., -0.4297,  0.8939, -0.3277]],

        [[-1.7933, -0.7376,  0.4566,  ..., -0.4913,  0.7839, -0.5965],
         [-1.2232, -0.9146,  0.6480,  ..., -0.4393,  0.8291, -0.7447],
         [-1.5681, -1.1877,  0.6803,  ..., -0.1483,  0.5153, -0.1796],
         [-1.9152, -0.4489,  0.5053,  ..., -0.3725,  0.9591, -0.6714]],

        [[-1.9998, -1.1111,  0.4487,  ..., -0.4333,  0.6915, -0.1478],
         [-1.6166, -0.4516,  0.4714,  ..., -0.5333,  0.4568, -0.4377],
         [-1.8575, -0.6606,  0.6634,  ...,  0.0704,  1.0176, -0.2597],
         [-1.7365, -1.1137,  0.5438,  ..., -0.2147,  0.8473, -0.4758]],

        ...,

        [[-0.8567, -1.1790,  0.7732,  ...,  0.0762,  0.9469, -0.8312],
         [-1.1157, -0.9891,  0.5973,  ..., -0.1645,  0.1857, -0.1387],
         [-1.1293, -0.7405,  0.5561,  ..., -0.5984,  0.6404, -0.7789],
         [-1.9478, -1.0593,  0.5501,  ..., -0.0458,  0.2875, -0.2768]],

        [[-1.7241, -0.7508, -0.1497,  ..., -0.2112,  0.8139, -0.4556],
         [-2.1116, -1.4120,  0.5845,  ..., -0.3891,  0.0302, -0.0630],
         [-0.8043, -0.7904,  0.4290,  ..., -0.4259,  0.9709, -0.3928],
         [-1.3951, -0.5464,  0.3591,  ..., -0.0760,  0.8862, -0.2052]],

        [[-1.0985, -0.2902,  0.2604,  ..., -0.1216,  0.5039, -0.1418],
         [-1.9677, -0.7464,  0.7821,  ..., -0.0405,  0.5392, -0.4076],
         [-0.8356, -0.7123,  0.5077,  ..., -0.0577,  0.6318, -0.5240],
         [-1.6986, -0.4736,  0.3406,  ...,  0.1965,  0.6185, -0.3989]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7450, -2.1129,  0.7722,  ...,  0.2986,  0.9327, -1.6538],
         [-1.9889, -2.2385,  0.4308,  ..., -0.1442,  1.0020, -1.9565],
         [-2.1062, -2.3301,  0.7929,  ..., -0.2537,  0.8456, -1.8689],
         [-2.1337, -2.4906, -0.2247,  ..., -0.2721,  1.1568, -1.7850]],

        [[-1.8922, -1.9746,  0.3194,  ..., -0.7956,  1.3363, -2.3471],
         [-1.3106, -2.3861,  0.9608,  ..., -0.5897,  1.1168, -2.1170],
         [-1.7320, -2.2552,  0.7362,  ..., -0.7941,  0.8381, -2.0753],
         [-1.8127, -2.3908,  0.5188,  ..., -0.2924,  1.0770, -1.6681]],

        [[-2.1248, -1.2653,  0.3688,  ...,  0.1461,  0.1830, -1.8997],
         [-1.8997, -2.1198,  0.3775,  ..., -0.6364,  0.6790, -2.0296],
         [-1.9576, -2.3189,  0.9845,  ..., -0.2959,  1.5187, -2.0124],
         [-1.7692, -2.0916,  0.5019,  ..., -0.3594,  0.1072, -2.1385]],

        ...,

        [[-1.6704, -2.3906,  0.7983,  ...,  0.0036,  1.0414, -2.5008],
         [-1.0034, -1.2289,  0.2099,  ..., -0.2398,  0.5896, -1.2737],
         [-1.6521, -2.2533,  0.3490,  ..., -0.8175,  0.9606, -1.4472],
         [-1.4859, -2.1741,  0.4585,  ..., -0.5387,  0.8004, -2.0321]],

        [[-1.8004, -2.1679,  0.2296,  ..., -0.3280,  0.8955, -1.8902],
         [-2.0738, -2.0561,  0.2788,  ..., -0.2498,  0.5977, -1.4634],
         [-1.8025, -2.0983,  0.6872,  ..., -0.3037,  1.1869, -1.7719],
         [-1.6603, -1.0883,  0.6817,  ..., -0.1069,  0.0629, -1.9617]],

        [[-1.8039, -1.8493,  0.3797,  ..., -0.1787, -0.2422, -1.7226],
         [-2.0552, -1.0772,  0.2356,  ..., -0.0683,  0.0030, -1.9402],
         [-1.5953, -1.1231,  0.5017,  ..., -0.3250,  1.3316, -1.6219],
         [-1.2193, -2.0555,  0.4462,  ...,  0.0983,  0.6667, -1.9998]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5995, -2.8719,  0.9853,  ..., -0.0481,  0.9742,  0.1538],
         [-1.6715, -2.7873,  0.5681,  ..., -0.8686,  1.1608,  0.2242],
         [-1.8247, -2.5386,  0.5786,  ...,  0.0257,  1.5981, -0.3117],
         [-2.1429, -2.8694,  0.2132,  ..., -1.0490,  1.7035,  0.0325]],

        [[-2.0421, -2.1764,  0.3849,  ..., -1.1320,  0.7700,  0.0301],
         [-1.3694, -2.7053,  0.5927,  ..., -0.9692,  0.9672, -0.2245],
         [-1.7740, -2.8143,  0.3627,  ..., -1.0070,  1.3555, -0.0173],
         [-1.9481, -2.2699,  0.4466,  ..., -1.1036,  1.4114,  0.3279]],

        [[-1.7586, -2.4712,  0.5241,  ..., -0.8579,  0.4381,  0.1957],
         [-1.4212, -2.7495,  0.4163,  ..., -0.4625,  1.1549,  0.1621],
         [-1.9325, -1.8341,  0.5635,  ..., -1.1328,  1.6844,  0.0137],
         [-1.4058, -1.7761,  0.1617,  ..., -0.9402,  0.7314,  0.1541]],

        ...,

        [[-0.8468, -2.5215,  0.4568,  ..., -0.1771,  1.2456, -0.2799],
         [-1.1903, -1.7367,  0.6541,  ..., -0.8331,  1.0114, -0.2963],
         [-1.6490, -2.3737,  0.1211,  ..., -1.3175,  1.4018,  0.4072],
         [-0.9714, -2.7103,  0.6744,  ..., -0.3903,  1.1509, -0.1000]],

        [[-1.8912, -2.7059,  0.3223,  ..., -0.9438,  1.2299, -0.2808],
         [-2.0030, -2.7781,  0.1782,  ..., -1.0350,  0.6569,  0.3894],
         [-1.6306, -2.7676,  0.4175,  ..., -0.9864,  1.0973, -0.8296],
         [-1.5138, -2.1608,  0.3016,  ..., -0.1865,  1.3151,  0.2479]],

        [[-1.9995, -2.7720,  0.4360,  ..., -0.9628,  0.0410,  0.5090],
         [-2.0523, -2.2038,  0.4337,  ..., -0.8080,  0.8679,  0.2668],
         [-1.7732, -2.1731,  0.3032,  ..., -1.0113,  1.7715, -0.1080],
         [-1.6109, -2.8584,  0.1806,  ..., -1.0505,  1.3766,  0.2528]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.0565e+00, -1.3841e+00,  7.0721e-01,  ..., -6.8813e-01,
           7.8789e-02, -1.2507e+00],
         [-1.3964e+00, -1.5149e+00,  5.5690e-01,  ..., -1.5644e+00,
          -1.0158e-01, -9.6191e-01],
         [-2.1588e+00, -8.2512e-01,  1.3011e-01,  ..., -1.1472e+00,
           6.1388e-01, -1.3146e+00],
         [-2.2245e+00, -1.1573e+00, -2.8596e-01,  ..., -1.2292e+00,
           6.7584e-02, -9.6642e-01]],

        [[-2.3939e+00, -1.1465e+00,  2.6161e-02,  ..., -1.3453e+00,
          -1.0353e-01, -1.2426e+00],
         [-2.0656e+00, -1.3136e+00, -2.7297e-01,  ..., -1.3911e+00,
          -4.3874e-04, -1.1203e+00],
         [-2.2818e+00, -1.1490e+00,  2.0593e-01,  ..., -1.0596e+00,
           5.5919e-01, -1.2985e+00],
         [-1.9337e+00, -9.9440e-01,  2.1975e-01,  ..., -1.2734e+00,
           2.6447e-01, -9.3128e-01]],

        [[-1.8680e+00, -9.6991e-01,  1.5194e-01,  ..., -1.2235e+00,
           1.3290e-01, -1.0315e+00],
         [-2.1485e+00, -9.1394e-01, -1.8837e-03,  ..., -1.1481e+00,
           2.5223e-01, -1.1094e+00],
         [-1.0582e+00, -9.8555e-01, -2.4887e-01,  ..., -8.2033e-01,
           8.0503e-01, -1.0015e+00],
         [-2.2049e+00, -6.2452e-01, -1.7093e-01,  ..., -1.6143e+00,
           2.1578e-01, -1.3704e+00]],

        ...,

        [[-1.8504e+00, -1.0505e+00,  1.6998e-01,  ..., -5.8082e-01,
           3.9441e-01, -1.3368e+00],
         [-2.3057e+00, -8.8525e-01,  2.8560e-01,  ..., -6.5212e-01,
           4.8218e-02, -1.0532e+00],
         [-2.2905e+00, -9.3052e-01, -2.7940e-01,  ..., -1.4489e+00,
           4.1725e-01, -7.6625e-01],
         [-1.8228e+00, -9.6264e-01,  6.6659e-01,  ..., -1.1493e+00,
           2.7493e-01, -3.4358e-01]],

        [[-1.9834e+00, -1.4313e+00,  2.6336e-02,  ..., -1.3928e+00,
           6.0858e-01, -1.0590e+00],
         [-2.3919e+00, -1.2442e+00, -3.3829e-01,  ..., -1.3562e+00,
           9.2918e-02, -9.3780e-01],
         [-2.2303e+00, -1.1601e+00,  5.5065e-01,  ..., -1.7507e+00,
           3.2564e-01, -1.2118e+00],
         [-1.4951e+00, -1.0182e+00, -2.5173e-01,  ..., -1.3088e+00,
           3.3887e-01, -1.1698e+00]],

        [[-1.5838e+00, -1.3351e+00, -1.5011e-01,  ..., -1.3795e+00,
          -2.4585e-01, -8.4025e-01],
         [-2.4596e+00, -8.3605e-01, -1.8025e-01,  ..., -1.4028e+00,
           2.5085e-01, -1.0604e+00],
         [-2.5319e+00, -6.2651e-01, -4.2588e-01,  ..., -1.2847e+00,
           8.0230e-01, -1.2682e+00],
         [-2.1253e+00, -1.1916e+00, -1.4480e-01,  ..., -1.1818e+00,
           1.6751e-01, -9.8615e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([660, 4, 256]), pos_embed.shape: torch.Size([660, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
encoder start :
	 src.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([660, 4, 256]), k.shape: torch.Size([660, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([660, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([660, 4, 256])
	 (after FFN) src2.shape: torch.Size([660, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([660, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1928, -1.6263,  0.0683,  ..., -0.7800,  0.6153, -1.5173],
         [-2.0000, -1.1285,  0.0317,  ..., -0.5645,  0.5216, -1.7263],
         [-0.3642, -1.4034,  0.2619,  ..., -1.0721,  0.8100, -1.7766],
         [-2.2433, -1.4013,  0.1270,  ..., -0.6339,  0.8018, -1.9786]],

        [[-2.2652, -1.2851,  0.0052,  ..., -0.1159,  1.0506, -0.8457],
         [-2.0932, -1.5557,  0.2486,  ..., -0.7191,  0.7755, -0.5571],
         [-1.4305, -1.5400,  0.2403,  ..., -0.6763,  0.4273, -1.8403],
         [-1.5077, -0.6754,  0.1305,  ..., -0.9179,  0.8661, -1.2797]],

        [[-2.4011, -1.3555, -0.0390,  ..., -0.0220,  0.8241, -1.9166],
         [-2.2127, -1.5401, -0.1381,  ..., -0.7876,  0.6939, -1.9158],
         [-2.2614, -1.6129, -0.4265,  ..., -0.7616,  1.1173, -0.2171],
         [-0.7126, -1.3395,  0.1551,  ..., -0.1922,  0.7107, -1.9204]],

        ...,

        [[-2.0080, -1.4511, -0.0565,  ..., -0.8074,  1.1969, -1.8409],
         [-0.5446, -1.6551,  0.2348,  ..., -0.7976,  0.7331, -0.8595],
         [-1.9807, -1.7472,  0.3925,  ..., -0.9736,  0.9436, -0.2256],
         [-2.0355, -1.6754,  0.1007,  ..., -0.6739,  0.8891, -0.8117]],

        [[-2.2193, -1.3544,  0.0830,  ..., -0.7570,  0.8433, -1.3048],
         [-2.3485, -1.4584,  0.0766,  ..., -1.0834,  0.8150, -1.6780],
         [-2.0237, -1.4083, -0.0420,  ..., -0.7405,  0.9491, -1.9201],
         [-2.1924, -1.7424, -0.1564,  ..., -0.2858,  0.1670, -1.7770]],

        [[-2.2224, -1.1772,  0.2347,  ..., -0.7614,  0.8621, -0.8066],
         [-1.7535, -1.8417,  0.1513,  ...,  0.0316,  0.7548, -1.5930],
         [-2.2669, -0.6123,  0.2563,  ..., -0.8856,  1.1459, -1.4742],
         [-2.2027, -1.4145, -0.0266,  ..., -0.8952,  0.8578, -1.8112]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.8947, -1.0339,  0.2001,  ..., -0.5306,  1.0486, -0.3485],
         [-1.6769, -0.8288,  0.3613,  ..., -0.5204,  0.8379, -0.5166],
         [-1.0185, -0.9928,  0.0474,  ..., -0.5143,  0.8363, -0.6008],
         [-2.0230, -0.4050,  0.3941,  ..., -0.2938,  0.7334, -0.5188]],

        [[-1.8612, -0.9612,  0.8157,  ..., -0.3159,  0.8093, -0.3903],
         [-1.5239, -1.1339,  0.6408,  ..., -0.4322,  0.5161,  0.1184],
         [-0.7952, -0.9466,  0.7419,  ...,  0.0487,  0.6043, -0.6072],
         [-1.5925, -0.3494,  0.4468,  ..., -0.3551,  0.2440, -0.2250]],

        [[-1.7064, -0.9533,  0.5439,  ..., -0.0398,  0.8165, -0.8900],
         [-1.4404, -1.1547,  0.6286,  ..., -0.2270,  0.5074, -0.3842],
         [-1.8363, -1.1597,  0.0572,  ..., -0.4950,  0.7269,  0.3816],
         [-1.4893, -0.7876,  0.3503,  ..., -0.1645,  0.6441, -0.8621]],

        ...,

        [[-1.0693, -0.9612,  0.2514,  ..., -0.4126,  0.8517, -0.9344],
         [-0.9906, -1.1290,  0.6205,  ..., -0.0772,  0.7429, -0.2935],
         [-1.5124, -1.3091,  0.6780,  ...,  0.0673,  0.6523,  0.1185],
         [-1.7220, -1.1955,  0.7599,  ..., -0.2030,  0.6008, -0.2431]],

        [[-1.9997, -1.2524,  0.3856,  ..., -0.0636,  0.8473, -0.3517],
         [-2.0945, -0.6856,  0.1993,  ..., -0.6097,  0.7504, -0.4851],
         [-1.5702, -0.9573,  0.4161,  ..., -0.5471,  0.7844, -0.5083],
         [-1.6045, -0.7752,  0.4052,  ..., -0.0935,  0.3574, -0.5043]],

        [[-1.9966, -0.5529,  0.2366,  ..., -0.1771,  0.5131, -0.1915],
         [-0.9160, -1.3416,  0.4461,  ..., -0.1342,  0.8064, -0.4763],
         [-1.8311, -0.1467,  0.2216,  ..., -0.0823,  1.0893, -0.5290],
         [-1.8916, -0.6390,  0.3289,  ...,  0.2236,  0.8419, -0.4516]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.4496, -2.1449,  0.8296,  ..., -0.5728,  1.1801, -0.5155],
         [-1.7220, -2.0497,  0.5202,  ..., -0.4747,  0.9245, -2.2313],
         [-1.2319, -2.3348, -0.0315,  ..., -0.4405,  1.0339, -2.1643],
         [-1.9613, -1.8320,  0.4705,  ..., -0.8425,  0.9534, -2.3095]],

        [[-1.8211, -1.9702,  0.5827,  ..., -0.4228,  1.0380, -1.9841],
         [-1.0198, -2.2948,  0.8789,  ..., -0.3986,  0.7512, -2.0550],
         [-1.5194, -1.3053,  0.9130,  ..., -0.1133,  1.0552, -2.5510],
         [-1.1905, -2.0556,  0.5234,  ..., -0.2314,  0.8540, -1.8715]],

        [[-1.6805, -2.2980,  0.4583,  ..., -0.2495,  0.9079, -2.0337],
         [-1.8366, -2.2237,  0.6468,  ..., -0.8050,  0.7562, -2.0542],
         [-1.9464, -2.2371,  0.4017,  ..., -0.2841,  0.9328, -1.8250],
         [-1.8648, -2.1788,  0.4460,  ..., -0.2302,  1.0940, -1.6942]],

        ...,

        [[-1.5132, -2.2749,  0.6614,  ..., -0.4962,  0.9046, -2.5013],
         [-1.4941, -1.2995,  0.7479,  ...,  0.0066,  0.7972, -1.7373],
         [-1.3587, -2.2522,  0.6183,  ..., -0.1192,  0.9242, -1.6318],
         [-1.7172, -2.6131,  0.8541,  ..., -0.7216,  0.9175, -2.0244]],

        [[-1.9495, -2.2533,  0.9128,  ..., -0.4163,  1.1756, -2.1199],
         [-1.2846, -2.2430,  0.4772,  ..., -0.3259,  0.8237, -2.2724],
         [-1.3372, -2.4028,  0.4456,  ..., -0.4958,  0.8850, -2.3442],
         [-1.8032, -2.2896,  0.9380,  ..., -0.3589, -0.0200, -2.3002]],

        [[-1.5843, -2.3580,  0.7276,  ..., -0.3490,  0.9666, -1.5740],
         [-1.3729, -1.3915,  0.5049,  ..., -0.2850,  0.8040, -1.0365],
         [-1.7889, -1.7409,  0.8852,  ..., -0.5857,  0.2998, -2.3690],
         [-1.8616, -2.1113,  0.5180,  ..., -0.4127,  0.8659, -1.9042]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5691, -2.4221,  0.6376,  ..., -1.0374,  1.5023,  0.7779],
         [-1.5513, -2.7744,  0.5023,  ..., -0.9756,  1.5620, -0.4930],
         [-0.2837, -2.7658,  0.2001,  ..., -1.0640,  1.4331,  0.1392],
         [-1.9797, -2.7743,  0.5331,  ..., -0.9327,  1.4540,  0.1683]],

        [[-1.8427, -2.8045,  0.4318,  ..., -0.6943,  1.3038, -0.0191],
         [-1.4713, -2.4267,  0.6260,  ..., -0.7544,  1.2997, -0.1902],
         [-1.4672, -1.5339,  0.3289,  ..., -0.7962,  1.4068, -0.1846],
         [-1.2290, -2.8808,  0.4985,  ..., -0.7735,  0.5267,  0.2541]],

        [[-1.8617, -2.5919,  0.3473,  ..., -0.8757,  1.4984, -0.1395],
         [-2.0213, -2.9603,  0.4686,  ..., -1.1834,  1.0957,  0.1701],
         [-0.9263, -2.7543,  0.2767,  ..., -1.1607,  0.7088, -0.0391],
         [-0.8597, -2.7827,  0.4891,  ..., -1.0635,  1.1893,  0.0615]],

        ...,

        [[-1.8233, -2.2157,  0.7411,  ..., -1.0185,  1.2275, -0.2747],
         [-2.0780, -2.2647,  0.4114,  ..., -0.8698,  1.3830,  0.5051],
         [-0.6725, -2.6920,  0.5399,  ..., -0.8307,  1.2486, -0.1007],
         [-1.6141, -2.8599,  0.4165,  ..., -0.0898,  1.4422, -0.0037]],

        [[-1.5158, -2.4834,  0.4846,  ..., -0.6853,  1.4534, -0.1012],
         [-1.8057, -2.9648,  0.6327,  ..., -1.0115,  1.0466, -0.0763],
         [-1.4254, -3.1798,  0.1565,  ..., -1.3311,  1.6489,  0.0384],
         [-1.4100, -2.5905,  0.5435,  ..., -0.9063,  1.0323, -0.0889]],

        [[-1.7024, -2.7222,  0.4441,  ..., -0.9345,  1.4739,  0.1641],
         [-1.2301, -2.5698,  0.4911,  ..., -0.7356,  1.1713, -0.4741],
         [-0.8706, -2.0450,  0.5774,  ..., -1.1055,  1.2937, -0.4206],
         [-1.9018, -1.7812,  0.5673,  ..., -1.1407,  1.1688, -0.0660]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3120, -1.0038,  0.2807,  ..., -0.4084,  0.1440, -0.8390],
         [-2.1530, -1.3253, -0.0545,  ..., -1.1911,  0.4928, -1.2895],
         [-1.3201, -1.1945, -0.0762,  ..., -0.6648,  0.0273, -0.9216],
         [-1.9882, -0.9165, -0.0851,  ..., -1.4024,  0.4745, -1.1115]],

        [[-2.5167, -0.8726,  0.4986,  ..., -1.3020,  0.4045, -1.2114],
         [-1.8703, -0.9137, -0.1979,  ..., -1.5406,  0.2386, -1.0716],
         [-2.2914, -0.8940, -0.0754,  ..., -1.2463,  0.8437, -1.1313],
         [-1.8873, -1.5263, -0.0933,  ..., -1.2840, -0.1749, -1.0118]],

        [[-1.8940, -1.0773, -0.3204,  ..., -1.4510,  0.2900, -1.3635],
         [-2.1859, -1.2425, -0.1741,  ..., -1.4613,  0.0911, -1.1398],
         [-1.2568, -1.0017, -0.4095,  ..., -1.5133, -0.1576, -1.3782],
         [-1.9233, -1.0098,  0.1253,  ..., -1.4956,  0.1722, -0.2170]],

        ...,

        [[-2.4128, -0.5909, -0.1054,  ..., -1.2474,  0.4645, -1.4185],
         [-2.3382, -1.1350,  0.5053,  ..., -1.5422, -0.1043, -0.9984],
         [-1.5761, -1.2351, -0.1995,  ..., -1.4446,  0.2014, -1.1602],
         [-2.0648, -1.2276, -0.0815,  ..., -1.0964,  0.2965, -0.7515]],

        [[-2.0393, -0.9898, -0.1125,  ..., -1.3553,  0.3925, -1.2331],
         [-2.4831, -1.7809, -0.0525,  ..., -1.5970,  0.3469, -1.3174],
         [-1.9515, -1.1852, -0.1731,  ..., -1.6581,  0.4386, -1.0249],
         [-2.1405, -1.1881,  0.5776,  ..., -1.0258,  0.2289, -0.9233]],

        [[-2.0216, -1.6171, -0.2054,  ..., -1.4081,  0.1394, -0.2961],
         [-1.7207, -1.1024, -0.1987,  ..., -1.2318,  0.4329, -1.3401],
         [-2.0866, -1.0046, -0.0848,  ..., -1.5958,  0.2794, -1.1498],
         [-2.3699, -0.6162, -0.1252,  ..., -0.9285,  0.2672, -0.9856]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([660, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([961, 4, 256]), pos_embed.shape: torch.Size([961, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
encoder start :
	 src.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([961, 4, 256]), k.shape: torch.Size([961, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([961, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([961, 4, 256])
	 (after FFN) src2.shape: torch.Size([961, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([961, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.1707, -0.4021, -0.0559,  ..., -0.8663,  0.8981, -1.8553],
         [-2.5097, -1.3741, -0.0998,  ..., -0.6755,  0.8269, -2.0506],
         [-2.0601, -1.5161,  0.1186,  ..., -0.8718,  0.7917, -0.8529],
         [-2.0361, -1.1527,  0.1570,  ..., -0.8235,  0.7883, -1.1914]],

        [[-2.4635, -1.6356,  0.0910,  ..., -1.0430,  0.9029, -1.8448],
         [-2.2458, -1.6479,  0.2804,  ..., -0.6645,  0.7794, -1.9254],
         [-2.2823, -1.1707,  0.3551,  ..., -1.1485,  1.0908, -1.2194],
         [-2.0391, -1.6323,  0.1657,  ..., -0.6438,  0.9147, -1.8058]],

        [[-2.1210, -0.6057,  0.0272,  ..., -0.7211,  0.7345, -1.6692],
         [-2.2332, -1.6776,  0.2775,  ..., -0.6766,  0.8176, -1.6608],
         [-2.1586, -1.7025,  0.2216,  ..., -0.7651,  0.9686, -1.7447],
         [-1.9706, -0.6300, -0.0098,  ..., -0.7483,  1.0126, -0.8938]],

        ...,

        [[-2.3614, -1.3194, -0.5078,  ..., -0.7481,  0.5794, -1.9468],
         [-0.5750, -1.6049,  0.0439,  ..., -1.1138,  0.7357, -1.0581],
         [-0.6248, -1.5063,  0.3596,  ..., -0.9816,  0.1026, -1.4786],
         [-1.7318, -1.4341,  0.5650,  ..., -0.9699,  0.8823, -1.6391]],

        [[-1.4865, -1.4877,  0.3202,  ..., -0.7700,  0.9460, -1.5801],
         [-2.0786, -1.0733,  0.0178,  ..., -1.1608,  1.1895, -1.2077],
         [-2.3036, -1.6857,  0.0957,  ..., -0.7809,  0.7621, -1.8082],
         [-2.2088, -1.6698,  0.2907,  ..., -0.8030,  0.7733, -1.5607]],

        [[-2.2980, -1.7888,  0.0164,  ..., -0.0913, -0.0796, -1.8137],
         [-2.3323, -1.4317,  0.2901,  ..., -0.8309,  0.0551, -0.8034],
         [-2.3591, -1.9113,  0.1932,  ..., -1.0204,  0.6090, -1.7521],
         [-2.0429, -1.3211,  0.3410,  ..., -0.8395,  0.5552, -1.6295]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.7776, -0.4602,  0.0821,  ..., -0.4365,  0.6333, -0.7880],
         [-1.1914, -1.1453,  0.4344,  ..., -0.3113,  0.5398, -0.3240],
         [-1.6436, -1.1225,  0.2090,  ..., -0.3722,  0.4097, -0.3863],
         [-1.6429, -0.6425,  0.7309,  ..., -0.4312,  0.6812, -0.6703]],

        [[-1.9537, -0.6701,  0.7385,  ..., -0.5485,  0.9221, -0.5781],
         [-1.7074, -0.7913,  0.8029,  ..., -0.0916,  0.4270, -0.6726],
         [-1.8385, -0.9319,  0.5448,  ..., -0.1833,  0.8316, -0.2697],
         [-1.8229, -0.9428,  0.7240,  ..., -0.6398,  0.6591, -0.5963]],

        [[-1.7032, -0.1911,  0.3721,  ..., -0.9296,  0.5923, -0.6880],
         [-1.6819, -1.1254,  0.5619,  ..., -0.5976,  0.4489, -0.4659],
         [-1.6775, -1.1235,  0.2634,  ..., -0.2884,  0.8597, -0.6865],
         [-0.9833, -0.5440,  0.5901,  ..., -0.3570,  0.5914, -0.2683]],

        ...,

        [[-1.8191, -1.1829,  0.5548,  ..., -0.5305,  0.8162, -0.6860],
         [-1.0767, -1.2309,  0.5090,  ..., -0.7803,  0.3994, -0.4618],
         [-0.4383, -1.1484,  0.6610,  ..., -0.3728,  0.5805, -0.3072],
         [-1.6669, -0.6619,  0.6731,  ..., -0.4594,  0.6095, -0.4807]],

        [[-1.2981, -1.0554,  0.7578,  ...,  0.2357,  0.5831, -0.5660],
         [-1.6845, -0.7379,  0.6389,  ..., -0.4560,  0.9080, -0.4738],
         [-1.7053, -1.1288,  0.4744,  ..., -0.7356,  0.6888, -0.7306],
         [-1.6387, -0.8175,  0.4033,  ..., -0.5238,  0.5886, -0.7794]],

        [[-1.0967, -0.9632, -0.0021,  ..., -0.3881,  0.6098, -0.5349],
         [-1.6827, -0.8849,  0.6731,  ..., -0.2039,  0.7257, -0.4684],
         [-1.6794, -1.0064,  0.6349,  ..., -0.5382,  0.7360, -0.4794],
         [-1.6789, -1.0713,  0.6190,  ..., -0.6228,  0.7194, -0.6300]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.6011, -2.1635,  0.6458,  ..., -0.4878,  1.1041, -2.3946],
         [-1.4744, -2.3349,  0.5111,  ..., -0.4436,  1.1198, -1.7582],
         [-1.3954, -1.5295,  0.5887,  ...,  0.0623,  0.8293, -1.9180],
         [-1.8809, -1.8943,  0.3816,  ..., -0.3839,  0.2933, -2.4678]],

        [[-1.1838, -2.2028,  1.0812,  ..., -0.4459,  0.9575, -2.4727],
         [-1.8462, -1.1535,  1.0665,  ..., -0.3497,  0.8462, -2.5384],
         [-1.7398, -2.0691,  0.7460,  ..., -0.3400,  1.2293, -2.3215],
         [-1.7989, -1.5270,  0.8623,  ..., -0.9601,  0.9491, -2.3021]],

        [[-1.7586, -1.8850,  0.7477,  ..., -0.0934,  0.8593, -2.3606],
         [-1.7513, -2.2793,  0.4201,  ..., -0.2106,  1.0082, -2.0742],
         [-1.2080, -2.2200,  0.0247,  ..., -0.2550,  0.8744, -2.5054],
         [-1.8350, -2.3771,  0.9681,  ..., -0.3402,  1.0103, -2.0804]],

        ...,

        [[-1.5308, -2.2083, -0.2474,  ...,  0.0990,  0.8827, -2.1545],
         [-1.0628, -2.3423,  0.2109,  ..., -0.4494,  0.8373, -2.2417],
         [-1.3894, -2.1287,  0.8193,  ..., -0.6998,  0.9736, -1.2561],
         [-1.7914, -2.2279,  0.7281,  ..., -0.0769,  1.0030, -2.0933]],

        [[-1.5083, -1.3049,  0.8377,  ..., -0.5287,  0.7828, -2.0263],
         [-0.9372, -0.8136,  0.7309,  ..., -0.4871,  0.6843, -1.7499],
         [-1.4058, -2.4180,  0.8268,  ..., -0.4681,  0.8396, -2.2874],
         [-1.3593, -2.0322,  0.4521,  ..., -0.7238,  1.0531, -2.2172]],

        [[-1.0634, -2.2123,  0.2632,  ..., -0.3338,  0.9265, -2.0879],
         [-1.8051, -2.1477,  0.6069,  ...,  0.1558,  0.9425, -1.4249],
         [-1.4687, -2.3472,  0.6991,  ..., -0.3550,  1.0841, -0.7142],
         [-1.4711, -1.5332,  0.4455,  ..., -0.0029,  0.8035, -1.0915]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.5992, -1.7617,  0.5481,  ..., -0.9979,  1.3834, -0.0790],
         [-1.3095, -2.6244,  0.6867,  ..., -1.1739,  1.2569, -0.0779],
         [-1.7247, -2.4958,  0.3120,  ..., -1.2010,  1.1956, -0.2011],
         [-1.6286, -2.5401,  0.4868,  ..., -1.3331,  1.4261, -0.2778]],

        [[-1.5543, -2.6739,  0.5994,  ..., -1.2053,  1.4642, -0.1653],
         [-1.9546, -2.0896,  0.5841,  ..., -0.4015,  1.6205, -0.4467],
         [-1.9116, -2.6977,  0.3933,  ..., -1.1963,  1.6791,  0.1973],
         [-1.5627, -2.5711,  0.5912,  ..., -1.2583,  1.5351, -0.3251]],

        [[-1.8477, -2.6415,  0.3665,  ..., -1.1913,  0.5920, -0.3508],
         [-1.5937, -2.0648,  0.0810,  ..., -1.1113,  0.6961,  0.0127],
         [-1.4641, -2.8186,  0.1752,  ..., -0.8863,  0.8659, -0.7542],
         [-1.4844, -1.4955,  0.7427,  ..., -1.0988,  1.5510,  0.2441]],

        ...,

        [[-1.5947, -3.0138,  0.1305,  ..., -0.9032,  1.4742,  0.2127],
         [-1.4960, -2.9994,  0.2331,  ..., -0.8885,  1.1199, -0.3818],
         [-1.5283, -2.6654,  0.4481,  ..., -1.2920,  1.5278,  0.5317],
         [-1.6056, -2.6653,  0.5462,  ..., -1.1462,  1.4121, -0.0049]],

        [[-1.7295, -2.5575,  0.2760,  ..., -1.1828,  1.4812,  0.0060],
         [-0.5396, -2.0251,  0.6227,  ..., -0.4819,  1.1598,  0.0220],
         [-1.9621, -1.9127,  0.5788,  ..., -1.1888,  1.6077, -0.3619],
         [-1.3437, -2.7843,  0.0827,  ..., -1.0150,  1.3269,  0.1696]],

        [[-1.3698, -1.6884,  0.4693,  ..., -0.8646,  1.1948,  0.1186],
         [-1.8637, -2.4941,  0.4070,  ..., -1.3580,  1.6406,  0.4325],
         [-1.6258, -2.9258,  0.2617,  ..., -1.1234,  1.3429,  0.3516],
         [-1.7091, -2.6859,  0.5943,  ..., -0.8003,  0.5348,  0.6161]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3011e+00, -9.3499e-01, -5.9626e-02,  ..., -1.1779e+00,
           5.0767e-01, -1.3253e+00],
         [-1.9249e+00, -1.1327e+00, -2.6142e-01,  ..., -1.7564e+00,
           1.9556e-01, -1.2865e+00],
         [-1.8205e+00, -9.1176e-01, -1.3891e-01,  ..., -1.6758e+00,
           6.9917e-02, -1.2146e+00],
         [-2.3002e+00, -7.7228e-01, -1.4373e-01,  ..., -1.2899e+00,
           3.2625e-01, -1.3202e+00]],

        [[-2.0777e+00, -1.1791e+00, -3.1569e-01,  ..., -1.5777e+00,
           6.3894e-01, -1.3358e+00],
         [-1.5713e+00, -8.5633e-01, -2.5268e-01,  ..., -1.7046e+00,
           6.3029e-01, -1.3300e+00],
         [-2.3867e+00, -1.1464e+00, -4.6441e-01,  ..., -1.7636e+00,
           5.9758e-01, -1.1679e+00],
         [-2.0819e+00, -1.0918e+00, -1.4662e-01,  ..., -1.1433e+00,
           2.8530e-01, -1.2384e+00]],

        [[-2.4661e+00, -1.5197e+00, -3.8484e-01,  ..., -1.3151e+00,
           4.3979e-01, -1.5160e+00],
         [-2.2994e+00, -8.1158e-01, -2.8910e-01,  ..., -1.8135e+00,
           4.2663e-02, -1.1371e+00],
         [-1.9737e+00, -1.2936e+00, -2.7428e-01,  ..., -1.4124e+00,
           1.8528e-01, -1.5270e+00],
         [-1.3371e+00, -7.4444e-01, -1.3387e-01,  ..., -1.6341e+00,
           2.5356e-01, -9.0954e-01]],

        ...,

        [[-1.9949e+00, -1.5224e+00, -1.0454e-01,  ..., -1.1281e+00,
           5.5732e-01, -1.2489e+00],
         [-2.2790e+00, -1.0382e+00, -4.0019e-01,  ..., -4.7491e-01,
           4.4688e-01, -1.2266e+00],
         [-1.9779e+00, -1.0970e+00, -1.5178e-01,  ..., -1.7025e+00,
           5.8725e-01, -1.1322e+00],
         [-1.6704e+00, -8.9635e-01, -3.5763e-01,  ..., -1.4645e+00,
           7.3354e-01, -1.1691e+00]],

        [[-1.7867e+00, -1.2644e+00, -5.9940e-02,  ..., -1.5074e+00,
           4.3221e-01, -1.0059e+00],
         [-1.8872e+00, -1.1098e+00, -2.0911e-01,  ..., -1.2883e+00,
           5.0390e-01, -1.4355e+00],
         [-2.3314e+00, -9.6175e-01, -1.6996e-01,  ..., -1.3788e+00,
           5.2778e-01, -3.8819e-01],
         [-2.2386e+00, -1.3555e+00, -2.4254e-01,  ..., -1.6144e+00,
           3.7584e-01, -1.0568e+00]],

        [[-6.7928e-01, -6.4040e-01,  2.7506e-01,  ..., -2.9021e-01,
           3.8176e-01, -7.2067e-01],
         [-2.4207e+00, -1.0953e+00,  4.4148e-01,  ..., -1.4559e+00,
           4.0354e-01, -1.1891e+00],
         [-2.0126e+00, -1.1318e+00, -8.7759e-02,  ..., -1.4989e+00,
           1.4799e-01, -1.0248e+00],
         [-1.9841e+00, -1.0057e+00,  4.6840e-02,  ..., -8.0942e-01,
          -4.2783e-04, -9.0880e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([961, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
self.query_embed.weight.shape : torch.Size([100, 256])
src.shape: torch.Size([1428, 4, 256]), pos_embed.shape: torch.Size([1428, 4, 256]), query_embed.shape: torch.Size([100, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
encoder start :
	 src.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([1428, 4, 256]), k.shape: torch.Size([1428, 4, 256])
	 (after Multi-Head Self-Attention) src2.shape: torch.Size([1428, 4, 256])
	 (after dropout1 & LayerNorm) src.shape: torch.Size([1428, 4, 256])
	 (after FFN) src2.shape: torch.Size([1428, 4, 256])
	 (encoder final output = decoder's Multi-Head Attention's value & key)
		torch.Size([1428, 4, 256])
tgt : tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-2.3363, -1.4050,  0.0571,  ..., -0.7325,  0.9858, -1.9405],
         [-1.5361, -1.3422,  0.2812,  ..., -0.8670,  0.8513, -1.9654],
         [-2.1905, -0.7154,  0.1443,  ..., -1.0202,  1.1699, -1.8437],
         [-1.9221, -1.5715, -0.0611,  ..., -0.5846,  0.7983, -0.8392]],

        [[-2.2455, -1.6142,  0.0640,  ..., -0.5157,  0.7411, -1.8016],
         [-1.8649, -1.7691,  0.3574,  ..., -1.1543,  0.7238, -1.6394],
         [-2.2261, -1.8021,  0.1275,  ..., -0.7317,  0.9293, -1.6402],
         [-2.2123, -1.5662,  0.3304,  ..., -1.1858,  0.7205, -0.9069]],

        [[-2.1499, -1.3107,  0.1872,  ..., -0.7029,  0.9159, -1.2540],
         [-2.1130, -1.1700,  0.1673,  ..., -1.0502,  0.6002, -1.7052],
         [-2.1849, -1.7727, -0.1061,  ..., -0.5978,  0.6897, -1.6719],
         [-1.9964, -1.5606,  0.0974,  ..., -0.7319,  0.7909, -1.6790]],

        ...,

        [[-2.4042, -1.6594,  0.0405,  ..., -0.8957,  1.0296, -1.0687],
         [-2.1867, -1.7842,  0.3832,  ..., -0.9080,  1.0023, -0.8474],
         [-1.5211, -1.2693,  0.2817,  ..., -0.9959,  0.8929, -1.4327],
         [-0.3962, -1.4632,  0.0116,  ..., -0.2427,  0.6306, -1.7774]],

        [[-1.9384, -0.6898,  0.3356,  ..., -0.8918,  0.9358, -1.9503],
         [-2.1359, -1.7177,  0.1929,  ..., -1.0613,  0.6775, -2.0219],
         [-1.6134, -1.3765,  0.5670,  ..., -1.0880,  0.9431, -1.6072],
         [-2.2280, -1.6994,  0.1812,  ..., -0.7325,  0.8175, -1.2050]],

        [[-0.0063, -1.5838, -0.1236,  ..., -0.8461,  0.6629, -1.3606],
         [-2.3906, -1.6888, -0.1534,  ..., -1.0116,  0.5974, -1.5942],
         [-2.1179, -1.2936,  0.2217,  ..., -0.7072,  0.5882, -1.3346],
         [-1.9320, -1.4884,  0.2726,  ..., -0.6047,  0.8976, -1.7391]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.9057, -0.8774,  0.3170,  ..., -0.4818,  0.7735, -0.9803],
         [-1.4128, -0.5830,  0.3506,  ..., -0.3291,  0.2789, -0.7735],
         [-1.8674, -0.8561,  0.4984,  ..., -0.6158,  0.9154, -1.0591],
         [-1.9986, -0.9131,  0.4985,  ..., -0.5914,  0.5261, -0.1632]],

        [[-1.8014, -1.0162,  0.4597,  ..., -0.3726,  0.8665, -0.7249],
         [-1.3569, -1.2771,  0.8495,  ..., -0.6489,  0.3643, -0.6356],
         [-1.0233, -0.9417,  0.5588,  ..., -0.5170,  0.7093, -0.7540],
         [-1.7080, -0.5788,  0.2566,  ..., -0.7997,  0.4494, -0.3294]],

        [[-1.7821, -0.8718,  0.6949,  ..., -0.2534,  0.6172, -0.5879],
         [-1.6495, -0.9729,  0.5145,  ..., -0.7447,  0.4604, -0.8661],
         [-1.8401, -0.9282,  0.7109,  ...,  0.0285,  0.5074, -0.6554],
         [-1.7626, -0.5798,  0.7428,  ..., -0.3416,  0.7880, -0.7710]],

        ...,

        [[-1.8723, -0.8494,  0.1510,  ..., -0.4632,  0.8506, -0.5274],
         [-1.7588, -1.0105,  0.7558,  ..., -0.4225,  0.7211, -0.1668],
         [-1.4008, -0.4140,  0.6716,  ..., -0.4519,  0.4979, -0.8980],
         [-1.1613, -0.8019,  0.3060,  ..., -0.1080,  0.7320, -0.9242]],

        [[-1.8588, -0.6934,  0.6043,  ..., -0.3124,  1.0215, -0.5561],
         [-1.4484, -0.6583,  0.6875,  ..., -0.7823,  0.6022, -0.7721],
         [-1.6187, -0.6189,  0.6317,  ..., -0.5367,  0.6906, -0.6324],
         [-1.9757, -0.8721,  0.5709,  ..., -0.2906,  0.6564, -0.1529]],

        [[-0.9572, -0.8823,  0.5242,  ..., -0.4806,  0.6634, -0.4185],
         [-1.5144, -0.8575,  0.5432,  ..., -0.4283,  0.4584, -0.4477],
         [-1.6225, -0.7773,  0.3490,  ..., -0.3042,  0.5471, -0.3450],
         [-1.7550, -0.7859,  0.2368,  ...,  0.3110,  0.7251, -0.3269]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.0427e+00, -2.3271e+00,  1.3578e-01,  ...,  9.5750e-02,
           1.1326e+00, -2.2563e+00],
         [-1.1512e+00, -2.2572e+00,  5.4150e-01,  ..., -4.6828e-01,
           7.9959e-01, -1.8264e+00],
         [-1.6303e+00, -2.3761e+00,  5.2390e-01,  ..., -5.4156e-01,
           1.0912e+00, -2.7646e+00],
         [-1.8569e+00, -2.4080e+00,  4.3012e-01,  ..., -5.2584e-01,
           1.1068e+00, -1.5038e+00]],

        [[-1.9620e+00, -1.4075e+00, -7.8288e-02,  ...,  1.3605e-01,
           1.2439e+00, -2.3829e+00],
         [-9.2189e-01, -2.4483e+00,  8.1735e-01,  ..., -4.1547e-01,
           6.9709e-01, -2.1665e+00],
         [-1.2738e+00, -2.1988e+00,  5.8732e-01,  ..., -6.2046e-01,
           8.6128e-01, -2.2170e+00],
         [-1.8410e+00, -1.1798e+00, -1.0293e-01,  ...,  2.4402e-03,
           1.3393e-01, -2.1160e+00]],

        [[-1.3954e+00, -1.1367e+00,  7.1358e-01,  ..., -4.6862e-01,
           7.6644e-01, -2.2309e+00],
         [-1.2770e+00, -1.1735e+00,  9.3146e-01,  ..., -9.1434e-01,
           7.8252e-01, -1.8684e+00],
         [-1.6784e+00, -2.3256e+00,  4.0918e-01,  ..., -4.8475e-01,
           8.9481e-01, -1.7285e+00],
         [-1.4937e+00, -2.2067e+00,  7.1107e-01,  ..., -3.6750e-01,
           1.0301e+00, -2.1150e+00]],

        ...,

        [[-1.1589e+00, -2.1800e+00,  4.5551e-01,  ..., -8.7398e-02,
           1.2483e+00, -2.0859e+00],
         [-1.3477e+00, -2.4234e+00,  6.0311e-01,  ..., -4.0796e-01,
           8.3134e-01, -1.9337e+00],
         [-1.2789e+00, -1.0954e+00,  9.7889e-01,  ..., -6.0965e-01,
           9.8613e-01, -2.4118e+00],
         [-9.0861e-01, -1.2340e+00,  2.5071e-01,  ..., -3.2387e-01,
           1.0061e+00, -2.6713e+00]],

        [[-1.6764e+00, -2.1681e+00,  7.9306e-01,  ..., -2.5372e-01,
           1.2915e+00, -1.6215e+00],
         [-1.8305e+00, -2.1254e+00,  6.8788e-01,  ..., -2.7243e-01,
           1.1825e+00, -7.6469e-01],
         [-2.2917e+00, -1.9724e+00,  7.9763e-01,  ..., -3.6682e-01,
           1.0002e+00, -2.2829e+00],
         [-1.6241e+00, -1.2288e+00,  8.1413e-01,  ..., -3.4124e-01,
           9.8729e-01, -1.0880e+00]],

        [[-1.4840e+00, -1.2712e+00,  7.0272e-01,  ..., -5.0327e-01,
           7.8534e-01, -2.4096e+00],
         [-1.3650e+00, -2.0441e+00,  7.6445e-01,  ..., -4.9891e-01,
           9.1523e-01, -2.1304e+00],
         [-2.0045e+00, -2.2839e+00,  7.0822e-01,  ..., -1.7530e-01,
           5.0467e-01, -2.1062e+00],
         [-1.9778e+00, -2.4075e+00,  8.0688e-01,  ...,  5.4152e-02,
           1.1523e+00, -1.8681e+00]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-0.7763, -2.8964,  0.2428,  ..., -0.6266,  1.6034, -0.5280],
         [-1.4613, -2.9982,  0.2845,  ..., -1.2763,  1.3549,  0.1701],
         [-1.8447, -2.4624,  0.6809,  ..., -1.1349,  1.4813, -0.4804],
         [-1.8468, -2.0803,  0.3880,  ..., -1.0644,  1.6920,  0.1115]],

        [[-1.9512, -2.4581,  0.1314,  ..., -0.6574,  1.5307, -0.2473],
         [-1.5002, -1.9057,  0.8021,  ..., -1.1033,  1.2476, -0.1231],
         [-0.6512, -2.5763,  0.2063,  ..., -1.0963,  1.2825,  0.1012],
         [-1.7840, -2.2756,  0.3506,  ..., -0.7731,  1.2707, -0.2642]],

        [[-0.6728, -1.3722,  0.3753,  ..., -1.1545,  1.5383,  0.1242],
         [-2.0839, -2.3671,  0.8460,  ..., -1.3876,  1.3924, -0.3255],
         [-1.6261, -1.7587,  0.5742,  ..., -1.1778,  1.3271,  0.0085],
         [-1.7318, -2.8630,  0.6873,  ..., -1.0507,  0.6472, -0.2976]],

        ...,

        [[-1.9132, -2.7191,  0.2555,  ..., -0.8422,  1.2009, -0.0527],
         [-1.5219, -2.8672,  0.5946,  ..., -1.0438,  1.3268,  0.0311],
         [-1.5046, -2.5022,  0.5966,  ..., -1.0634,  1.5873, -0.3542],
         [-1.0070, -2.6831,  0.4761,  ..., -0.2740,  0.8544, -0.3545]],

        [[-1.7817, -2.7341,  0.2627,  ..., -0.9663,  1.5385,  0.0091],
         [-1.6939, -2.8267,  0.7594,  ..., -1.1574,  1.4685,  0.4365],
         [-1.8399, -2.9126,  0.6249,  ..., -1.0800,  1.1509, -0.3468],
         [-2.1717, -1.9799,  0.7006,  ..., -0.2479,  1.3580,  0.4795]],

        [[-1.8931, -2.4528,  0.5408,  ..., -0.9476,  1.5486, -0.3116],
         [-1.8367, -2.6808,  0.6200,  ..., -1.2350,  0.8869, -0.1814],
         [-1.9056, -2.8639,  0.4563,  ..., -0.8237,  1.3326,  0.0510],
         [-2.0549, -3.0764,  0.6374,  ..., -0.7447,  1.6819,  0.0886]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
decoder start :
	 tgt: tensor([[[-1.9208, -1.7140,  0.0394,  ..., -0.5837,  0.7129, -1.4100],
         [-2.4273, -1.2721, -0.3760,  ..., -1.8024,  0.3449, -0.7315],
         [-1.0764, -1.1901, -0.0723,  ..., -1.6020, -0.0166, -1.7966],
         [-2.2064, -0.7868, -0.0718,  ..., -1.3763,  0.4052, -0.9966]],

        [[-2.2381, -1.1342, -0.2945,  ..., -1.3388,  0.1590, -1.3594],
         [-2.5158, -1.0613,  0.1361,  ..., -1.4901,  0.3976, -1.5208],
         [-1.9906, -1.0827, -0.3703,  ..., -1.4114,  0.4470, -0.2121],
         [-2.2752, -0.9421, -0.4516,  ..., -1.3090,  0.4362, -1.2057]],

        [[-2.0963, -0.9176,  0.2611,  ..., -1.5256,  0.5692, -0.6857],
         [-1.9657, -0.6905,  0.0902,  ..., -1.6885,  0.3625, -1.4368],
         [-1.2355, -1.4411, -0.0550,  ..., -1.2766,  0.1480, -1.4342],
         [-2.3620, -1.5824, -0.1761,  ..., -1.1932, -0.1596, -1.3547]],

        ...,

        [[-1.9987, -1.2869, -0.4344,  ..., -1.3947,  0.3995, -1.1758],
         [-2.3338, -1.1598,  0.5763,  ..., -1.5132,  0.7809, -1.2681],
         [-2.1592, -1.3425, -0.1602,  ..., -0.4533,  0.4631, -1.3536],
         [-1.7424, -1.2559, -0.1778,  ..., -0.4683,  0.1228, -1.2342]],

        [[-2.2814, -1.1792, -0.0137,  ..., -1.3827,  0.3195, -1.3345],
         [-1.8941, -1.4813, -0.2296,  ..., -1.5720,  0.7862, -0.9701],
         [-2.2571, -1.3440, -0.3664,  ..., -1.7011,  0.2809, -0.3769],
         [-2.2186, -1.0115, -0.0456,  ..., -1.0904,  0.1043, -1.0944]],

        [[-2.4292, -0.9877, -0.0921,  ..., -1.1898,  0.5363, -1.2177],
         [-2.1583, -1.2535, -0.1695,  ..., -1.3786,  0.0526, -0.8036],
         [-1.4569, -1.5554,  0.4370,  ..., -1.3043,  0.4246, -1.1118],
         [-1.8028, -1.0996, -0.1244,  ..., -1.3348,  0.2386, -1.5657]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)
	 tgt.shape: torch.Size([100, 4, 256]), memory.shape: torch.Size([1428, 4, 256])
	 q.shape: torch.Size([100, 4, 256]), k.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Self-Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout1 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after Multi-Head Attention) tgt2.shape: torch.Size([100, 4, 256])
	 (after dropout2 & LayerNorm) tgt.shape: torch.Size([100, 4, 256])
	 (after FFN) tgt2.shape: torch.Size([100, 4, 256])
	 (decoder final output) tgt.shape: torch.Size([100, 4, 256])
hs.shape: torch.Size([6, 100, 4, 256])
