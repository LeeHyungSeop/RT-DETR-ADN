WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
make layer i: 0, num_blocks: 3
make layer i: 1, num_blocks: 4
make layer i: 2, num_blocks: 6
make layer i: 3, num_blocks: 3
Load state_dict from https://download.pytorch.org/models/resnet50-0676ba61.pth
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d(64, eps=1e-05)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(256, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(512, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(1024, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(2048, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
start creating model... (in yaml_config.py)
start creating model... (in yaml_config.py)
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.22s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
number of params: 42862860
super_config : [False, False, False, False]
base_config : [True, True, True, True]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch: [0]  [    0/14786]  eta: 8:28:57  lr: 0.000010  loss: 50.6369 (50.6369)  loss_bbox: 2.1460 (2.1460)  loss_bbox_aux_0: 2.1231 (2.1231)  loss_bbox_aux_1: 2.1455 (2.1455)  loss_bbox_aux_2: 2.1372 (2.1372)  loss_bbox_aux_3: 2.2045 (2.2045)  loss_bbox_aux_4: 2.1236 (2.1236)  loss_bbox_aux_5: 2.1894 (2.1894)  loss_bbox_dn_0: 1.2247 (1.2247)  loss_bbox_dn_1: 1.2247 (1.2247)  loss_bbox_dn_2: 1.2247 (1.2247)  loss_bbox_dn_3: 1.2247 (1.2247)  loss_bbox_dn_4: 1.2247 (1.2247)  loss_bbox_dn_5: 1.2247 (1.2247)  loss_giou: 1.8545 (1.8545)  loss_giou_aux_0: 1.8542 (1.8542)  loss_giou_aux_1: 1.8086 (1.8086)  loss_giou_aux_2: 1.8254 (1.8254)  loss_giou_aux_3: 1.8373 (1.8373)  loss_giou_aux_4: 1.8305 (1.8305)  loss_giou_aux_5: 1.8126 (1.8126)  loss_giou_dn_0: 1.3373 (1.3373)  loss_giou_dn_1: 1.3373 (1.3373)  loss_giou_dn_2: 1.3373 (1.3373)  loss_giou_dn_3: 1.3373 (1.3373)  loss_giou_dn_4: 1.3373 (1.3373)  loss_giou_dn_5: 1.3373 (1.3373)  loss_vfl: 0.3341 (0.3341)  loss_vfl_aux_0: 0.3562 (0.3562)  loss_vfl_aux_1: 0.3329 (0.3329)  loss_vfl_aux_2: 0.3416 (0.3416)  loss_vfl_aux_3: 0.3264 (0.3264)  loss_vfl_aux_4: 0.3239 (0.3239)  loss_vfl_aux_5: 0.3388 (0.3388)  loss_vfl_dn_0: 0.8579 (0.8579)  loss_vfl_dn_1: 0.8335 (0.8335)  loss_vfl_dn_2: 0.8350 (0.8350)  loss_vfl_dn_3: 0.8292 (0.8292)  loss_vfl_dn_4: 0.8307 (0.8307)  loss_vfl_dn_5: 0.8326 (0.8326)  time: 2.0653  data: 0.5321  max mem: 3198
Epoch: [0]  [  100/14786]  eta: 1:10:04  lr: 0.000010  loss: 38.2287 (41.2023)  loss_bbox: 1.0643 (1.2907)  loss_bbox_aux_0: 1.1005 (1.3449)  loss_bbox_aux_1: 1.0832 (1.3219)  loss_bbox_aux_2: 1.0690 (1.3068)  loss_bbox_aux_3: 1.0656 (1.3013)  loss_bbox_aux_4: 1.0615 (1.2934)  loss_bbox_aux_5: 1.1112 (1.3935)  loss_bbox_dn_0: 0.7164 (0.8706)  loss_bbox_dn_1: 0.7356 (0.8827)  loss_bbox_dn_2: 0.7449 (0.8943)  loss_bbox_dn_3: 0.7488 (0.9025)  loss_bbox_dn_4: 0.7521 (0.9100)  loss_bbox_dn_5: 0.7543 (0.9168)  loss_giou: 1.5125 (1.5493)  loss_giou_aux_0: 1.5242 (1.5818)  loss_giou_aux_1: 1.5310 (1.5661)  loss_giou_aux_2: 1.5200 (1.5593)  loss_giou_aux_3: 1.5208 (1.5545)  loss_giou_aux_4: 1.5190 (1.5522)  loss_giou_aux_5: 1.5426 (1.6088)  loss_giou_dn_0: 1.3337 (1.3368)  loss_giou_dn_1: 1.3413 (1.3403)  loss_giou_dn_2: 1.3407 (1.3459)  loss_giou_dn_3: 1.3407 (1.3514)  loss_giou_dn_4: 1.3407 (1.3572)  loss_giou_dn_5: 1.3415 (1.3626)  loss_vfl: 0.6453 (0.5677)  loss_vfl_aux_0: 0.5717 (0.5107)  loss_vfl_aux_1: 0.5999 (0.5313)  loss_vfl_aux_2: 0.6175 (0.5439)  loss_vfl_aux_3: 0.6116 (0.5516)  loss_vfl_aux_4: 0.6306 (0.5615)  loss_vfl_aux_5: 0.5266 (0.4787)  loss_vfl_dn_0: 0.5682 (0.6393)  loss_vfl_dn_1: 0.5589 (0.6243)  loss_vfl_dn_2: 0.5666 (0.6260)  loss_vfl_dn_3: 0.5809 (0.6278)  loss_vfl_dn_4: 0.5777 (0.6270)  loss_vfl_dn_5: 0.5750 (0.6174)  time: 0.2650  data: 0.0096  max mem: 5569
Epoch: [0]  [  200/14786]  eta: 1:06:51  lr: 0.000010  loss: 37.7093 (39.8506)  loss_bbox: 0.8553 (1.1202)  loss_bbox_aux_0: 0.8551 (1.1597)  loss_bbox_aux_1: 0.8720 (1.1423)  loss_bbox_aux_2: 0.8584 (1.1314)  loss_bbox_aux_3: 0.8518 (1.1279)  loss_bbox_aux_4: 0.8543 (1.1216)  loss_bbox_aux_5: 0.8604 (1.2024)  loss_bbox_dn_0: 0.8049 (0.8607)  loss_bbox_dn_1: 0.8081 (0.8662)  loss_bbox_dn_2: 0.8079 (0.8714)  loss_bbox_dn_3: 0.8067 (0.8751)  loss_bbox_dn_4: 0.8060 (0.8788)  loss_bbox_dn_5: 0.8047 (0.8820)  loss_giou: 1.4151 (1.5016)  loss_giou_aux_0: 1.4116 (1.5250)  loss_giou_aux_1: 1.4102 (1.5142)  loss_giou_aux_2: 1.4031 (1.5083)  loss_giou_aux_3: 1.4131 (1.5039)  loss_giou_aux_4: 1.4143 (1.5032)  loss_giou_aux_5: 1.4080 (1.5471)  loss_giou_dn_0: 1.3337 (1.3384)  loss_giou_dn_1: 1.3387 (1.3415)  loss_giou_dn_2: 1.3396 (1.3468)  loss_giou_dn_3: 1.3395 (1.3521)  loss_giou_dn_4: 1.3455 (1.3571)  loss_giou_dn_5: 1.3501 (1.3625)  loss_vfl: 0.8111 (0.6638)  loss_vfl_aux_0: 0.6949 (0.5942)  loss_vfl_aux_1: 0.7370 (0.6229)  loss_vfl_aux_2: 0.7455 (0.6390)  loss_vfl_aux_3: 0.7874 (0.6480)  loss_vfl_aux_4: 0.7966 (0.6568)  loss_vfl_aux_5: 0.6794 (0.5515)  loss_vfl_dn_0: 0.4935 (0.5808)  loss_vfl_dn_1: 0.4972 (0.5810)  loss_vfl_dn_2: 0.5383 (0.5897)  loss_vfl_dn_3: 0.5546 (0.5960)  loss_vfl_dn_4: 0.5545 (0.5968)  loss_vfl_dn_5: 0.5423 (0.5890)  time: 0.2616  data: 0.0094  max mem: 5569
Epoch: [0]  [  300/14786]  eta: 1:05:14  lr: 0.000010  loss: 39.0916 (39.3369)  loss_bbox: 0.9595 (1.0467)  loss_bbox_aux_0: 0.9993 (1.0924)  loss_bbox_aux_1: 0.9992 (1.0743)  loss_bbox_aux_2: 0.9723 (1.0596)  loss_bbox_aux_3: 0.9639 (1.0536)  loss_bbox_aux_4: 0.9633 (1.0477)  loss_bbox_aux_5: 1.0172 (1.1319)  loss_bbox_dn_0: 0.8686 (0.8758)  loss_bbox_dn_1: 0.8659 (0.8775)  loss_bbox_dn_2: 0.8670 (0.8799)  loss_bbox_dn_3: 0.8704 (0.8823)  loss_bbox_dn_4: 0.8721 (0.8847)  loss_bbox_dn_5: 0.8746 (0.8869)  loss_giou: 1.3381 (1.4551)  loss_giou_aux_0: 1.3799 (1.4843)  loss_giou_aux_1: 1.3605 (1.4714)  loss_giou_aux_2: 1.3380 (1.4645)  loss_giou_aux_3: 1.3304 (1.4589)  loss_giou_aux_4: 1.3342 (1.4574)  loss_giou_aux_5: 1.3998 (1.5075)  loss_giou_dn_0: 1.3318 (1.3364)  loss_giou_dn_1: 1.3120 (1.3389)  loss_giou_dn_2: 1.3211 (1.3441)  loss_giou_dn_3: 1.3349 (1.3495)  loss_giou_dn_4: 1.3351 (1.3541)  loss_giou_dn_5: 1.3415 (1.3587)  loss_vfl: 0.8418 (0.7318)  loss_vfl_aux_0: 0.6871 (0.6419)  loss_vfl_aux_1: 0.7256 (0.6747)  loss_vfl_aux_2: 0.7786 (0.6953)  loss_vfl_aux_3: 0.8030 (0.7114)  loss_vfl_aux_4: 0.8313 (0.7259)  loss_vfl_aux_5: 0.6812 (0.5935)  loss_vfl_dn_0: 0.4534 (0.5429)  loss_vfl_dn_1: 0.4946 (0.5518)  loss_vfl_dn_2: 0.5192 (0.5660)  loss_vfl_dn_3: 0.5439 (0.5756)  loss_vfl_dn_4: 0.5411 (0.5777)  loss_vfl_dn_5: 0.5521 (0.5743)  time: 0.2621  data: 0.0091  max mem: 5569
Epoch: [0]  [  400/14786]  eta: 1:06:05  lr: 0.000010  loss: 35.6952 (38.8034)  loss_bbox: 0.7555 (0.9830)  loss_bbox_aux_0: 0.7799 (1.0371)  loss_bbox_aux_1: 0.7855 (1.0120)  loss_bbox_aux_2: 0.7751 (0.9965)  loss_bbox_aux_3: 0.7634 (0.9904)  loss_bbox_aux_4: 0.7670 (0.9846)  loss_bbox_aux_5: 0.8352 (1.0769)  loss_bbox_dn_0: 0.7594 (0.8693)  loss_bbox_dn_1: 0.7390 (0.8672)  loss_bbox_dn_2: 0.7254 (0.8678)  loss_bbox_dn_3: 0.7214 (0.8694)  loss_bbox_dn_4: 0.7212 (0.8712)  loss_bbox_dn_5: 0.7209 (0.8728)  loss_giou: 1.3231 (1.4247)  loss_giou_aux_0: 1.3575 (1.4607)  loss_giou_aux_1: 1.3436 (1.4420)  loss_giou_aux_2: 1.3426 (1.4330)  loss_giou_aux_3: 1.3385 (1.4278)  loss_giou_aux_4: 1.3251 (1.4262)  loss_giou_aux_5: 1.4075 (1.4860)  loss_giou_dn_0: 1.3275 (1.3344)  loss_giou_dn_1: 1.3228 (1.3344)  loss_giou_dn_2: 1.3105 (1.3389)  loss_giou_dn_3: 1.3237 (1.3439)  loss_giou_dn_4: 1.3217 (1.3480)  loss_giou_dn_5: 1.3195 (1.3521)  loss_vfl: 0.8296 (0.7782)  loss_vfl_aux_0: 0.6883 (0.6672)  loss_vfl_aux_1: 0.6898 (0.7075)  loss_vfl_aux_2: 0.7473 (0.7337)  loss_vfl_aux_3: 0.8090 (0.7523)  loss_vfl_aux_4: 0.8083 (0.7693)  loss_vfl_aux_5: 0.6412 (0.6176)  loss_vfl_dn_0: 0.4452 (0.5210)  loss_vfl_dn_1: 0.4816 (0.5380)  loss_vfl_dn_2: 0.5308 (0.5568)  loss_vfl_dn_3: 0.5528 (0.5685)  loss_vfl_dn_4: 0.5513 (0.5720)  loss_vfl_dn_5: 0.5634 (0.5709)  time: 0.2685  data: 0.0100  max mem: 5569
Epoch: [0]  [  500/14786]  eta: 1:05:05  lr: 0.000010  loss: 37.5669 (38.3615)  loss_bbox: 0.7597 (0.9392)  loss_bbox_aux_0: 0.8400 (0.9937)  loss_bbox_aux_1: 0.8222 (0.9678)  loss_bbox_aux_2: 0.7930 (0.9519)  loss_bbox_aux_3: 0.7772 (0.9463)  loss_bbox_aux_4: 0.7655 (0.9411)  loss_bbox_aux_5: 0.9162 (1.0396)  loss_bbox_dn_0: 0.8985 (0.8615)  loss_bbox_dn_1: 0.8845 (0.8563)  loss_bbox_dn_2: 0.8829 (0.8553)  loss_bbox_dn_3: 0.8828 (0.8562)  loss_bbox_dn_4: 0.8836 (0.8575)  loss_bbox_dn_5: 0.8838 (0.8588)  loss_giou: 1.2543 (1.4035)  loss_giou_aux_0: 1.3600 (1.4413)  loss_giou_aux_1: 1.2980 (1.4212)  loss_giou_aux_2: 1.2638 (1.4125)  loss_giou_aux_3: 1.2510 (1.4072)  loss_giou_aux_4: 1.2558 (1.4051)  loss_giou_aux_5: 1.3939 (1.4726)  loss_giou_dn_0: 1.3137 (1.3322)  loss_giou_dn_1: 1.2936 (1.3293)  loss_giou_dn_2: 1.2824 (1.3322)  loss_giou_dn_3: 1.2780 (1.3363)  loss_giou_dn_4: 1.2837 (1.3398)  loss_giou_dn_5: 1.2882 (1.3432)  loss_vfl: 0.8968 (0.8069)  loss_vfl_aux_0: 0.7742 (0.6859)  loss_vfl_aux_1: 0.8166 (0.7274)  loss_vfl_aux_2: 0.8582 (0.7549)  loss_vfl_aux_3: 0.8792 (0.7751)  loss_vfl_aux_4: 0.8876 (0.7937)  loss_vfl_aux_5: 0.6986 (0.6314)  loss_vfl_dn_0: 0.4402 (0.5054)  loss_vfl_dn_1: 0.4926 (0.5273)  loss_vfl_dn_2: 0.5194 (0.5498)  loss_vfl_dn_3: 0.5403 (0.5638)  loss_vfl_dn_4: 0.5586 (0.5689)  loss_vfl_dn_5: 0.5631 (0.5693)  time: 0.2640  data: 0.0094  max mem: 5569
