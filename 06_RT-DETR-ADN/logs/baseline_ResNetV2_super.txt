WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
make layer i: 0, num_blocks: 3
make layer i: 1, num_blocks: 4
make layer i: 2, num_blocks: 6
make layer i: 3, num_blocks: 3
Load state_dict from https://download.pytorch.org/models/resnet50-11ad3fa6.pth
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d(64, eps=1e-05)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(256, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(512, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(1024, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(2048, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.26s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
module.backbone.layer1.0.conv1.weight 4096
module.backbone.layer1.0.conv2.weight 36864
module.backbone.layer1.0.conv3.weight 16384
module.backbone.layer1.0.downsample.0.weight 16384
module.backbone.layer1.1.conv1.weight 16384
module.backbone.layer1.1.conv2.weight 36864
module.backbone.layer1.1.conv3.weight 16384
module.backbone.layer1.2.conv1.weight 16384
module.backbone.layer1.2.conv2.weight 36864
module.backbone.layer1.2.conv3.weight 16384
module.backbone.layer2.0.conv1.weight 32768
module.backbone.layer2.0.conv2.weight 147456
module.backbone.layer2.0.conv3.weight 65536
module.backbone.layer2.0.downsample.0.weight 131072
module.backbone.layer2.1.conv1.weight 65536
module.backbone.layer2.1.conv2.weight 147456
module.backbone.layer2.1.conv3.weight 65536
module.backbone.layer2.2.conv1.weight 65536
module.backbone.layer2.2.conv2.weight 147456
module.backbone.layer2.2.conv3.weight 65536
module.backbone.layer2.3.conv1.weight 65536
module.backbone.layer2.3.conv2.weight 147456
module.backbone.layer2.3.conv3.weight 65536
module.backbone.layer3.0.conv1.weight 131072
module.backbone.layer3.0.conv2.weight 589824
module.backbone.layer3.0.conv3.weight 262144
module.backbone.layer3.0.downsample.0.weight 524288
module.backbone.layer3.1.conv1.weight 262144
module.backbone.layer3.1.conv2.weight 589824
module.backbone.layer3.1.conv3.weight 262144
module.backbone.layer3.2.conv1.weight 262144
module.backbone.layer3.2.conv2.weight 589824
module.backbone.layer3.2.conv3.weight 262144
module.backbone.layer3.3.conv1.weight 262144
module.backbone.layer3.3.conv2.weight 589824
module.backbone.layer3.3.conv3.weight 262144
module.backbone.layer3.4.conv1.weight 262144
module.backbone.layer3.4.conv2.weight 589824
module.backbone.layer3.4.conv3.weight 262144
module.backbone.layer3.5.conv1.weight 262144
module.backbone.layer3.5.conv2.weight 589824
module.backbone.layer3.5.conv3.weight 262144
module.backbone.layer4.0.conv1.weight 524288
module.backbone.layer4.0.conv2.weight 2359296
module.backbone.layer4.0.conv3.weight 1048576
module.backbone.layer4.0.downsample.0.weight 2097152
module.backbone.layer4.1.conv1.weight 1048576
module.backbone.layer4.1.conv2.weight 2359296
module.backbone.layer4.1.conv3.weight 1048576
module.backbone.layer4.2.conv1.weight 1048576
module.backbone.layer4.2.conv2.weight 2359296
module.backbone.layer4.2.conv3.weight 1048576
module.decoder.input_proj.0.conv.weight 65536
module.decoder.input_proj.0.norm.weight 256
module.decoder.input_proj.0.norm.bias 256
module.decoder.input_proj.1.conv.weight 65536
module.decoder.input_proj.1.norm.weight 256
module.decoder.input_proj.1.norm.bias 256
module.decoder.input_proj.2.conv.weight 65536
module.decoder.input_proj.2.norm.weight 256
module.decoder.input_proj.2.norm.bias 256
module.decoder.decoder.layers.0.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.0.self_attn.in_proj_bias 768
module.decoder.decoder.layers.0.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.0.self_attn.out_proj.bias 256
module.decoder.decoder.layers.0.norm1.weight 256
module.decoder.decoder.layers.0.norm1.bias 256
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.0.norm2.weight 256
module.decoder.decoder.layers.0.norm2.bias 256
module.decoder.decoder.layers.0.linear1.weight 262144
module.decoder.decoder.layers.0.linear1.bias 1024
module.decoder.decoder.layers.0.linear2.weight 262144
module.decoder.decoder.layers.0.linear2.bias 256
module.decoder.decoder.layers.0.norm3.weight 256
module.decoder.decoder.layers.0.norm3.bias 256
module.decoder.decoder.layers.1.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.1.self_attn.in_proj_bias 768
module.decoder.decoder.layers.1.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.1.self_attn.out_proj.bias 256
module.decoder.decoder.layers.1.norm1.weight 256
module.decoder.decoder.layers.1.norm1.bias 256
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.1.norm2.weight 256
module.decoder.decoder.layers.1.norm2.bias 256
module.decoder.decoder.layers.1.linear1.weight 262144
module.decoder.decoder.layers.1.linear1.bias 1024
module.decoder.decoder.layers.1.linear2.weight 262144
module.decoder.decoder.layers.1.linear2.bias 256
module.decoder.decoder.layers.1.norm3.weight 256
module.decoder.decoder.layers.1.norm3.bias 256
module.decoder.decoder.layers.2.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.2.self_attn.in_proj_bias 768
module.decoder.decoder.layers.2.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.2.self_attn.out_proj.bias 256
module.decoder.decoder.layers.2.norm1.weight 256
module.decoder.decoder.layers.2.norm1.bias 256
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.2.norm2.weight 256
module.decoder.decoder.layers.2.norm2.bias 256
module.decoder.decoder.layers.2.linear1.weight 262144
module.decoder.decoder.layers.2.linear1.bias 1024
module.decoder.decoder.layers.2.linear2.weight 262144
module.decoder.decoder.layers.2.linear2.bias 256
module.decoder.decoder.layers.2.norm3.weight 256
module.decoder.decoder.layers.2.norm3.bias 256
module.decoder.decoder.layers.3.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.3.self_attn.in_proj_bias 768
module.decoder.decoder.layers.3.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.3.self_attn.out_proj.bias 256
module.decoder.decoder.layers.3.norm1.weight 256
module.decoder.decoder.layers.3.norm1.bias 256
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.3.norm2.weight 256
module.decoder.decoder.layers.3.norm2.bias 256
module.decoder.decoder.layers.3.linear1.weight 262144
module.decoder.decoder.layers.3.linear1.bias 1024
module.decoder.decoder.layers.3.linear2.weight 262144
module.decoder.decoder.layers.3.linear2.bias 256
module.decoder.decoder.layers.3.norm3.weight 256
module.decoder.decoder.layers.3.norm3.bias 256
module.decoder.decoder.layers.4.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.4.self_attn.in_proj_bias 768
module.decoder.decoder.layers.4.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.4.self_attn.out_proj.bias 256
module.decoder.decoder.layers.4.norm1.weight 256
module.decoder.decoder.layers.4.norm1.bias 256
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.4.norm2.weight 256
module.decoder.decoder.layers.4.norm2.bias 256
module.decoder.decoder.layers.4.linear1.weight 262144
module.decoder.decoder.layers.4.linear1.bias 1024
module.decoder.decoder.layers.4.linear2.weight 262144
module.decoder.decoder.layers.4.linear2.bias 256
module.decoder.decoder.layers.4.norm3.weight 256
module.decoder.decoder.layers.4.norm3.bias 256
module.decoder.decoder.layers.5.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.5.self_attn.in_proj_bias 768
module.decoder.decoder.layers.5.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.5.self_attn.out_proj.bias 256
module.decoder.decoder.layers.5.norm1.weight 256
module.decoder.decoder.layers.5.norm1.bias 256
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.5.norm2.weight 256
module.decoder.decoder.layers.5.norm2.bias 256
module.decoder.decoder.layers.5.linear1.weight 262144
module.decoder.decoder.layers.5.linear1.bias 1024
module.decoder.decoder.layers.5.linear2.weight 262144
module.decoder.decoder.layers.5.linear2.bias 256
module.decoder.decoder.layers.5.norm3.weight 256
module.decoder.decoder.layers.5.norm3.bias 256
module.decoder.denoising_class_embed.weight 20736
module.decoder.query_pos_head.layers.0.weight 2048
module.decoder.query_pos_head.layers.0.bias 512
module.decoder.query_pos_head.layers.1.weight 131072
module.decoder.query_pos_head.layers.1.bias 256
module.decoder.enc_output.0.weight 65536
module.decoder.enc_output.0.bias 256
module.decoder.enc_output.1.weight 256
module.decoder.enc_output.1.bias 256
module.decoder.enc_score_head.weight 20480
module.decoder.enc_score_head.bias 80
module.decoder.enc_bbox_head.layers.0.weight 65536
module.decoder.enc_bbox_head.layers.0.bias 256
module.decoder.enc_bbox_head.layers.1.weight 65536
module.decoder.enc_bbox_head.layers.1.bias 256
module.decoder.enc_bbox_head.layers.2.weight 1024
module.decoder.enc_bbox_head.layers.2.bias 4
module.decoder.dec_score_head.0.weight 20480
module.decoder.dec_score_head.0.bias 80
module.decoder.dec_score_head.1.weight 20480
module.decoder.dec_score_head.1.bias 80
module.decoder.dec_score_head.2.weight 20480
module.decoder.dec_score_head.2.bias 80
module.decoder.dec_score_head.3.weight 20480
module.decoder.dec_score_head.3.bias 80
module.decoder.dec_score_head.4.weight 20480
module.decoder.dec_score_head.4.bias 80
module.decoder.dec_score_head.5.weight 20480
module.decoder.dec_score_head.5.bias 80
module.decoder.dec_bbox_head.0.layers.0.weight 65536
module.decoder.dec_bbox_head.0.layers.0.bias 256
module.decoder.dec_bbox_head.0.layers.1.weight 65536
module.decoder.dec_bbox_head.0.layers.1.bias 256
module.decoder.dec_bbox_head.0.layers.2.weight 1024
module.decoder.dec_bbox_head.0.layers.2.bias 4
module.decoder.dec_bbox_head.1.layers.0.weight 65536
module.decoder.dec_bbox_head.1.layers.0.bias 256
module.decoder.dec_bbox_head.1.layers.1.weight 65536
module.decoder.dec_bbox_head.1.layers.1.bias 256
module.decoder.dec_bbox_head.1.layers.2.weight 1024
module.decoder.dec_bbox_head.1.layers.2.bias 4
module.decoder.dec_bbox_head.2.layers.0.weight 65536
module.decoder.dec_bbox_head.2.layers.0.bias 256
module.decoder.dec_bbox_head.2.layers.1.weight 65536
module.decoder.dec_bbox_head.2.layers.1.bias 256
module.decoder.dec_bbox_head.2.layers.2.weight 1024
module.decoder.dec_bbox_head.2.layers.2.bias 4
module.decoder.dec_bbox_head.3.layers.0.weight 65536
module.decoder.dec_bbox_head.3.layers.0.bias 256
module.decoder.dec_bbox_head.3.layers.1.weight 65536
module.decoder.dec_bbox_head.3.layers.1.bias 256
module.decoder.dec_bbox_head.3.layers.2.weight 1024
module.decoder.dec_bbox_head.3.layers.2.bias 4
module.decoder.dec_bbox_head.4.layers.0.weight 65536
module.decoder.dec_bbox_head.4.layers.0.bias 256
module.decoder.dec_bbox_head.4.layers.1.weight 65536
module.decoder.dec_bbox_head.4.layers.1.bias 256
module.decoder.dec_bbox_head.4.layers.2.weight 1024
module.decoder.dec_bbox_head.4.layers.2.bias 4
module.decoder.dec_bbox_head.5.layers.0.weight 65536
module.decoder.dec_bbox_head.5.layers.0.bias 256
module.decoder.dec_bbox_head.5.layers.1.weight 65536
module.decoder.dec_bbox_head.5.layers.1.bias 256
module.decoder.dec_bbox_head.5.layers.2.weight 1024
module.decoder.dec_bbox_head.5.layers.2.bias 4
module.encoder.input_proj.0.0.weight 131072
module.encoder.input_proj.0.1.weight 256
module.encoder.input_proj.0.1.bias 256
module.encoder.input_proj.1.0.weight 262144
module.encoder.input_proj.1.1.weight 256
module.encoder.input_proj.1.1.bias 256
module.encoder.input_proj.2.0.weight 524288
module.encoder.input_proj.2.1.weight 256
module.encoder.input_proj.2.1.bias 256
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
module.encoder.encoder.0.layers.0.linear1.weight 262144
module.encoder.encoder.0.layers.0.linear1.bias 1024
module.encoder.encoder.0.layers.0.linear2.weight 262144
module.encoder.encoder.0.layers.0.linear2.bias 256
module.encoder.encoder.0.layers.0.norm1.weight 256
module.encoder.encoder.0.layers.0.norm1.bias 256
module.encoder.encoder.0.layers.0.norm2.weight 256
module.encoder.encoder.0.layers.0.norm2.bias 256
module.encoder.lateral_convs.0.conv.weight 65536
module.encoder.lateral_convs.0.norm.weight 256
module.encoder.lateral_convs.0.norm.bias 256
module.encoder.lateral_convs.1.conv.weight 65536
module.encoder.lateral_convs.1.norm.weight 256
module.encoder.lateral_convs.1.norm.bias 256
module.encoder.fpn_blocks.0.conv1.conv.weight 131072
module.encoder.fpn_blocks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.conv2.conv.weight 131072
module.encoder.fpn_blocks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.fpn_blocks.1.conv1.conv.weight 131072
module.encoder.fpn_blocks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.conv2.conv.weight 131072
module.encoder.fpn_blocks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
module.encoder.downsample_convs.0.conv.weight 589824
module.encoder.downsample_convs.0.norm.weight 256
module.encoder.downsample_convs.0.norm.bias 256
module.encoder.downsample_convs.1.conv.weight 589824
module.encoder.downsample_convs.1.norm.weight 256
module.encoder.downsample_convs.1.norm.bias 256
module.encoder.pan_blocks.0.conv1.conv.weight 131072
module.encoder.pan_blocks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.conv2.conv.weight 131072
module.encoder.pan_blocks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.pan_blocks.1.conv1.conv.weight 131072
module.encoder.pan_blocks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.conv2.conv.weight 131072
module.encoder.pan_blocks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 42862860
super_config : [False, False, False, False]
base_config : [True, True, True, True]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch: [0]  [    0/14786]  eta: 7:26:46  lr: 0.000010  loss: 43.2804 (43.2804)  loss_bbox: 1.4971 (1.4971)  loss_bbox_aux_0: 1.5206 (1.5206)  loss_bbox_aux_1: 1.5151 (1.5151)  loss_bbox_aux_2: 1.5124 (1.5124)  loss_bbox_aux_3: 1.4748 (1.4748)  loss_bbox_aux_4: 1.5130 (1.5130)  loss_bbox_aux_5: 1.5565 (1.5565)  loss_bbox_dn_0: 0.8239 (0.8239)  loss_bbox_dn_1: 0.8239 (0.8239)  loss_bbox_dn_2: 0.8239 (0.8239)  loss_bbox_dn_3: 0.8239 (0.8239)  loss_bbox_dn_4: 0.8239 (0.8239)  loss_bbox_dn_5: 0.8239 (0.8239)  loss_giou: 1.7906 (1.7906)  loss_giou_aux_0: 1.8272 (1.8272)  loss_giou_aux_1: 1.8110 (1.8110)  loss_giou_aux_2: 1.7910 (1.7910)  loss_giou_aux_3: 1.8114 (1.8114)  loss_giou_aux_4: 1.7903 (1.7903)  loss_giou_aux_5: 1.8323 (1.8323)  loss_giou_dn_0: 1.3313 (1.3313)  loss_giou_dn_1: 1.3313 (1.3313)  loss_giou_dn_2: 1.3313 (1.3313)  loss_giou_dn_3: 1.3313 (1.3313)  loss_giou_dn_4: 1.3313 (1.3313)  loss_giou_dn_5: 1.3313 (1.3313)  loss_vfl: 0.2749 (0.2749)  loss_vfl_aux_0: 0.2461 (0.2461)  loss_vfl_aux_1: 0.2563 (0.2563)  loss_vfl_aux_2: 0.2775 (0.2775)  loss_vfl_aux_3: 0.2658 (0.2658)  loss_vfl_aux_4: 0.2902 (0.2902)  loss_vfl_aux_5: 0.2526 (0.2526)  loss_vfl_dn_0: 0.8694 (0.8694)  loss_vfl_dn_1: 0.9067 (0.9067)  loss_vfl_dn_2: 0.8675 (0.8675)  loss_vfl_dn_3: 0.8494 (0.8494)  loss_vfl_dn_4: 0.8810 (0.8810)  loss_vfl_dn_5: 0.8683 (0.8683)  time: 1.8130  data: 0.5678  max mem: 4453
Epoch: [0]  [  100/14786]  eta: 1:09:33  lr: 0.000010  loss: 37.4441 (40.4300)  loss_bbox: 0.9293 (1.2038)  loss_bbox_aux_0: 0.9398 (1.2531)  loss_bbox_aux_1: 0.9518 (1.2298)  loss_bbox_aux_2: 0.9407 (1.2171)  loss_bbox_aux_3: 0.9336 (1.2114)  loss_bbox_aux_4: 0.9372 (1.2080)  loss_bbox_aux_5: 1.0101 (1.3047)  loss_bbox_dn_0: 0.7300 (0.8596)  loss_bbox_dn_1: 0.7340 (0.8678)  loss_bbox_dn_2: 0.7358 (0.8746)  loss_bbox_dn_3: 0.7353 (0.8813)  loss_bbox_dn_4: 0.7344 (0.8859)  loss_bbox_dn_5: 0.7329 (0.8908)  loss_giou: 1.4790 (1.5015)  loss_giou_aux_0: 1.5121 (1.5304)  loss_giou_aux_1: 1.4980 (1.5157)  loss_giou_aux_2: 1.4881 (1.5112)  loss_giou_aux_3: 1.4840 (1.5081)  loss_giou_aux_4: 1.4824 (1.5035)  loss_giou_aux_5: 1.5142 (1.5545)  loss_giou_dn_0: 1.3534 (1.3399)  loss_giou_dn_1: 1.3575 (1.3434)  loss_giou_dn_2: 1.3604 (1.3477)  loss_giou_dn_3: 1.3600 (1.3526)  loss_giou_dn_4: 1.3669 (1.3580)  loss_giou_dn_5: 1.3654 (1.3635)  loss_vfl: 0.6060 (0.6162)  loss_vfl_aux_0: 0.5762 (0.5499)  loss_vfl_aux_1: 0.5907 (0.5770)  loss_vfl_aux_2: 0.6017 (0.5945)  loss_vfl_aux_3: 0.6162 (0.6042)  loss_vfl_aux_4: 0.6397 (0.6120)  loss_vfl_aux_5: 0.5621 (0.5163)  loss_vfl_dn_0: 0.5367 (0.6360)  loss_vfl_dn_1: 0.5338 (0.6204)  loss_vfl_dn_2: 0.5407 (0.6311)  loss_vfl_dn_3: 0.5475 (0.6285)  loss_vfl_dn_4: 0.5499 (0.6158)  loss_vfl_dn_5: 0.5365 (0.6100)  time: 0.2637  data: 0.0091  max mem: 5569
Epoch: [0]  [  200/14786]  eta: 1:06:33  lr: 0.000010  loss: 38.1115 (39.1421)  loss_bbox: 0.7767 (1.0256)  loss_bbox_aux_0: 0.8446 (1.0693)  loss_bbox_aux_1: 0.7955 (1.0492)  loss_bbox_aux_2: 0.7703 (1.0380)  loss_bbox_aux_3: 0.7584 (1.0323)  loss_bbox_aux_4: 0.7774 (1.0285)  loss_bbox_aux_5: 0.8958 (1.1143)  loss_bbox_dn_0: 0.8120 (0.8616)  loss_bbox_dn_1: 0.8126 (0.8650)  loss_bbox_dn_2: 0.8115 (0.8681)  loss_bbox_dn_3: 0.8129 (0.8715)  loss_bbox_dn_4: 0.8142 (0.8740)  loss_bbox_dn_5: 0.8141 (0.8763)  loss_giou: 1.3545 (1.4404)  loss_giou_aux_0: 1.4066 (1.4673)  loss_giou_aux_1: 1.3859 (1.4543)  loss_giou_aux_2: 1.3743 (1.4492)  loss_giou_aux_3: 1.3691 (1.4456)  loss_giou_aux_4: 1.3609 (1.4423)  loss_giou_aux_5: 1.4087 (1.4888)  loss_giou_dn_0: 1.3296 (1.3386)  loss_giou_dn_1: 1.3297 (1.3418)  loss_giou_dn_2: 1.3385 (1.3474)  loss_giou_dn_3: 1.3446 (1.3529)  loss_giou_dn_4: 1.3503 (1.3581)  loss_giou_dn_5: 1.3542 (1.3630)  loss_vfl: 0.7546 (0.7261)  loss_vfl_aux_0: 0.6421 (0.6435)  loss_vfl_aux_1: 0.6715 (0.6726)  loss_vfl_aux_2: 0.6936 (0.6945)  loss_vfl_aux_3: 0.7171 (0.7067)  loss_vfl_aux_4: 0.7475 (0.7204)  loss_vfl_aux_5: 0.6021 (0.6006)  loss_vfl_dn_0: 0.4992 (0.5823)  loss_vfl_dn_1: 0.5076 (0.5762)  loss_vfl_dn_2: 0.5248 (0.5916)  loss_vfl_dn_3: 0.5321 (0.5923)  loss_vfl_dn_4: 0.5392 (0.5871)  loss_vfl_dn_5: 0.5426 (0.5844)  time: 0.2609  data: 0.0092  max mem: 5569
Epoch: [0]  [  300/14786]  eta: 1:05:21  lr: 0.000010  loss: 37.3784 (38.5334)  loss_bbox: 0.7219 (0.9446)  loss_bbox_aux_0: 0.7662 (0.9942)  loss_bbox_aux_1: 0.7505 (0.9690)  loss_bbox_aux_2: 0.7324 (0.9560)  loss_bbox_aux_3: 0.7566 (0.9509)  loss_bbox_aux_4: 0.7395 (0.9471)  loss_bbox_aux_5: 0.8565 (1.0390)  loss_bbox_dn_0: 0.7831 (0.8654)  loss_bbox_dn_1: 0.7770 (0.8652)  loss_bbox_dn_2: 0.7741 (0.8667)  loss_bbox_dn_3: 0.7735 (0.8689)  loss_bbox_dn_4: 0.7727 (0.8705)  loss_bbox_dn_5: 0.7725 (0.8720)  loss_giou: 1.3593 (1.3991)  loss_giou_aux_0: 1.3935 (1.4312)  loss_giou_aux_1: 1.3555 (1.4151)  loss_giou_aux_2: 1.3506 (1.4087)  loss_giou_aux_3: 1.3661 (1.4043)  loss_giou_aux_4: 1.3717 (1.4008)  loss_giou_aux_5: 1.4014 (1.4574)  loss_giou_dn_0: 1.3310 (1.3330)  loss_giou_dn_1: 1.3313 (1.3347)  loss_giou_dn_2: 1.3275 (1.3404)  loss_giou_dn_3: 1.3284 (1.3457)  loss_giou_dn_4: 1.3354 (1.3504)  loss_giou_dn_5: 1.3373 (1.3543)  loss_vfl: 0.9494 (0.7958)  loss_vfl_aux_0: 0.8070 (0.6883)  loss_vfl_aux_1: 0.8619 (0.7250)  loss_vfl_aux_2: 0.9004 (0.7516)  loss_vfl_aux_3: 0.9014 (0.7707)  loss_vfl_aux_4: 0.9294 (0.7856)  loss_vfl_aux_5: 0.6744 (0.6376)  loss_vfl_dn_0: 0.4690 (0.5484)  loss_vfl_dn_1: 0.5064 (0.5532)  loss_vfl_dn_2: 0.5269 (0.5709)  loss_vfl_dn_3: 0.5318 (0.5744)  loss_vfl_dn_4: 0.5494 (0.5738)  loss_vfl_dn_5: 0.5471 (0.5735)  time: 0.2670  data: 0.0088  max mem: 5569
Epoch: [0]  [  400/14786]  eta: 1:06:09  lr: 0.000010  loss: 35.8666 (38.1552)  loss_bbox: 0.6624 (0.8946)  loss_bbox_aux_0: 0.7451 (0.9422)  loss_bbox_aux_1: 0.7030 (0.9175)  loss_bbox_aux_2: 0.6864 (0.9062)  loss_bbox_aux_3: 0.6734 (0.9011)  loss_bbox_aux_4: 0.6754 (0.8969)  loss_bbox_aux_5: 0.7728 (0.9902)  loss_bbox_dn_0: 0.7097 (0.8632)  loss_bbox_dn_1: 0.6991 (0.8603)  loss_bbox_dn_2: 0.6930 (0.8605)  loss_bbox_dn_3: 0.6897 (0.8616)  loss_bbox_dn_4: 0.6886 (0.8625)  loss_bbox_dn_5: 0.6885 (0.8636)  loss_giou: 1.2837 (1.3697)  loss_giou_aux_0: 1.3330 (1.4022)  loss_giou_aux_1: 1.3060 (1.3863)  loss_giou_aux_2: 1.2838 (1.3792)  loss_giou_aux_3: 1.2966 (1.3750)  loss_giou_aux_4: 1.2878 (1.3719)  loss_giou_aux_5: 1.3682 (1.4370)  loss_giou_dn_0: 1.2991 (1.3272)  loss_giou_dn_1: 1.2881 (1.3267)  loss_giou_dn_2: 1.2945 (1.3317)  loss_giou_dn_3: 1.2887 (1.3361)  loss_giou_dn_4: 1.2952 (1.3402)  loss_giou_dn_5: 1.2954 (1.3435)  loss_vfl: 0.9202 (0.8461)  loss_vfl_aux_0: 0.7442 (0.7291)  loss_vfl_aux_1: 0.7945 (0.7666)  loss_vfl_aux_2: 0.8111 (0.7950)  loss_vfl_aux_3: 0.8576 (0.8160)  loss_vfl_aux_4: 0.8669 (0.8328)  loss_vfl_aux_5: 0.6975 (0.6688)  loss_vfl_dn_0: 0.4580 (0.5296)  loss_vfl_dn_1: 0.5086 (0.5430)  loss_vfl_dn_2: 0.5588 (0.5642)  loss_vfl_dn_3: 0.5604 (0.5704)  loss_vfl_dn_4: 0.5619 (0.5724)  loss_vfl_dn_5: 0.5723 (0.5740)  time: 0.2646  data: 0.0095  max mem: 5569
Epoch: [0]  [  500/14786]  eta: 1:05:13  lr: 0.000010  loss: 35.8395 (37.7067)  loss_bbox: 0.6380 (0.8523)  loss_bbox_aux_0: 0.7035 (0.8998)  loss_bbox_aux_1: 0.6449 (0.8752)  loss_bbox_aux_2: 0.6509 (0.8647)  loss_bbox_aux_3: 0.6468 (0.8590)  loss_bbox_aux_4: 0.6442 (0.8547)  loss_bbox_aux_5: 0.7873 (0.9486)  loss_bbox_dn_0: 0.7581 (0.8518)  loss_bbox_dn_1: 0.7244 (0.8461)  loss_bbox_dn_2: 0.7128 (0.8451)  loss_bbox_dn_3: 0.7042 (0.8453)  loss_bbox_dn_4: 0.7028 (0.8456)  loss_bbox_dn_5: 0.7031 (0.8464)  loss_giou: 1.2109 (1.3507)  loss_giou_aux_0: 1.2346 (1.3845)  loss_giou_aux_1: 1.2049 (1.3677)  loss_giou_aux_2: 1.1976 (1.3598)  loss_giou_aux_3: 1.1932 (1.3559)  loss_giou_aux_4: 1.1978 (1.3530)  loss_giou_aux_5: 1.3305 (1.4248)  loss_giou_dn_0: 1.3019 (1.3219)  loss_giou_dn_1: 1.2780 (1.3190)  loss_giou_dn_2: 1.2754 (1.3224)  loss_giou_dn_3: 1.2791 (1.3258)  loss_giou_dn_4: 1.2725 (1.3291)  loss_giou_dn_5: 1.2737 (1.3320)  loss_vfl: 1.0533 (0.8733)  loss_vfl_aux_0: 0.8838 (0.7496)  loss_vfl_aux_1: 0.9512 (0.7874)  loss_vfl_aux_2: 0.9928 (0.8165)  loss_vfl_aux_3: 1.0002 (0.8382)  loss_vfl_aux_4: 1.0289 (0.8559)  loss_vfl_aux_5: 0.7776 (0.6842)  loss_vfl_dn_0: 0.4723 (0.5155)  loss_vfl_dn_1: 0.4929 (0.5338)  loss_vfl_dn_2: 0.5471 (0.5582)  loss_vfl_dn_3: 0.5624 (0.5668)  loss_vfl_dn_4: 0.5658 (0.5713)  loss_vfl_dn_5: 0.5946 (0.5745)  time: 0.2607  data: 0.0096  max mem: 5569
Epoch: [0]  [  600/14786]  eta: 1:04:24  lr: 0.000010  loss: 35.8880 (37.4146)  loss_bbox: 0.6574 (0.8251)  loss_bbox_aux_0: 0.7141 (0.8752)  loss_bbox_aux_1: 0.6725 (0.8489)  loss_bbox_aux_2: 0.6798 (0.8384)  loss_bbox_aux_3: 0.6756 (0.8323)  loss_bbox_aux_4: 0.6742 (0.8279)  loss_bbox_aux_5: 0.7842 (0.9263)  loss_bbox_dn_0: 0.8630 (0.8456)  loss_bbox_dn_1: 0.8489 (0.8377)  loss_bbox_dn_2: 0.8491 (0.8358)  loss_bbox_dn_3: 0.8490 (0.8354)  loss_bbox_dn_4: 0.8473 (0.8356)  loss_bbox_dn_5: 0.8478 (0.8363)  loss_giou: 1.1432 (1.3342)  loss_giou_aux_0: 1.1981 (1.3715)  loss_giou_aux_1: 1.1680 (1.3530)  loss_giou_aux_2: 1.1354 (1.3435)  loss_giou_aux_3: 1.1417 (1.3394)  loss_giou_aux_4: 1.1518 (1.3362)  loss_giou_aux_5: 1.2796 (1.4155)  loss_giou_dn_0: 1.2753 (1.3151)  loss_giou_dn_1: 1.2509 (1.3095)  loss_giou_dn_2: 1.2385 (1.3114)  loss_giou_dn_3: 1.2271 (1.3139)  loss_giou_dn_4: 1.2222 (1.3166)  loss_giou_dn_5: 1.2220 (1.3190)  loss_vfl: 1.0655 (0.8951)  loss_vfl_aux_0: 0.8741 (0.7658)  loss_vfl_aux_1: 0.9595 (0.8051)  loss_vfl_aux_2: 1.0213 (0.8351)  loss_vfl_aux_3: 1.0054 (0.8560)  loss_vfl_aux_4: 0.9896 (0.8748)  loss_vfl_aux_5: 0.8218 (0.6972)  loss_vfl_dn_0: 0.4542 (0.5060)  loss_vfl_dn_1: 0.4938 (0.5286)  loss_vfl_dn_2: 0.5369 (0.5551)  loss_vfl_dn_3: 0.5656 (0.5666)  loss_vfl_dn_4: 0.5861 (0.5727)  loss_vfl_dn_5: 0.6033 (0.5773)  time: 0.2688  data: 0.0082  max mem: 5569
Epoch: [0]  [  700/14786]  eta: 1:03:37  lr: 0.000010  loss: 34.3357 (37.1222)  loss_bbox: 0.6292 (0.7991)  loss_bbox_aux_0: 0.6937 (0.8513)  loss_bbox_aux_1: 0.6648 (0.8244)  loss_bbox_aux_2: 0.6670 (0.8126)  loss_bbox_aux_3: 0.6344 (0.8065)  loss_bbox_aux_4: 0.6407 (0.8019)  loss_bbox_aux_5: 0.7949 (0.9051)  loss_bbox_dn_0: 0.7525 (0.8372)  loss_bbox_dn_1: 0.7179 (0.8275)  loss_bbox_dn_2: 0.7055 (0.8249)  loss_bbox_dn_3: 0.7017 (0.8243)  loss_bbox_dn_4: 0.7038 (0.8244)  loss_bbox_dn_5: 0.7033 (0.8250)  loss_giou: 1.2726 (1.3170)  loss_giou_aux_0: 1.3091 (1.3577)  loss_giou_aux_1: 1.3050 (1.3372)  loss_giou_aux_2: 1.3150 (1.3265)  loss_giou_aux_3: 1.2937 (1.3219)  loss_giou_aux_4: 1.2833 (1.3187)  loss_giou_aux_5: 1.3849 (1.4059)  loss_giou_dn_0: 1.2643 (1.3095)  loss_giou_dn_1: 1.2243 (1.3012)  loss_giou_dn_2: 1.2155 (1.3018)  loss_giou_dn_3: 1.2154 (1.3036)  loss_giou_dn_4: 1.2214 (1.3059)  loss_giou_dn_5: 1.2221 (1.3081)  loss_vfl: 0.8703 (0.9142)  loss_vfl_aux_0: 0.7908 (0.7826)  loss_vfl_aux_1: 0.7866 (0.8225)  loss_vfl_aux_2: 0.8398 (0.8546)  loss_vfl_aux_3: 0.8866 (0.8752)  loss_vfl_aux_4: 0.8937 (0.8933)  loss_vfl_aux_5: 0.6755 (0.7083)  loss_vfl_dn_0: 0.4421 (0.4984)  loss_vfl_dn_1: 0.4856 (0.5233)  loss_vfl_dn_2: 0.5352 (0.5524)  loss_vfl_dn_3: 0.5566 (0.5661)  loss_vfl_dn_4: 0.5705 (0.5734)  loss_vfl_dn_5: 0.5887 (0.5788)  time: 0.2702  data: 0.0088  max mem: 5569
Epoch: [0]  [  800/14786]  eta: 1:02:52  lr: 0.000010  loss: 34.6390 (36.9194)  loss_bbox: 0.5621 (0.7769)  loss_bbox_aux_0: 0.6343 (0.8316)  loss_bbox_aux_1: 0.5961 (0.8030)  loss_bbox_aux_2: 0.5882 (0.7903)  loss_bbox_aux_3: 0.5647 (0.7840)  loss_bbox_aux_4: 0.5630 (0.7796)  loss_bbox_aux_5: 0.7229 (0.8881)  loss_bbox_dn_0: 0.7300 (0.8362)  loss_bbox_dn_1: 0.7214 (0.8244)  loss_bbox_dn_2: 0.7219 (0.8213)  loss_bbox_dn_3: 0.7250 (0.8206)  loss_bbox_dn_4: 0.7278 (0.8207)  loss_bbox_dn_5: 0.7278 (0.8212)  loss_giou: 1.1775 (1.2962)  loss_giou_aux_0: 1.2599 (1.3403)  loss_giou_aux_1: 1.1779 (1.3178)  loss_giou_aux_2: 1.1566 (1.3061)  loss_giou_aux_3: 1.1596 (1.3013)  loss_giou_aux_4: 1.1628 (1.2978)  loss_giou_aux_5: 1.3169 (1.3923)  loss_giou_dn_0: 1.2463 (1.3026)  loss_giou_dn_1: 1.2205 (1.2912)  loss_giou_dn_2: 1.2162 (1.2905)  loss_giou_dn_3: 1.2156 (1.2917)  loss_giou_dn_4: 1.2113 (1.2936)  loss_giou_dn_5: 1.2109 (1.2956)  loss_vfl: 0.9898 (0.9382)  loss_vfl_aux_0: 0.8541 (0.8038)  loss_vfl_aux_1: 0.9054 (0.8473)  loss_vfl_aux_2: 0.9224 (0.8805)  loss_vfl_aux_3: 0.9895 (0.8996)  loss_vfl_aux_4: 0.9695 (0.9176)  loss_vfl_aux_5: 0.7907 (0.7259)  loss_vfl_dn_0: 0.4532 (0.4936)  loss_vfl_dn_1: 0.4869 (0.5206)  loss_vfl_dn_2: 0.5292 (0.5516)  loss_vfl_dn_3: 0.5654 (0.5678)  loss_vfl_dn_4: 0.5894 (0.5761)  loss_vfl_dn_5: 0.6042 (0.5821)  time: 0.2604  data: 0.0085  max mem: 5569
Epoch: [0]  [  900/14786]  eta: 1:02:06  lr: 0.000010  loss: 33.5142 (36.7327)  loss_bbox: 0.5590 (0.7555)  loss_bbox_aux_0: 0.6465 (0.8134)  loss_bbox_aux_1: 0.6006 (0.7831)  loss_bbox_aux_2: 0.5644 (0.7697)  loss_bbox_aux_3: 0.5601 (0.7629)  loss_bbox_aux_4: 0.5577 (0.7585)  loss_bbox_aux_5: 0.6692 (0.8720)  loss_bbox_dn_0: 0.6708 (0.8348)  loss_bbox_dn_1: 0.6352 (0.8212)  loss_bbox_dn_2: 0.6260 (0.8178)  loss_bbox_dn_3: 0.6229 (0.8170)  loss_bbox_dn_4: 0.6257 (0.8172)  loss_bbox_dn_5: 0.6258 (0.8177)  loss_giou: 1.1476 (1.2764)  loss_giou_aux_0: 1.2003 (1.3240)  loss_giou_aux_1: 1.1743 (1.2993)  loss_giou_aux_2: 1.1553 (1.2867)  loss_giou_aux_3: 1.1604 (1.2815)  loss_giou_aux_4: 1.1472 (1.2779)  loss_giou_aux_5: 1.2990 (1.3792)  loss_giou_dn_0: 1.2487 (1.2953)  loss_giou_dn_1: 1.2024 (1.2812)  loss_giou_dn_2: 1.1923 (1.2797)  loss_giou_dn_3: 1.1871 (1.2803)  loss_giou_dn_4: 1.1888 (1.2819)  loss_giou_dn_5: 1.1894 (1.2837)  loss_vfl: 1.0396 (0.9612)  loss_vfl_aux_0: 0.8976 (0.8242)  loss_vfl_aux_1: 0.9223 (0.8708)  loss_vfl_aux_2: 0.9690 (0.9053)  loss_vfl_aux_3: 0.9836 (0.9239)  loss_vfl_aux_4: 1.0334 (0.9413)  loss_vfl_aux_5: 0.7321 (0.7428)  loss_vfl_dn_0: 0.4703 (0.4905)  loss_vfl_dn_1: 0.4906 (0.5187)  loss_vfl_dn_2: 0.5275 (0.5514)  loss_vfl_dn_3: 0.5520 (0.5697)  loss_vfl_dn_4: 0.5736 (0.5793)  loss_vfl_dn_5: 0.5903 (0.5858)  time: 0.2593  data: 0.0096  max mem: 5569
Epoch: [0]  [ 1000/14786]  eta: 1:01:35  lr: 0.000010  loss: 34.8894 (36.4811)  loss_bbox: 0.5940 (0.7360)  loss_bbox_aux_0: 0.6753 (0.7951)  loss_bbox_aux_1: 0.6144 (0.7642)  loss_bbox_aux_2: 0.6050 (0.7504)  loss_bbox_aux_3: 0.6078 (0.7434)  loss_bbox_aux_4: 0.6034 (0.7390)  loss_bbox_aux_5: 0.7497 (0.8564)  loss_bbox_dn_0: 0.7799 (0.8250)  loss_bbox_dn_1: 0.7668 (0.8104)  loss_bbox_dn_2: 0.7590 (0.8069)  loss_bbox_dn_3: 0.7542 (0.8062)  loss_bbox_dn_4: 0.7527 (0.8063)  loss_bbox_dn_5: 0.7532 (0.8068)  loss_giou: 1.1377 (1.2642)  loss_giou_aux_0: 1.2206 (1.3144)  loss_giou_aux_1: 1.1723 (1.2880)  loss_giou_aux_2: 1.1472 (1.2750)  loss_giou_aux_3: 1.1455 (1.2696)  loss_giou_aux_4: 1.1456 (1.2659)  loss_giou_aux_5: 1.2939 (1.3715)  loss_giou_dn_0: 1.2232 (1.2904)  loss_giou_dn_1: 1.1739 (1.2740)  loss_giou_dn_2: 1.1587 (1.2717)  loss_giou_dn_3: 1.1485 (1.2717)  loss_giou_dn_4: 1.1454 (1.2730)  loss_giou_dn_5: 1.1450 (1.2747)  loss_vfl: 1.1210 (0.9709)  loss_vfl_aux_0: 0.9042 (0.8340)  loss_vfl_aux_1: 1.0124 (0.8824)  loss_vfl_aux_2: 1.0426 (0.9168)  loss_vfl_aux_3: 1.0964 (0.9354)  loss_vfl_aux_4: 1.1115 (0.9517)  loss_vfl_aux_5: 0.8263 (0.7509)  loss_vfl_dn_0: 0.4644 (0.4870)  loss_vfl_dn_1: 0.4966 (0.5156)  loss_vfl_dn_2: 0.5351 (0.5490)  loss_vfl_dn_3: 0.5770 (0.5694)  loss_vfl_dn_4: 0.6028 (0.5804)  loss_vfl_dn_5: 0.6259 (0.5874)  time: 0.2626  data: 0.0096  max mem: 5569
Epoch: [0]  [ 1100/14786]  eta: 1:01:01  lr: 0.000010  loss: 35.0632 (36.3316)  loss_bbox: 0.5094 (0.7219)  loss_bbox_aux_0: 0.5969 (0.7823)  loss_bbox_aux_1: 0.5435 (0.7508)  loss_bbox_aux_2: 0.5144 (0.7365)  loss_bbox_aux_3: 0.5109 (0.7293)  loss_bbox_aux_4: 0.5052 (0.7248)  loss_bbox_aux_5: 0.6890 (0.8461)  loss_bbox_dn_0: 0.7207 (0.8219)  loss_bbox_dn_1: 0.7133 (0.8059)  loss_bbox_dn_2: 0.7142 (0.8022)  loss_bbox_dn_3: 0.7167 (0.8014)  loss_bbox_dn_4: 0.7170 (0.8016)  loss_bbox_dn_5: 0.7171 (0.8020)  loss_giou: 1.1413 (1.2502)  loss_giou_aux_0: 1.2005 (1.3028)  loss_giou_aux_1: 1.1749 (1.2749)  loss_giou_aux_2: 1.1711 (1.2614)  loss_giou_aux_3: 1.1485 (1.2554)  loss_giou_aux_4: 1.1500 (1.2518)  loss_giou_aux_5: 1.3192 (1.3629)  loss_giou_dn_0: 1.2267 (1.2835)  loss_giou_dn_1: 1.1843 (1.2648)  loss_giou_dn_2: 1.1738 (1.2615)  loss_giou_dn_3: 1.1632 (1.2610)  loss_giou_dn_4: 1.1639 (1.2621)  loss_giou_dn_5: 1.1646 (1.2636)  loss_vfl: 1.0887 (0.9871)  loss_vfl_aux_0: 0.9132 (0.8497)  loss_vfl_aux_1: 0.9907 (0.8996)  loss_vfl_aux_2: 1.0289 (0.9343)  loss_vfl_aux_3: 1.0519 (0.9533)  loss_vfl_aux_4: 1.0470 (0.9687)  loss_vfl_aux_5: 0.7711 (0.7639)  loss_vfl_dn_0: 0.4701 (0.4854)  loss_vfl_dn_1: 0.4959 (0.5140)  loss_vfl_dn_2: 0.5306 (0.5482)  loss_vfl_dn_3: 0.5762 (0.5706)  loss_vfl_dn_4: 0.6029 (0.5832)  loss_vfl_dn_5: 0.6183 (0.5911)  time: 0.2690  data: 0.0106  max mem: 5570
Epoch: [0]  [ 1200/14786]  eta: 1:00:35  lr: 0.000010  loss: 33.0927 (36.1312)  loss_bbox: 0.5858 (0.7072)  loss_bbox_aux_0: 0.6162 (0.7678)  loss_bbox_aux_1: 0.5727 (0.7366)  loss_bbox_aux_2: 0.5882 (0.7221)  loss_bbox_aux_3: 0.5864 (0.7149)  loss_bbox_aux_4: 0.5964 (0.7103)  loss_bbox_aux_5: 0.7009 (0.8339)  loss_bbox_dn_0: 0.7175 (0.8135)  loss_bbox_dn_1: 0.6918 (0.7964)  loss_bbox_dn_2: 0.6889 (0.7924)  loss_bbox_dn_3: 0.6889 (0.7915)  loss_bbox_dn_4: 0.6897 (0.7917)  loss_bbox_dn_5: 0.6895 (0.7921)  loss_giou: 1.1467 (1.2379)  loss_giou_aux_0: 1.2228 (1.2929)  loss_giou_aux_1: 1.2008 (1.2639)  loss_giou_aux_2: 1.1710 (1.2498)  loss_giou_aux_3: 1.1621 (1.2433)  loss_giou_aux_4: 1.1477 (1.2395)  loss_giou_aux_5: 1.2958 (1.3554)  loss_giou_dn_0: 1.1915 (1.2775)  loss_giou_dn_1: 1.1610 (1.2564)  loss_giou_dn_2: 1.1442 (1.2522)  loss_giou_dn_3: 1.1345 (1.2511)  loss_giou_dn_4: 1.1309 (1.2520)  loss_giou_dn_5: 1.1308 (1.2534)  loss_vfl: 1.0668 (0.9990)  loss_vfl_aux_0: 0.9166 (0.8610)  loss_vfl_aux_1: 0.9801 (0.9116)  loss_vfl_aux_2: 1.0388 (0.9469)  loss_vfl_aux_3: 1.1006 (0.9665)  loss_vfl_aux_4: 1.0598 (0.9812)  loss_vfl_aux_5: 0.8940 (0.7736)  loss_vfl_dn_0: 0.4528 (0.4838)  loss_vfl_dn_1: 0.4744 (0.5127)  loss_vfl_dn_2: 0.5166 (0.5474)  loss_vfl_dn_3: 0.5499 (0.5715)  loss_vfl_dn_4: 0.5743 (0.5855)  loss_vfl_dn_5: 0.5946 (0.5944)  time: 0.2762  data: 0.0092  max mem: 5570
Epoch: [0]  [ 1300/14786]  eta: 1:00:06  lr: 0.000010  loss: 32.9575 (35.9392)  loss_bbox: 0.4725 (0.6937)  loss_bbox_aux_0: 0.5291 (0.7552)  loss_bbox_aux_1: 0.4796 (0.7233)  loss_bbox_aux_2: 0.4678 (0.7087)  loss_bbox_aux_3: 0.4644 (0.7011)  loss_bbox_aux_4: 0.4660 (0.6968)  loss_bbox_aux_5: 0.6020 (0.8224)  loss_bbox_dn_0: 0.6713 (0.8064)  loss_bbox_dn_1: 0.6499 (0.7882)  loss_bbox_dn_2: 0.6526 (0.7840)  loss_bbox_dn_3: 0.6546 (0.7829)  loss_bbox_dn_4: 0.6568 (0.7831)  loss_bbox_dn_5: 0.6567 (0.7835)  loss_giou: 0.9765 (1.2253)  loss_giou_aux_0: 1.0885 (1.2828)  loss_giou_aux_1: 1.0353 (1.2525)  loss_giou_aux_2: 1.0047 (1.2376)  loss_giou_aux_3: 0.9768 (1.2306)  loss_giou_aux_4: 0.9765 (1.2268)  loss_giou_aux_5: 1.2156 (1.3475)  loss_giou_dn_0: 1.2104 (1.2715)  loss_giou_dn_1: 1.1714 (1.2482)  loss_giou_dn_2: 1.1620 (1.2431)  loss_giou_dn_3: 1.1539 (1.2415)  loss_giou_dn_4: 1.1520 (1.2422)  loss_giou_dn_5: 1.1523 (1.2435)  loss_vfl: 1.1975 (1.0107)  loss_vfl_aux_0: 1.0365 (0.8720)  loss_vfl_aux_1: 1.1449 (0.9238)  loss_vfl_aux_2: 1.1931 (0.9594)  loss_vfl_aux_3: 1.1974 (0.9793)  loss_vfl_aux_4: 1.2202 (0.9936)  loss_vfl_aux_5: 0.8832 (0.7823)  loss_vfl_dn_0: 0.4638 (0.4828)  loss_vfl_dn_1: 0.4880 (0.5116)  loss_vfl_dn_2: 0.5118 (0.5462)  loss_vfl_dn_3: 0.5543 (0.5717)  loss_vfl_dn_4: 0.5870 (0.5871)  loss_vfl_dn_5: 0.6011 (0.5966)  time: 0.2657  data: 0.0109  max mem: 5570
Epoch: [0]  [ 1400/14786]  eta: 0:59:36  lr: 0.000010  loss: 33.0248 (35.7885)  loss_bbox: 0.4734 (0.6812)  loss_bbox_aux_0: 0.5766 (0.7433)  loss_bbox_aux_1: 0.5384 (0.7111)  loss_bbox_aux_2: 0.4909 (0.6964)  loss_bbox_aux_3: 0.4759 (0.6889)  loss_bbox_aux_4: 0.4767 (0.6843)  loss_bbox_aux_5: 0.6972 (0.8122)  loss_bbox_dn_0: 0.6689 (0.8019)  loss_bbox_dn_1: 0.6294 (0.7827)  loss_bbox_dn_2: 0.6178 (0.7783)  loss_bbox_dn_3: 0.6136 (0.7772)  loss_bbox_dn_4: 0.6119 (0.7773)  loss_bbox_dn_5: 0.6118 (0.7777)  loss_giou: 1.0559 (1.2128)  loss_giou_aux_0: 1.1434 (1.2725)  loss_giou_aux_1: 1.0845 (1.2411)  loss_giou_aux_2: 1.0603 (1.2256)  loss_giou_aux_3: 1.0560 (1.2182)  loss_giou_aux_4: 1.0438 (1.2144)  loss_giou_aux_5: 1.2497 (1.3397)  loss_giou_dn_0: 1.2054 (1.2656)  loss_giou_dn_1: 1.1692 (1.2405)  loss_giou_dn_2: 1.1604 (1.2347)  loss_giou_dn_3: 1.1579 (1.2326)  loss_giou_dn_4: 1.1592 (1.2331)  loss_giou_dn_5: 1.1597 (1.2343)  loss_vfl: 1.1145 (1.0240)  loss_vfl_aux_0: 0.9929 (0.8851)  loss_vfl_aux_1: 1.0378 (0.9377)  loss_vfl_aux_2: 1.0698 (0.9736)  loss_vfl_aux_3: 1.1218 (0.9936)  loss_vfl_aux_4: 1.1301 (1.0076)  loss_vfl_aux_5: 0.8921 (0.7924)  loss_vfl_dn_0: 0.4647 (0.4822)  loss_vfl_dn_1: 0.4833 (0.5106)  loss_vfl_dn_2: 0.5126 (0.5449)  loss_vfl_dn_3: 0.5504 (0.5717)  loss_vfl_dn_4: 0.5947 (0.5886)  loss_vfl_dn_5: 0.6096 (0.5988)  time: 0.2570  data: 0.0080  max mem: 5570
Epoch: [0]  [ 1500/14786]  eta: 0:59:03  lr: 0.000010  loss: 33.2014 (35.6174)  loss_bbox: 0.5139 (0.6692)  loss_bbox_aux_0: 0.6031 (0.7316)  loss_bbox_aux_1: 0.5590 (0.6991)  loss_bbox_aux_2: 0.5482 (0.6844)  loss_bbox_aux_3: 0.5215 (0.6767)  loss_bbox_aux_4: 0.5200 (0.6722)  loss_bbox_aux_5: 0.6647 (0.8014)  loss_bbox_dn_0: 0.7361 (0.7956)  loss_bbox_dn_1: 0.7317 (0.7757)  loss_bbox_dn_2: 0.7320 (0.7711)  loss_bbox_dn_3: 0.7307 (0.7699)  loss_bbox_dn_4: 0.7293 (0.7700)  loss_bbox_dn_5: 0.7293 (0.7703)  loss_giou: 0.9393 (1.2030)  loss_giou_aux_0: 1.0313 (1.2641)  loss_giou_aux_1: 0.9517 (1.2319)  loss_giou_aux_2: 0.9431 (1.2161)  loss_giou_aux_3: 0.9456 (1.2083)  loss_giou_aux_4: 0.9340 (1.2046)  loss_giou_aux_5: 1.1401 (1.3332)  loss_giou_dn_0: 1.1597 (1.2602)  loss_giou_dn_1: 1.0998 (1.2333)  loss_giou_dn_2: 1.0885 (1.2269)  loss_giou_dn_3: 1.0761 (1.2245)  loss_giou_dn_4: 1.0727 (1.2247)  loss_giou_dn_5: 1.0730 (1.2259)  loss_vfl: 1.2861 (1.0326)  loss_vfl_aux_0: 1.0964 (0.8940)  loss_vfl_aux_1: 1.1965 (0.9477)  loss_vfl_aux_2: 1.2420 (0.9835)  loss_vfl_aux_3: 1.2452 (1.0032)  loss_vfl_aux_4: 1.2637 (1.0166)  loss_vfl_aux_5: 0.9983 (0.8001)  loss_vfl_dn_0: 0.4954 (0.4817)  loss_vfl_dn_1: 0.5152 (0.5097)  loss_vfl_dn_2: 0.5348 (0.5434)  loss_vfl_dn_3: 0.5712 (0.5714)  loss_vfl_dn_4: 0.6001 (0.5893)  loss_vfl_dn_5: 0.6304 (0.6003)  time: 0.2569  data: 0.0088  max mem: 5570
Epoch: [0]  [ 1600/14786]  eta: 0:58:30  lr: 0.000010  loss: 32.5669 (35.4826)  loss_bbox: 0.4829 (0.6593)  loss_bbox_aux_0: 0.5434 (0.7222)  loss_bbox_aux_1: 0.5120 (0.6897)  loss_bbox_aux_2: 0.5148 (0.6748)  loss_bbox_aux_3: 0.4992 (0.6670)  loss_bbox_aux_4: 0.4810 (0.6624)  loss_bbox_aux_5: 0.6359 (0.7928)  loss_bbox_dn_0: 0.6387 (0.7924)  loss_bbox_dn_1: 0.6084 (0.7717)  loss_bbox_dn_2: 0.5913 (0.7669)  loss_bbox_dn_3: 0.5865 (0.7657)  loss_bbox_dn_4: 0.5850 (0.7658)  loss_bbox_dn_5: 0.5851 (0.7661)  loss_giou: 1.0439 (1.1931)  loss_giou_aux_0: 1.1580 (1.2558)  loss_giou_aux_1: 1.1010 (1.2228)  loss_giou_aux_2: 1.0542 (1.2066)  loss_giou_aux_3: 1.0424 (1.1986)  loss_giou_aux_4: 1.0440 (1.1947)  loss_giou_aux_5: 1.2724 (1.3266)  loss_giou_dn_0: 1.1933 (1.2554)  loss_giou_dn_1: 1.1231 (1.2269)  loss_giou_dn_2: 1.1015 (1.2198)  loss_giou_dn_3: 1.0940 (1.2171)  loss_giou_dn_4: 1.0908 (1.2172)  loss_giou_dn_5: 1.0917 (1.2183)  loss_vfl: 1.0950 (1.0414)  loss_vfl_aux_0: 0.9145 (0.9024)  loss_vfl_aux_1: 0.9819 (0.9572)  loss_vfl_aux_2: 1.0139 (0.9929)  loss_vfl_aux_3: 1.0016 (1.0127)  loss_vfl_aux_4: 1.0313 (1.0260)  loss_vfl_aux_5: 0.7824 (0.8066)  loss_vfl_dn_0: 0.4587 (0.4814)  loss_vfl_dn_1: 0.4845 (0.5087)  loss_vfl_dn_2: 0.5078 (0.5418)  loss_vfl_dn_3: 0.5398 (0.5706)  loss_vfl_dn_4: 0.5723 (0.5895)  loss_vfl_dn_5: 0.5926 (0.6015)  time: 0.2575  data: 0.0094  max mem: 5570
Epoch: [0]  [ 1700/14786]  eta: 0:57:59  lr: 0.000010  loss: 33.0993 (35.3636)  loss_bbox: 0.5291 (0.6498)  loss_bbox_aux_0: 0.5824 (0.7135)  loss_bbox_aux_1: 0.5739 (0.6806)  loss_bbox_aux_2: 0.5120 (0.6655)  loss_bbox_aux_3: 0.5406 (0.6575)  loss_bbox_aux_4: 0.5440 (0.6528)  loss_bbox_aux_5: 0.7130 (0.7855)  loss_bbox_dn_0: 0.8384 (0.7897)  loss_bbox_dn_1: 0.8160 (0.7680)  loss_bbox_dn_2: 0.8081 (0.7630)  loss_bbox_dn_3: 0.7993 (0.7617)  loss_bbox_dn_4: 0.7935 (0.7617)  loss_bbox_dn_5: 0.7932 (0.7620)  loss_giou: 0.9594 (1.1829)  loss_giou_aux_0: 1.0287 (1.2465)  loss_giou_aux_1: 0.9880 (1.2127)  loss_giou_aux_2: 0.9580 (1.1964)  loss_giou_aux_3: 0.9644 (1.1884)  loss_giou_aux_4: 0.9582 (1.1845)  loss_giou_aux_5: 1.1101 (1.3190)  loss_giou_dn_0: 1.1575 (1.2503)  loss_giou_dn_1: 1.0976 (1.2204)  loss_giou_dn_2: 1.0730 (1.2127)  loss_giou_dn_3: 1.0639 (1.2097)  loss_giou_dn_4: 1.0577 (1.2097)  loss_giou_dn_5: 1.0577 (1.2108)  loss_vfl: 1.1915 (1.0520)  loss_vfl_aux_0: 1.1058 (0.9134)  loss_vfl_aux_1: 1.1653 (0.9686)  loss_vfl_aux_2: 1.1514 (1.0045)  loss_vfl_aux_3: 1.1608 (1.0242)  loss_vfl_aux_4: 1.1491 (1.0368)  loss_vfl_aux_5: 0.9256 (0.8160)  loss_vfl_dn_0: 0.4807 (0.4814)  loss_vfl_dn_1: 0.5072 (0.5082)  loss_vfl_dn_2: 0.5271 (0.5406)  loss_vfl_dn_3: 0.5609 (0.5701)  loss_vfl_dn_4: 0.6046 (0.5899)  loss_vfl_dn_5: 0.6285 (0.6027)  time: 0.2628  data: 0.0090  max mem: 5570
Epoch: [0]  [ 1800/14786]  eta: 0:57:28  lr: 0.000010  loss: 31.7153 (35.2333)  loss_bbox: 0.4581 (0.6403)  loss_bbox_aux_0: 0.5332 (0.7043)  loss_bbox_aux_1: 0.4915 (0.6712)  loss_bbox_aux_2: 0.4626 (0.6561)  loss_bbox_aux_3: 0.4641 (0.6479)  loss_bbox_aux_4: 0.4594 (0.6433)  loss_bbox_aux_5: 0.5525 (0.7773)  loss_bbox_dn_0: 0.6389 (0.7864)  loss_bbox_dn_1: 0.6214 (0.7639)  loss_bbox_dn_2: 0.6109 (0.7587)  loss_bbox_dn_3: 0.6098 (0.7574)  loss_bbox_dn_4: 0.6092 (0.7574)  loss_bbox_dn_5: 0.6091 (0.7577)  loss_giou: 0.9873 (1.1725)  loss_giou_aux_0: 1.0614 (1.2371)  loss_giou_aux_1: 1.0006 (1.2027)  loss_giou_aux_2: 1.0005 (1.1862)  loss_giou_aux_3: 0.9984 (1.1780)  loss_giou_aux_4: 0.9931 (1.1741)  loss_giou_aux_5: 1.1353 (1.3113)  loss_giou_dn_0: 1.1713 (1.2452)  loss_giou_dn_1: 1.1062 (1.2138)  loss_giou_dn_2: 1.0891 (1.2055)  loss_giou_dn_3: 1.0800 (1.2022)  loss_giou_dn_4: 1.0775 (1.2020)  loss_giou_dn_5: 1.0775 (1.2031)  loss_vfl: 1.0312 (1.0618)  loss_vfl_aux_0: 0.9243 (0.9241)  loss_vfl_aux_1: 0.9995 (0.9792)  loss_vfl_aux_2: 1.0168 (1.0146)  loss_vfl_aux_3: 1.0278 (1.0344)  loss_vfl_aux_4: 1.0193 (1.0471)  loss_vfl_aux_5: 0.8516 (0.8248)  loss_vfl_dn_0: 0.4689 (0.4815)  loss_vfl_dn_1: 0.4886 (0.5079)  loss_vfl_dn_2: 0.5054 (0.5394)  loss_vfl_dn_3: 0.5340 (0.5692)  loss_vfl_dn_4: 0.5836 (0.5900)  loss_vfl_dn_5: 0.5983 (0.6037)  time: 0.2652  data: 0.0097  max mem: 5570
Epoch: [0]  [ 1900/14786]  eta: 0:57:00  lr: 0.000010  loss: 31.5623 (35.1177)  loss_bbox: 0.3821 (0.6313)  loss_bbox_aux_0: 0.4518 (0.6960)  loss_bbox_aux_1: 0.4223 (0.6624)  loss_bbox_aux_2: 0.4025 (0.6471)  loss_bbox_aux_3: 0.4013 (0.6388)  loss_bbox_aux_4: 0.3917 (0.6343)  loss_bbox_aux_5: 0.5065 (0.7698)  loss_bbox_dn_0: 0.6220 (0.7838)  loss_bbox_dn_1: 0.5895 (0.7603)  loss_bbox_dn_2: 0.5725 (0.7548)  loss_bbox_dn_3: 0.5647 (0.7533)  loss_bbox_dn_4: 0.5605 (0.7533)  loss_bbox_dn_5: 0.5604 (0.7536)  loss_giou: 0.8849 (1.1626)  loss_giou_aux_0: 0.9476 (1.2283)  loss_giou_aux_1: 0.9399 (1.1934)  loss_giou_aux_2: 0.9182 (1.1765)  loss_giou_aux_3: 0.9000 (1.1682)  loss_giou_aux_4: 0.8926 (1.1643)  loss_giou_aux_5: 1.0632 (1.3038)  loss_giou_dn_0: 1.1712 (1.2405)  loss_giou_dn_1: 1.0922 (1.2075)  loss_giou_dn_2: 1.0713 (1.1987)  loss_giou_dn_3: 1.0691 (1.1950)  loss_giou_dn_4: 1.0622 (1.1948)  loss_giou_dn_5: 1.0631 (1.1958)  loss_vfl: 1.1669 (1.0719)  loss_vfl_aux_0: 1.0558 (0.9339)  loss_vfl_aux_1: 1.0877 (0.9897)  loss_vfl_aux_2: 1.1414 (1.0251)  loss_vfl_aux_3: 1.1415 (1.0452)  loss_vfl_aux_4: 1.1360 (1.0576)  loss_vfl_aux_5: 0.9856 (0.8340)  loss_vfl_dn_0: 0.4712 (0.4817)  loss_vfl_dn_1: 0.4922 (0.5076)  loss_vfl_dn_2: 0.5194 (0.5386)  loss_vfl_dn_3: 0.5581 (0.5688)  loss_vfl_dn_4: 0.5989 (0.5904)  loss_vfl_dn_5: 0.6163 (0.6048)  time: 0.2616  data: 0.0087  max mem: 5570
