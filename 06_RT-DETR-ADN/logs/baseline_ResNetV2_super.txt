WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
make layer i: 0, num_blocks: 3, num_shared: 2, num_skippable: 1
make layer i: 1, num_blocks: 4, num_shared: 2, num_skippable: 2
make layer i: 2, num_blocks: 6, num_shared: 3, num_skippable: 3
make layer i: 3, num_blocks: 3, num_shared: 2, num_skippable: 1
Load state_dict from https://download.pytorch.org/models/resnet50-11ad3fa6.pth
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d(64, eps=1e-05)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(256, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=1e-05)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=1e-05)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(512, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=1e-05)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=1e-05)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(1024, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=1e-05)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=1e-05)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): SkippableSequentialBlocks(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(2048, eps=1e-05)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=1e-05)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=1e-05)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/train2017', 'ann_file': '/media/data/coco/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/media/data/coco/images/val2017', 'ann_file': '/media/data/coco/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'ResNet', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.29s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
module.backbone.layer1.0.conv1.weight 4096
module.backbone.layer1.0.conv2.weight 36864
module.backbone.layer1.0.conv3.weight 16384
module.backbone.layer1.0.downsample.0.weight 16384
module.backbone.layer1.1.conv1.weight 16384
module.backbone.layer1.1.conv2.weight 36864
module.backbone.layer1.1.conv3.weight 16384
module.backbone.layer1.2.conv1.weight 16384
module.backbone.layer1.2.conv2.weight 36864
module.backbone.layer1.2.conv3.weight 16384
module.backbone.layer2.0.conv1.weight 32768
module.backbone.layer2.0.conv2.weight 147456
module.backbone.layer2.0.conv3.weight 65536
module.backbone.layer2.0.downsample.0.weight 131072
module.backbone.layer2.1.conv1.weight 65536
module.backbone.layer2.1.conv2.weight 147456
module.backbone.layer2.1.conv3.weight 65536
module.backbone.layer2.2.conv1.weight 65536
module.backbone.layer2.2.conv2.weight 147456
module.backbone.layer2.2.conv3.weight 65536
module.backbone.layer2.3.conv1.weight 65536
module.backbone.layer2.3.conv2.weight 147456
module.backbone.layer2.3.conv3.weight 65536
module.backbone.layer3.0.conv1.weight 131072
module.backbone.layer3.0.conv2.weight 589824
module.backbone.layer3.0.conv3.weight 262144
module.backbone.layer3.0.downsample.0.weight 524288
module.backbone.layer3.1.conv1.weight 262144
module.backbone.layer3.1.conv2.weight 589824
module.backbone.layer3.1.conv3.weight 262144
module.backbone.layer3.2.conv1.weight 262144
module.backbone.layer3.2.conv2.weight 589824
module.backbone.layer3.2.conv3.weight 262144
module.backbone.layer3.3.conv1.weight 262144
module.backbone.layer3.3.conv2.weight 589824
module.backbone.layer3.3.conv3.weight 262144
module.backbone.layer3.4.conv1.weight 262144
module.backbone.layer3.4.conv2.weight 589824
module.backbone.layer3.4.conv3.weight 262144
module.backbone.layer3.5.conv1.weight 262144
module.backbone.layer3.5.conv2.weight 589824
module.backbone.layer3.5.conv3.weight 262144
module.backbone.layer4.0.conv1.weight 524288
module.backbone.layer4.0.conv2.weight 2359296
module.backbone.layer4.0.conv3.weight 1048576
module.backbone.layer4.0.downsample.0.weight 2097152
module.backbone.layer4.1.conv1.weight 1048576
module.backbone.layer4.1.conv2.weight 2359296
module.backbone.layer4.1.conv3.weight 1048576
module.backbone.layer4.2.conv1.weight 1048576
module.backbone.layer4.2.conv2.weight 2359296
module.backbone.layer4.2.conv3.weight 1048576
module.decoder.input_proj.0.conv.weight 65536
module.decoder.input_proj.0.norm.weight 256
module.decoder.input_proj.0.norm.bias 256
module.decoder.input_proj.1.conv.weight 65536
module.decoder.input_proj.1.norm.weight 256
module.decoder.input_proj.1.norm.bias 256
module.decoder.input_proj.2.conv.weight 65536
module.decoder.input_proj.2.norm.weight 256
module.decoder.input_proj.2.norm.bias 256
module.decoder.decoder.layers.0.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.0.self_attn.in_proj_bias 768
module.decoder.decoder.layers.0.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.0.self_attn.out_proj.bias 256
module.decoder.decoder.layers.0.norm1.weight 256
module.decoder.decoder.layers.0.norm1.bias 256
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.0.norm2.weight 256
module.decoder.decoder.layers.0.norm2.bias 256
module.decoder.decoder.layers.0.linear1.weight 262144
module.decoder.decoder.layers.0.linear1.bias 1024
module.decoder.decoder.layers.0.linear2.weight 262144
module.decoder.decoder.layers.0.linear2.bias 256
module.decoder.decoder.layers.0.norm3.weight 256
module.decoder.decoder.layers.0.norm3.bias 256
module.decoder.decoder.layers.1.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.1.self_attn.in_proj_bias 768
module.decoder.decoder.layers.1.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.1.self_attn.out_proj.bias 256
module.decoder.decoder.layers.1.norm1.weight 256
module.decoder.decoder.layers.1.norm1.bias 256
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.1.norm2.weight 256
module.decoder.decoder.layers.1.norm2.bias 256
module.decoder.decoder.layers.1.linear1.weight 262144
module.decoder.decoder.layers.1.linear1.bias 1024
module.decoder.decoder.layers.1.linear2.weight 262144
module.decoder.decoder.layers.1.linear2.bias 256
module.decoder.decoder.layers.1.norm3.weight 256
module.decoder.decoder.layers.1.norm3.bias 256
module.decoder.decoder.layers.2.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.2.self_attn.in_proj_bias 768
module.decoder.decoder.layers.2.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.2.self_attn.out_proj.bias 256
module.decoder.decoder.layers.2.norm1.weight 256
module.decoder.decoder.layers.2.norm1.bias 256
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.2.norm2.weight 256
module.decoder.decoder.layers.2.norm2.bias 256
module.decoder.decoder.layers.2.linear1.weight 262144
module.decoder.decoder.layers.2.linear1.bias 1024
module.decoder.decoder.layers.2.linear2.weight 262144
module.decoder.decoder.layers.2.linear2.bias 256
module.decoder.decoder.layers.2.norm3.weight 256
module.decoder.decoder.layers.2.norm3.bias 256
module.decoder.decoder.layers.3.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.3.self_attn.in_proj_bias 768
module.decoder.decoder.layers.3.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.3.self_attn.out_proj.bias 256
module.decoder.decoder.layers.3.norm1.weight 256
module.decoder.decoder.layers.3.norm1.bias 256
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.3.norm2.weight 256
module.decoder.decoder.layers.3.norm2.bias 256
module.decoder.decoder.layers.3.linear1.weight 262144
module.decoder.decoder.layers.3.linear1.bias 1024
module.decoder.decoder.layers.3.linear2.weight 262144
module.decoder.decoder.layers.3.linear2.bias 256
module.decoder.decoder.layers.3.norm3.weight 256
module.decoder.decoder.layers.3.norm3.bias 256
module.decoder.decoder.layers.4.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.4.self_attn.in_proj_bias 768
module.decoder.decoder.layers.4.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.4.self_attn.out_proj.bias 256
module.decoder.decoder.layers.4.norm1.weight 256
module.decoder.decoder.layers.4.norm1.bias 256
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.4.norm2.weight 256
module.decoder.decoder.layers.4.norm2.bias 256
module.decoder.decoder.layers.4.linear1.weight 262144
module.decoder.decoder.layers.4.linear1.bias 1024
module.decoder.decoder.layers.4.linear2.weight 262144
module.decoder.decoder.layers.4.linear2.bias 256
module.decoder.decoder.layers.4.norm3.weight 256
module.decoder.decoder.layers.4.norm3.bias 256
module.decoder.decoder.layers.5.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.5.self_attn.in_proj_bias 768
module.decoder.decoder.layers.5.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.5.self_attn.out_proj.bias 256
module.decoder.decoder.layers.5.norm1.weight 256
module.decoder.decoder.layers.5.norm1.bias 256
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.5.norm2.weight 256
module.decoder.decoder.layers.5.norm2.bias 256
module.decoder.decoder.layers.5.linear1.weight 262144
module.decoder.decoder.layers.5.linear1.bias 1024
module.decoder.decoder.layers.5.linear2.weight 262144
module.decoder.decoder.layers.5.linear2.bias 256
module.decoder.decoder.layers.5.norm3.weight 256
module.decoder.decoder.layers.5.norm3.bias 256
module.decoder.denoising_class_embed.weight 20736
module.decoder.query_pos_head.layers.0.weight 2048
module.decoder.query_pos_head.layers.0.bias 512
module.decoder.query_pos_head.layers.1.weight 131072
module.decoder.query_pos_head.layers.1.bias 256
module.decoder.enc_output.0.weight 65536
module.decoder.enc_output.0.bias 256
module.decoder.enc_output.1.weight 256
module.decoder.enc_output.1.bias 256
module.decoder.enc_score_head.weight 20480
module.decoder.enc_score_head.bias 80
module.decoder.enc_bbox_head.layers.0.weight 65536
module.decoder.enc_bbox_head.layers.0.bias 256
module.decoder.enc_bbox_head.layers.1.weight 65536
module.decoder.enc_bbox_head.layers.1.bias 256
module.decoder.enc_bbox_head.layers.2.weight 1024
module.decoder.enc_bbox_head.layers.2.bias 4
module.decoder.dec_score_head.0.weight 20480
module.decoder.dec_score_head.0.bias 80
module.decoder.dec_score_head.1.weight 20480
module.decoder.dec_score_head.1.bias 80
module.decoder.dec_score_head.2.weight 20480
module.decoder.dec_score_head.2.bias 80
module.decoder.dec_score_head.3.weight 20480
module.decoder.dec_score_head.3.bias 80
module.decoder.dec_score_head.4.weight 20480
module.decoder.dec_score_head.4.bias 80
module.decoder.dec_score_head.5.weight 20480
module.decoder.dec_score_head.5.bias 80
module.decoder.dec_bbox_head.0.layers.0.weight 65536
module.decoder.dec_bbox_head.0.layers.0.bias 256
module.decoder.dec_bbox_head.0.layers.1.weight 65536
module.decoder.dec_bbox_head.0.layers.1.bias 256
module.decoder.dec_bbox_head.0.layers.2.weight 1024
module.decoder.dec_bbox_head.0.layers.2.bias 4
module.decoder.dec_bbox_head.1.layers.0.weight 65536
module.decoder.dec_bbox_head.1.layers.0.bias 256
module.decoder.dec_bbox_head.1.layers.1.weight 65536
module.decoder.dec_bbox_head.1.layers.1.bias 256
module.decoder.dec_bbox_head.1.layers.2.weight 1024
module.decoder.dec_bbox_head.1.layers.2.bias 4
module.decoder.dec_bbox_head.2.layers.0.weight 65536
module.decoder.dec_bbox_head.2.layers.0.bias 256
module.decoder.dec_bbox_head.2.layers.1.weight 65536
module.decoder.dec_bbox_head.2.layers.1.bias 256
module.decoder.dec_bbox_head.2.layers.2.weight 1024
module.decoder.dec_bbox_head.2.layers.2.bias 4
module.decoder.dec_bbox_head.3.layers.0.weight 65536
module.decoder.dec_bbox_head.3.layers.0.bias 256
module.decoder.dec_bbox_head.3.layers.1.weight 65536
module.decoder.dec_bbox_head.3.layers.1.bias 256
module.decoder.dec_bbox_head.3.layers.2.weight 1024
module.decoder.dec_bbox_head.3.layers.2.bias 4
module.decoder.dec_bbox_head.4.layers.0.weight 65536
module.decoder.dec_bbox_head.4.layers.0.bias 256
module.decoder.dec_bbox_head.4.layers.1.weight 65536
module.decoder.dec_bbox_head.4.layers.1.bias 256
module.decoder.dec_bbox_head.4.layers.2.weight 1024
module.decoder.dec_bbox_head.4.layers.2.bias 4
module.decoder.dec_bbox_head.5.layers.0.weight 65536
module.decoder.dec_bbox_head.5.layers.0.bias 256
module.decoder.dec_bbox_head.5.layers.1.weight 65536
module.decoder.dec_bbox_head.5.layers.1.bias 256
module.decoder.dec_bbox_head.5.layers.2.weight 1024
module.decoder.dec_bbox_head.5.layers.2.bias 4
module.encoder.input_proj.0.0.weight 131072
module.encoder.input_proj.0.1.weight 256
module.encoder.input_proj.0.1.bias 256
module.encoder.input_proj.1.0.weight 262144
module.encoder.input_proj.1.1.weight 256
module.encoder.input_proj.1.1.bias 256
module.encoder.input_proj.2.0.weight 524288
module.encoder.input_proj.2.1.weight 256
module.encoder.input_proj.2.1.bias 256
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
module.encoder.encoder.0.layers.0.linear1.weight 262144
module.encoder.encoder.0.layers.0.linear1.bias 1024
module.encoder.encoder.0.layers.0.linear2.weight 262144
module.encoder.encoder.0.layers.0.linear2.bias 256
module.encoder.encoder.0.layers.0.norm1.weight 256
module.encoder.encoder.0.layers.0.norm1.bias 256
module.encoder.encoder.0.layers.0.norm2.weight 256
module.encoder.encoder.0.layers.0.norm2.bias 256
module.encoder.lateral_convs.0.conv.weight 65536
module.encoder.lateral_convs.0.norm.weight 256
module.encoder.lateral_convs.0.norm.bias 256
module.encoder.lateral_convs.1.conv.weight 65536
module.encoder.lateral_convs.1.norm.weight 256
module.encoder.lateral_convs.1.norm.bias 256
module.encoder.fpn_blocks.0.conv1.conv.weight 131072
module.encoder.fpn_blocks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.conv2.conv.weight 131072
module.encoder.fpn_blocks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.fpn_blocks.1.conv1.conv.weight 131072
module.encoder.fpn_blocks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.conv2.conv.weight 131072
module.encoder.fpn_blocks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
module.encoder.downsample_convs.0.conv.weight 589824
module.encoder.downsample_convs.0.norm.weight 256
module.encoder.downsample_convs.0.norm.bias 256
module.encoder.downsample_convs.1.conv.weight 589824
module.encoder.downsample_convs.1.norm.weight 256
module.encoder.downsample_convs.1.norm.bias 256
module.encoder.pan_blocks.0.conv1.conv.weight 131072
module.encoder.pan_blocks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.conv2.conv.weight 131072
module.encoder.pan_blocks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.pan_blocks.1.conv1.conv.weight 131072
module.encoder.pan_blocks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.conv2.conv.weight 131072
module.encoder.pan_blocks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 42862860
super_config : [False, False, False, False]
base_config : [True, True, True, True]
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch: [0]  [    0/14786]  eta: 7:43:52  lr: 0.000010  loss: 42.8717 (42.8717)  loss_bbox: 1.5230 (1.5230)  loss_bbox_aux_0: 1.5041 (1.5041)  loss_bbox_aux_1: 1.5319 (1.5319)  loss_bbox_aux_2: 1.5286 (1.5286)  loss_bbox_aux_3: 1.5443 (1.5443)  loss_bbox_aux_4: 1.5230 (1.5230)  loss_bbox_aux_5: 1.5248 (1.5248)  loss_bbox_dn_0: 0.7368 (0.7368)  loss_bbox_dn_1: 0.7368 (0.7368)  loss_bbox_dn_2: 0.7368 (0.7368)  loss_bbox_dn_3: 0.7368 (0.7368)  loss_bbox_dn_4: 0.7368 (0.7368)  loss_bbox_dn_5: 0.7368 (0.7368)  loss_giou: 1.8500 (1.8500)  loss_giou_aux_0: 1.8653 (1.8653)  loss_giou_aux_1: 1.8405 (1.8405)  loss_giou_aux_2: 1.8327 (1.8327)  loss_giou_aux_3: 1.8495 (1.8495)  loss_giou_aux_4: 1.8764 (1.8764)  loss_giou_aux_5: 1.8879 (1.8879)  loss_giou_dn_0: 1.3004 (1.3004)  loss_giou_dn_1: 1.3004 (1.3004)  loss_giou_dn_2: 1.3004 (1.3004)  loss_giou_dn_3: 1.3004 (1.3004)  loss_giou_dn_4: 1.3004 (1.3004)  loss_giou_dn_5: 1.3004 (1.3004)  loss_vfl: 0.2034 (0.2034)  loss_vfl_aux_0: 0.1897 (0.1897)  loss_vfl_aux_1: 0.2141 (0.2141)  loss_vfl_aux_2: 0.2210 (0.2210)  loss_vfl_aux_3: 0.2169 (0.2169)  loss_vfl_aux_4: 0.1883 (0.1883)  loss_vfl_aux_5: 0.2403 (0.2403)  loss_vfl_dn_0: 0.9177 (0.9177)  loss_vfl_dn_1: 0.9107 (0.9107)  loss_vfl_dn_2: 0.9539 (0.9539)  loss_vfl_dn_3: 0.9151 (0.9151)  loss_vfl_dn_4: 0.9227 (0.9227)  loss_vfl_dn_5: 0.8724 (0.8724)  time: 1.8824  data: 0.6628  max mem: 3414
Epoch: [0]  [  100/14786]  eta: 1:08:56  lr: 0.000010  loss: 36.7216 (41.3818)  loss_bbox: 0.9127 (1.2820)  loss_bbox_aux_0: 0.9499 (1.3419)  loss_bbox_aux_1: 0.9299 (1.3151)  loss_bbox_aux_2: 0.9160 (1.2989)  loss_bbox_aux_3: 0.9172 (1.2915)  loss_bbox_aux_4: 0.9076 (1.2858)  loss_bbox_aux_5: 0.9666 (1.3912)  loss_bbox_dn_0: 0.7305 (0.9227)  loss_bbox_dn_1: 0.7303 (0.9355)  loss_bbox_dn_2: 0.7299 (0.9454)  loss_bbox_dn_3: 0.7297 (0.9536)  loss_bbox_dn_4: 0.7291 (0.9606)  loss_bbox_dn_5: 0.7291 (0.9673)  loss_giou: 1.4770 (1.5195)  loss_giou_aux_0: 1.4908 (1.5563)  loss_giou_aux_1: 1.4875 (1.5397)  loss_giou_aux_2: 1.4842 (1.5312)  loss_giou_aux_3: 1.4806 (1.5259)  loss_giou_aux_4: 1.4801 (1.5236)  loss_giou_aux_5: 1.5272 (1.5864)  loss_giou_dn_0: 1.3449 (1.3364)  loss_giou_dn_1: 1.3444 (1.3422)  loss_giou_dn_2: 1.3508 (1.3479)  loss_giou_dn_3: 1.3529 (1.3533)  loss_giou_dn_4: 1.3554 (1.3588)  loss_giou_dn_5: 1.3580 (1.3640)  loss_vfl: 0.6005 (0.5900)  loss_vfl_aux_0: 0.5324 (0.5223)  loss_vfl_aux_1: 0.5538 (0.5464)  loss_vfl_aux_2: 0.5696 (0.5710)  loss_vfl_aux_3: 0.5709 (0.5754)  loss_vfl_aux_4: 0.5810 (0.5863)  loss_vfl_aux_5: 0.4995 (0.4875)  loss_vfl_dn_0: 0.5398 (0.6308)  loss_vfl_dn_1: 0.5389 (0.6210)  loss_vfl_dn_2: 0.5455 (0.6277)  loss_vfl_dn_3: 0.5504 (0.6275)  loss_vfl_dn_4: 0.5545 (0.6149)  loss_vfl_dn_5: 0.5635 (0.6043)  time: 0.2635  data: 0.0087  max mem: 5570
Epoch: [0]  [  200/14786]  eta: 1:06:10  lr: 0.000010  loss: 37.2390 (39.7782)  loss_bbox: 0.7962 (1.0839)  loss_bbox_aux_0: 0.8503 (1.1346)  loss_bbox_aux_1: 0.8525 (1.1133)  loss_bbox_aux_2: 0.8282 (1.1003)  loss_bbox_aux_3: 0.8036 (1.0925)  loss_bbox_aux_4: 0.8211 (1.0871)  loss_bbox_aux_5: 0.8855 (1.1736)  loss_bbox_dn_0: 0.8722 (0.8894)  loss_bbox_dn_1: 0.8652 (0.8943)  loss_bbox_dn_2: 0.8613 (0.8981)  loss_bbox_dn_3: 0.8585 (0.9016)  loss_bbox_dn_4: 0.8589 (0.9050)  loss_bbox_dn_5: 0.8578 (0.9083)  loss_giou: 1.3725 (1.4710)  loss_giou_aux_0: 1.4237 (1.5021)  loss_giou_aux_1: 1.4306 (1.4892)  loss_giou_aux_2: 1.4277 (1.4808)  loss_giou_aux_3: 1.3897 (1.4759)  loss_giou_aux_4: 1.3706 (1.4740)  loss_giou_aux_5: 1.4097 (1.5258)  loss_giou_dn_0: 1.3367 (1.3341)  loss_giou_dn_1: 1.3276 (1.3369)  loss_giou_dn_2: 1.3233 (1.3416)  loss_giou_dn_3: 1.3266 (1.3469)  loss_giou_dn_4: 1.3281 (1.3518)  loss_giou_dn_5: 1.3344 (1.3566)  loss_vfl: 0.7037 (0.6996)  loss_vfl_aux_0: 0.6248 (0.6155)  loss_vfl_aux_1: 0.6703 (0.6455)  loss_vfl_aux_2: 0.6849 (0.6727)  loss_vfl_aux_3: 0.6855 (0.6793)  loss_vfl_aux_4: 0.6997 (0.6985)  loss_vfl_aux_5: 0.5926 (0.5744)  loss_vfl_dn_0: 0.4975 (0.5782)  loss_vfl_dn_1: 0.5113 (0.5819)  loss_vfl_dn_2: 0.5306 (0.5903)  loss_vfl_dn_3: 0.5471 (0.5972)  loss_vfl_dn_4: 0.5574 (0.5897)  loss_vfl_dn_5: 0.5597 (0.5867)  time: 0.2614  data: 0.0086  max mem: 5570
