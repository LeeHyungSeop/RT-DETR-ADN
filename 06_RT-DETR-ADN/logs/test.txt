Not init distributed mode.
Start training
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/train2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/val2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'SwinTransformer', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'SwinTransformer': {'num_stages': 4, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
name : pre.0.weight, param : torch.Size([96, 3, 4, 4])
name : pre.0.bias, param : torch.Size([96])
name : pre.2.weight, param : torch.Size([96])
name : pre.2.bias, param : torch.Size([96])
name : features.0.0.norm1.weight, param : torch.Size([96])
name : features.0.0.norm1.bias, param : torch.Size([96])
name : features.0.0.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.0.attn.relative_position_index, param : torch.Size([2401])
name : features.0.0.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.0.attn.qkv.bias, param : torch.Size([288])
name : features.0.0.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.0.attn.proj.bias, param : torch.Size([96])
name : features.0.0.norm2.weight, param : torch.Size([96])
name : features.0.0.norm2.bias, param : torch.Size([96])
name : features.0.0.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.0.mlp.0.bias, param : torch.Size([384])
name : features.0.0.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.0.mlp.3.bias, param : torch.Size([96])
name : features.0.0.norm1_skip.weight, param : torch.Size([96])
name : features.0.0.norm1_skip.bias, param : torch.Size([96])
name : features.0.0.norm2_skip.weight, param : torch.Size([96])
name : features.0.0.norm2_skip.bias, param : torch.Size([96])
name : features.0.1.norm1.weight, param : torch.Size([96])
name : features.0.1.norm1.bias, param : torch.Size([96])
name : features.0.1.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.1.attn.relative_position_index, param : torch.Size([2401])
name : features.0.1.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.1.attn.qkv.bias, param : torch.Size([288])
name : features.0.1.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.1.attn.proj.bias, param : torch.Size([96])
name : features.0.1.norm2.weight, param : torch.Size([96])
name : features.0.1.norm2.bias, param : torch.Size([96])
name : features.0.1.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.1.mlp.0.bias, param : torch.Size([384])
name : features.0.1.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.1.mlp.3.bias, param : torch.Size([96])
name : features.1.reduction.weight, param : torch.Size([192, 384])
name : features.1.norm.weight, param : torch.Size([384])
name : features.1.norm.bias, param : torch.Size([384])
name : features.1.norm_skip.weight, param : torch.Size([384])
name : features.1.norm_skip.bias, param : torch.Size([384])
name : features.2.0.norm1.weight, param : torch.Size([192])
name : features.2.0.norm1.bias, param : torch.Size([192])
name : features.2.0.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.0.attn.relative_position_index, param : torch.Size([2401])
name : features.2.0.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.0.attn.qkv.bias, param : torch.Size([576])
name : features.2.0.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.0.attn.proj.bias, param : torch.Size([192])
name : features.2.0.norm2.weight, param : torch.Size([192])
name : features.2.0.norm2.bias, param : torch.Size([192])
name : features.2.0.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.0.mlp.0.bias, param : torch.Size([768])
name : features.2.0.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.0.mlp.3.bias, param : torch.Size([192])
name : features.2.0.norm1_skip.weight, param : torch.Size([192])
name : features.2.0.norm1_skip.bias, param : torch.Size([192])
name : features.2.0.norm2_skip.weight, param : torch.Size([192])
name : features.2.0.norm2_skip.bias, param : torch.Size([192])
name : features.2.1.norm1.weight, param : torch.Size([192])
name : features.2.1.norm1.bias, param : torch.Size([192])
name : features.2.1.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.1.attn.relative_position_index, param : torch.Size([2401])
name : features.2.1.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.1.attn.qkv.bias, param : torch.Size([576])
name : features.2.1.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.1.attn.proj.bias, param : torch.Size([192])
name : features.2.1.norm2.weight, param : torch.Size([192])
name : features.2.1.norm2.bias, param : torch.Size([192])
name : features.2.1.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.1.mlp.0.bias, param : torch.Size([768])
name : features.2.1.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.1.mlp.3.bias, param : torch.Size([192])
name : features.3.reduction.weight, param : torch.Size([384, 768])
name : features.3.norm.weight, param : torch.Size([768])
name : features.3.norm.bias, param : torch.Size([768])
name : features.3.norm_skip.weight, param : torch.Size([768])
name : features.3.norm_skip.bias, param : torch.Size([768])
name : features.4.0.norm1.weight, param : torch.Size([384])
name : features.4.0.norm1.bias, param : torch.Size([384])
name : features.4.0.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.0.attn.relative_position_index, param : torch.Size([2401])
name : features.4.0.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.0.attn.qkv.bias, param : torch.Size([1152])
name : features.4.0.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.0.attn.proj.bias, param : torch.Size([384])
name : features.4.0.norm2.weight, param : torch.Size([384])
name : features.4.0.norm2.bias, param : torch.Size([384])
name : features.4.0.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.0.mlp.0.bias, param : torch.Size([1536])
name : features.4.0.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.0.mlp.3.bias, param : torch.Size([384])
name : features.4.0.norm1_skip.weight, param : torch.Size([384])
name : features.4.0.norm1_skip.bias, param : torch.Size([384])
name : features.4.0.norm2_skip.weight, param : torch.Size([384])
name : features.4.0.norm2_skip.bias, param : torch.Size([384])
name : features.4.1.norm1.weight, param : torch.Size([384])
name : features.4.1.norm1.bias, param : torch.Size([384])
name : features.4.1.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.1.attn.relative_position_index, param : torch.Size([2401])
name : features.4.1.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.1.attn.qkv.bias, param : torch.Size([1152])
name : features.4.1.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.1.attn.proj.bias, param : torch.Size([384])
name : features.4.1.norm2.weight, param : torch.Size([384])
name : features.4.1.norm2.bias, param : torch.Size([384])
name : features.4.1.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.1.mlp.0.bias, param : torch.Size([1536])
name : features.4.1.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.1.mlp.3.bias, param : torch.Size([384])
name : features.4.1.norm1_skip.weight, param : torch.Size([384])
name : features.4.1.norm1_skip.bias, param : torch.Size([384])
name : features.4.1.norm2_skip.weight, param : torch.Size([384])
name : features.4.1.norm2_skip.bias, param : torch.Size([384])
name : features.4.2.norm1.weight, param : torch.Size([384])
name : features.4.2.norm1.bias, param : torch.Size([384])
name : features.4.2.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.2.attn.relative_position_index, param : torch.Size([2401])
name : features.4.2.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.2.attn.qkv.bias, param : torch.Size([1152])
name : features.4.2.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.2.attn.proj.bias, param : torch.Size([384])
name : features.4.2.norm2.weight, param : torch.Size([384])
name : features.4.2.norm2.bias, param : torch.Size([384])
name : features.4.2.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.2.mlp.0.bias, param : torch.Size([1536])
name : features.4.2.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.2.mlp.3.bias, param : torch.Size([384])
name : features.4.2.norm1_skip.weight, param : torch.Size([384])
name : features.4.2.norm1_skip.bias, param : torch.Size([384])
name : features.4.2.norm2_skip.weight, param : torch.Size([384])
name : features.4.2.norm2_skip.bias, param : torch.Size([384])
name : features.4.3.norm1.weight, param : torch.Size([384])
name : features.4.3.norm1.bias, param : torch.Size([384])
name : features.4.3.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.3.attn.relative_position_index, param : torch.Size([2401])
name : features.4.3.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.3.attn.qkv.bias, param : torch.Size([1152])
name : features.4.3.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.3.attn.proj.bias, param : torch.Size([384])
name : features.4.3.norm2.weight, param : torch.Size([384])
name : features.4.3.norm2.bias, param : torch.Size([384])
name : features.4.3.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.3.mlp.0.bias, param : torch.Size([1536])
name : features.4.3.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.3.mlp.3.bias, param : torch.Size([384])
name : features.4.4.norm1.weight, param : torch.Size([384])
name : features.4.4.norm1.bias, param : torch.Size([384])
name : features.4.4.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.4.attn.relative_position_index, param : torch.Size([2401])
name : features.4.4.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.4.attn.qkv.bias, param : torch.Size([1152])
name : features.4.4.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.4.attn.proj.bias, param : torch.Size([384])
name : features.4.4.norm2.weight, param : torch.Size([384])
name : features.4.4.norm2.bias, param : torch.Size([384])
name : features.4.4.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.4.mlp.0.bias, param : torch.Size([1536])
name : features.4.4.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.4.mlp.3.bias, param : torch.Size([384])
name : features.4.5.norm1.weight, param : torch.Size([384])
name : features.4.5.norm1.bias, param : torch.Size([384])
name : features.4.5.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.5.attn.relative_position_index, param : torch.Size([2401])
name : features.4.5.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.5.attn.qkv.bias, param : torch.Size([1152])
name : features.4.5.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.5.attn.proj.bias, param : torch.Size([384])
name : features.4.5.norm2.weight, param : torch.Size([384])
name : features.4.5.norm2.bias, param : torch.Size([384])
name : features.4.5.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.5.mlp.0.bias, param : torch.Size([1536])
name : features.4.5.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.5.mlp.3.bias, param : torch.Size([384])
name : features.5.reduction.weight, param : torch.Size([768, 1536])
name : features.5.norm.weight, param : torch.Size([1536])
name : features.5.norm.bias, param : torch.Size([1536])
name : features.5.norm_skip.weight, param : torch.Size([1536])
name : features.5.norm_skip.bias, param : torch.Size([1536])
name : features.6.0.norm1.weight, param : torch.Size([768])
name : features.6.0.norm1.bias, param : torch.Size([768])
name : features.6.0.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.0.attn.relative_position_index, param : torch.Size([2401])
name : features.6.0.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.0.attn.qkv.bias, param : torch.Size([2304])
name : features.6.0.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.0.attn.proj.bias, param : torch.Size([768])
name : features.6.0.norm2.weight, param : torch.Size([768])
name : features.6.0.norm2.bias, param : torch.Size([768])
name : features.6.0.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.0.mlp.0.bias, param : torch.Size([3072])
name : features.6.0.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.0.mlp.3.bias, param : torch.Size([768])
name : features.6.0.norm1_skip.weight, param : torch.Size([768])
name : features.6.0.norm1_skip.bias, param : torch.Size([768])
name : features.6.0.norm2_skip.weight, param : torch.Size([768])
name : features.6.0.norm2_skip.bias, param : torch.Size([768])
name : features.6.1.norm1.weight, param : torch.Size([768])
name : features.6.1.norm1.bias, param : torch.Size([768])
name : features.6.1.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.1.attn.relative_position_index, param : torch.Size([2401])
name : features.6.1.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.1.attn.qkv.bias, param : torch.Size([2304])
name : features.6.1.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.1.attn.proj.bias, param : torch.Size([768])
name : features.6.1.norm2.weight, param : torch.Size([768])
name : features.6.1.norm2.bias, param : torch.Size([768])
name : features.6.1.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.1.mlp.0.bias, param : torch.Size([3072])
name : features.6.1.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.1.mlp.3.bias, param : torch.Size([768])
name : norm.weight, param : torch.Size([768])
name : norm.bias, param : torch.Size([768])
Load state_dict from /home/hslee/Desktop/Embedded_AI/INU_4-1/RISE/RetinaNet-ADN/02_AdaptiveDepthNetwork/pretrained/checkpoint_swin-t-epoch297.pth
self.model (in solver.py): 
RTDETR(
  (backbone): SwinTransformer(
    (pre): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): Permute()
      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (features): SkippableSequentialStages(
      (0): SkippableSequentialBlocks(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.0, mode=row)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (proj): Linear(in_features=96, out_features=96, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.009090909090909092, mode=row)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=96, out_features=384, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=384, out_features=96, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (1): PatchMerging(
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (norm_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (2): SkippableSequentialBlocks(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.02727272727272728, mode=row)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=768, out_features=192, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (3): PatchMerging(
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (4): SkippableSequentialBlocks(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.045454545454545456, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (2): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (3): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.06363636363636364, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.08181818181818182, mode=row)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=384, out_features=1536, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=1536, out_features=384, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (5): PatchMerging(
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (norm_skip): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
      (6): SkippableSequentialBlocks(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (norm1_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm2_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftedWindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (proj): Linear(in_features=768, out_features=768, bias=True)
          )
          (stochastic_depth): StochasticDepth(p=0.1, mode=row)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (0): Linear(in_features=768, out_features=3072, bias=True)
            (1): GELU(approximate='none')
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=3072, out_features=768, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (permute): Permute()
  )
  (decoder): RTDETRTransformer(
    (input_proj): ModuleList(
      (0-2): 3 x Sequential(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.0, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MSDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
            (attention_weights): Linear(in_features=256, out_features=96, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.0, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.0, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (denoising_class_embed): Embedding(81, 256, padding_idx=80)
    (query_pos_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=4, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=256, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (enc_output): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
    (enc_bbox_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (dec_score_head): ModuleList(
      (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
    )
    (dec_bbox_head): ModuleList(
      (0-5): 6 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
    )
  )
  (encoder): HybridEncoder(
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (input_proj_swinT): ModuleList(
      (0): Sequential(
        (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (encoder): ModuleList(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (activation): GELU(approximate='none')
          )
        )
      )
    )
    (lateral_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (fpn_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
    (downsample_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (pan_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
  )
)
backbone.pre.0.weight
backbone.pre.0.bias
backbone.pre.2.weight
backbone.pre.2.bias
backbone.features.0.0.norm1.weight
backbone.features.0.0.norm1.bias
backbone.features.0.0.attn.relative_position_bias_table
backbone.features.0.0.attn.qkv.weight
backbone.features.0.0.attn.qkv.bias
backbone.features.0.0.attn.proj.weight
backbone.features.0.0.attn.proj.bias
backbone.features.0.0.norm2.weight
backbone.features.0.0.norm2.bias
backbone.features.0.0.mlp.0.weight
backbone.features.0.0.mlp.0.bias
backbone.features.0.0.mlp.3.weight
backbone.features.0.0.mlp.3.bias
backbone.features.0.0.norm1_skip.weight
backbone.features.0.0.norm1_skip.bias
backbone.features.0.0.norm2_skip.weight
backbone.features.0.0.norm2_skip.bias
backbone.features.0.1.norm1.weight
backbone.features.0.1.norm1.bias
backbone.features.0.1.attn.relative_position_bias_table
backbone.features.0.1.attn.qkv.weight
backbone.features.0.1.attn.qkv.bias
backbone.features.0.1.attn.proj.weight
backbone.features.0.1.attn.proj.bias
backbone.features.0.1.norm2.weight
backbone.features.0.1.norm2.bias
backbone.features.0.1.mlp.0.weight
backbone.features.0.1.mlp.0.bias
backbone.features.0.1.mlp.3.weight
backbone.features.0.1.mlp.3.bias
backbone.features.1.reduction.weight
backbone.features.1.norm.weight
backbone.features.1.norm.bias
backbone.features.1.norm_skip.weight
backbone.features.1.norm_skip.bias
backbone.features.2.0.norm1.weight
backbone.features.2.0.norm1.bias
backbone.features.2.0.attn.relative_position_bias_table
backbone.features.2.0.attn.qkv.weight
backbone.features.2.0.attn.qkv.bias
backbone.features.2.0.attn.proj.weight
backbone.features.2.0.attn.proj.bias
backbone.features.2.0.norm2.weight
backbone.features.2.0.norm2.bias
backbone.features.2.0.mlp.0.weight
backbone.features.2.0.mlp.0.bias
backbone.features.2.0.mlp.3.weight
backbone.features.2.0.mlp.3.bias
backbone.features.2.0.norm1_skip.weight
backbone.features.2.0.norm1_skip.bias
backbone.features.2.0.norm2_skip.weight
backbone.features.2.0.norm2_skip.bias
backbone.features.2.1.norm1.weight
backbone.features.2.1.norm1.bias
backbone.features.2.1.attn.relative_position_bias_table
backbone.features.2.1.attn.qkv.weight
backbone.features.2.1.attn.qkv.bias
backbone.features.2.1.attn.proj.weight
backbone.features.2.1.attn.proj.bias
backbone.features.2.1.norm2.weight
backbone.features.2.1.norm2.bias
backbone.features.2.1.mlp.0.weight
backbone.features.2.1.mlp.0.bias
backbone.features.2.1.mlp.3.weight
backbone.features.2.1.mlp.3.bias
backbone.features.3.reduction.weight
backbone.features.3.norm.weight
backbone.features.3.norm.bias
backbone.features.3.norm_skip.weight
backbone.features.3.norm_skip.bias
backbone.features.4.0.norm1.weight
backbone.features.4.0.norm1.bias
backbone.features.4.0.attn.relative_position_bias_table
backbone.features.4.0.attn.qkv.weight
backbone.features.4.0.attn.qkv.bias
backbone.features.4.0.attn.proj.weight
backbone.features.4.0.attn.proj.bias
backbone.features.4.0.norm2.weight
backbone.features.4.0.norm2.bias
backbone.features.4.0.mlp.0.weight
backbone.features.4.0.mlp.0.bias
backbone.features.4.0.mlp.3.weight
backbone.features.4.0.mlp.3.bias
backbone.features.4.0.norm1_skip.weight
backbone.features.4.0.norm1_skip.bias
backbone.features.4.0.norm2_skip.weight
backbone.features.4.0.norm2_skip.bias
backbone.features.4.1.norm1.weight
backbone.features.4.1.norm1.bias
backbone.features.4.1.attn.relative_position_bias_table
backbone.features.4.1.attn.qkv.weight
backbone.features.4.1.attn.qkv.bias
backbone.features.4.1.attn.proj.weight
backbone.features.4.1.attn.proj.bias
backbone.features.4.1.norm2.weight
backbone.features.4.1.norm2.bias
backbone.features.4.1.mlp.0.weight
backbone.features.4.1.mlp.0.bias
backbone.features.4.1.mlp.3.weight
backbone.features.4.1.mlp.3.bias
backbone.features.4.1.norm1_skip.weight
backbone.features.4.1.norm1_skip.bias
backbone.features.4.1.norm2_skip.weight
backbone.features.4.1.norm2_skip.bias
backbone.features.4.2.norm1.weight
backbone.features.4.2.norm1.bias
backbone.features.4.2.attn.relative_position_bias_table
backbone.features.4.2.attn.qkv.weight
backbone.features.4.2.attn.qkv.bias
backbone.features.4.2.attn.proj.weight
backbone.features.4.2.attn.proj.bias
backbone.features.4.2.norm2.weight
backbone.features.4.2.norm2.bias
backbone.features.4.2.mlp.0.weight
backbone.features.4.2.mlp.0.bias
backbone.features.4.2.mlp.3.weight
backbone.features.4.2.mlp.3.bias
backbone.features.4.2.norm1_skip.weight
backbone.features.4.2.norm1_skip.bias
backbone.features.4.2.norm2_skip.weight
backbone.features.4.2.norm2_skip.bias
backbone.features.4.3.norm1.weight
backbone.features.4.3.norm1.bias
backbone.features.4.3.attn.relative_position_bias_table
backbone.features.4.3.attn.qkv.weight
backbone.features.4.3.attn.qkv.bias
backbone.features.4.3.attn.proj.weight
backbone.features.4.3.attn.proj.bias
backbone.features.4.3.norm2.weight
backbone.features.4.3.norm2.bias
backbone.features.4.3.mlp.0.weight
backbone.features.4.3.mlp.0.bias
backbone.features.4.3.mlp.3.weight
backbone.features.4.3.mlp.3.bias
backbone.features.4.4.norm1.weight
backbone.features.4.4.norm1.bias
backbone.features.4.4.attn.relative_position_bias_table
backbone.features.4.4.attn.qkv.weight
backbone.features.4.4.attn.qkv.bias
backbone.features.4.4.attn.proj.weight
backbone.features.4.4.attn.proj.bias
backbone.features.4.4.norm2.weight
backbone.features.4.4.norm2.bias
backbone.features.4.4.mlp.0.weight
backbone.features.4.4.mlp.0.bias
backbone.features.4.4.mlp.3.weight
backbone.features.4.4.mlp.3.bias
backbone.features.4.5.norm1.weight
backbone.features.4.5.norm1.bias
backbone.features.4.5.attn.relative_position_bias_table
backbone.features.4.5.attn.qkv.weight
backbone.features.4.5.attn.qkv.bias
backbone.features.4.5.attn.proj.weight
backbone.features.4.5.attn.proj.bias
backbone.features.4.5.norm2.weight
backbone.features.4.5.norm2.bias
backbone.features.4.5.mlp.0.weight
backbone.features.4.5.mlp.0.bias
backbone.features.4.5.mlp.3.weight
backbone.features.4.5.mlp.3.bias
backbone.features.5.reduction.weight
backbone.features.5.norm.weight
backbone.features.5.norm.bias
backbone.features.5.norm_skip.weight
backbone.features.5.norm_skip.bias
backbone.features.6.0.norm1.weight
backbone.features.6.0.norm1.bias
backbone.features.6.0.attn.relative_position_bias_table
backbone.features.6.0.attn.qkv.weight
backbone.features.6.0.attn.qkv.bias
backbone.features.6.0.attn.proj.weight
backbone.features.6.0.attn.proj.bias
backbone.features.6.0.norm2.weight
backbone.features.6.0.norm2.bias
backbone.features.6.0.mlp.0.weight
backbone.features.6.0.mlp.0.bias
backbone.features.6.0.mlp.3.weight
backbone.features.6.0.mlp.3.bias
backbone.features.6.0.norm1_skip.weight
backbone.features.6.0.norm1_skip.bias
backbone.features.6.0.norm2_skip.weight
backbone.features.6.0.norm2_skip.bias
backbone.features.6.1.norm1.weight
backbone.features.6.1.norm1.bias
backbone.features.6.1.attn.relative_position_bias_table
backbone.features.6.1.attn.qkv.weight
backbone.features.6.1.attn.qkv.bias
backbone.features.6.1.attn.proj.weight
backbone.features.6.1.attn.proj.bias
backbone.features.6.1.norm2.weight
backbone.features.6.1.norm2.bias
backbone.features.6.1.mlp.0.weight
backbone.features.6.1.mlp.0.bias
backbone.features.6.1.mlp.3.weight
backbone.features.6.1.mlp.3.bias
backbone.norm.weight
backbone.norm.bias
decoder.input_proj.0.conv.weight
decoder.input_proj.0.norm.weight
decoder.input_proj.0.norm.bias
decoder.input_proj.1.conv.weight
decoder.input_proj.1.norm.weight
decoder.input_proj.1.norm.bias
decoder.input_proj.2.conv.weight
decoder.input_proj.2.norm.weight
decoder.input_proj.2.norm.bias
decoder.decoder.layers.0.self_attn.in_proj_weight
decoder.decoder.layers.0.self_attn.in_proj_bias
decoder.decoder.layers.0.self_attn.out_proj.weight
decoder.decoder.layers.0.self_attn.out_proj.bias
decoder.decoder.layers.0.norm1.weight
decoder.decoder.layers.0.norm1.bias
decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
decoder.decoder.layers.0.cross_attn.attention_weights.weight
decoder.decoder.layers.0.cross_attn.attention_weights.bias
decoder.decoder.layers.0.cross_attn.value_proj.weight
decoder.decoder.layers.0.cross_attn.value_proj.bias
decoder.decoder.layers.0.cross_attn.output_proj.weight
decoder.decoder.layers.0.cross_attn.output_proj.bias
decoder.decoder.layers.0.norm2.weight
decoder.decoder.layers.0.norm2.bias
decoder.decoder.layers.0.linear1.weight
decoder.decoder.layers.0.linear1.bias
decoder.decoder.layers.0.linear2.weight
decoder.decoder.layers.0.linear2.bias
decoder.decoder.layers.0.norm3.weight
decoder.decoder.layers.0.norm3.bias
decoder.decoder.layers.1.self_attn.in_proj_weight
decoder.decoder.layers.1.self_attn.in_proj_bias
decoder.decoder.layers.1.self_attn.out_proj.weight
decoder.decoder.layers.1.self_attn.out_proj.bias
decoder.decoder.layers.1.norm1.weight
decoder.decoder.layers.1.norm1.bias
decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
decoder.decoder.layers.1.cross_attn.attention_weights.weight
decoder.decoder.layers.1.cross_attn.attention_weights.bias
decoder.decoder.layers.1.cross_attn.value_proj.weight
decoder.decoder.layers.1.cross_attn.value_proj.bias
decoder.decoder.layers.1.cross_attn.output_proj.weight
decoder.decoder.layers.1.cross_attn.output_proj.bias
decoder.decoder.layers.1.norm2.weight
decoder.decoder.layers.1.norm2.bias
decoder.decoder.layers.1.linear1.weight
decoder.decoder.layers.1.linear1.bias
decoder.decoder.layers.1.linear2.weight
decoder.decoder.layers.1.linear2.bias
decoder.decoder.layers.1.norm3.weight
decoder.decoder.layers.1.norm3.bias
decoder.decoder.layers.2.self_attn.in_proj_weight
decoder.decoder.layers.2.self_attn.in_proj_bias
decoder.decoder.layers.2.self_attn.out_proj.weight
decoder.decoder.layers.2.self_attn.out_proj.bias
decoder.decoder.layers.2.norm1.weight
decoder.decoder.layers.2.norm1.bias
decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
decoder.decoder.layers.2.cross_attn.attention_weights.weight
decoder.decoder.layers.2.cross_attn.attention_weights.bias
decoder.decoder.layers.2.cross_attn.value_proj.weight
decoder.decoder.layers.2.cross_attn.value_proj.bias
decoder.decoder.layers.2.cross_attn.output_proj.weight
decoder.decoder.layers.2.cross_attn.output_proj.bias
decoder.decoder.layers.2.norm2.weight
decoder.decoder.layers.2.norm2.bias
decoder.decoder.layers.2.linear1.weight
decoder.decoder.layers.2.linear1.bias
decoder.decoder.layers.2.linear2.weight
decoder.decoder.layers.2.linear2.bias
decoder.decoder.layers.2.norm3.weight
decoder.decoder.layers.2.norm3.bias
decoder.decoder.layers.3.self_attn.in_proj_weight
decoder.decoder.layers.3.self_attn.in_proj_bias
decoder.decoder.layers.3.self_attn.out_proj.weight
decoder.decoder.layers.3.self_attn.out_proj.bias
decoder.decoder.layers.3.norm1.weight
decoder.decoder.layers.3.norm1.bias
decoder.decoder.layers.3.cross_attn.sampling_offsets.weight
decoder.decoder.layers.3.cross_attn.sampling_offsets.bias
decoder.decoder.layers.3.cross_attn.attention_weights.weight
decoder.decoder.layers.3.cross_attn.attention_weights.bias
decoder.decoder.layers.3.cross_attn.value_proj.weight
decoder.decoder.layers.3.cross_attn.value_proj.bias
decoder.decoder.layers.3.cross_attn.output_proj.weight
decoder.decoder.layers.3.cross_attn.output_proj.bias
decoder.decoder.layers.3.norm2.weight
decoder.decoder.layers.3.norm2.bias
decoder.decoder.layers.3.linear1.weight
decoder.decoder.layers.3.linear1.bias
decoder.decoder.layers.3.linear2.weight
decoder.decoder.layers.3.linear2.bias
decoder.decoder.layers.3.norm3.weight
decoder.decoder.layers.3.norm3.bias
decoder.decoder.layers.4.self_attn.in_proj_weight
decoder.decoder.layers.4.self_attn.in_proj_bias
decoder.decoder.layers.4.self_attn.out_proj.weight
decoder.decoder.layers.4.self_attn.out_proj.bias
decoder.decoder.layers.4.norm1.weight
decoder.decoder.layers.4.norm1.bias
decoder.decoder.layers.4.cross_attn.sampling_offsets.weight
decoder.decoder.layers.4.cross_attn.sampling_offsets.bias
decoder.decoder.layers.4.cross_attn.attention_weights.weight
decoder.decoder.layers.4.cross_attn.attention_weights.bias
decoder.decoder.layers.4.cross_attn.value_proj.weight
decoder.decoder.layers.4.cross_attn.value_proj.bias
decoder.decoder.layers.4.cross_attn.output_proj.weight
decoder.decoder.layers.4.cross_attn.output_proj.bias
decoder.decoder.layers.4.norm2.weight
decoder.decoder.layers.4.norm2.bias
decoder.decoder.layers.4.linear1.weight
decoder.decoder.layers.4.linear1.bias
decoder.decoder.layers.4.linear2.weight
decoder.decoder.layers.4.linear2.bias
decoder.decoder.layers.4.norm3.weight
decoder.decoder.layers.4.norm3.bias
decoder.decoder.layers.5.self_attn.in_proj_weight
decoder.decoder.layers.5.self_attn.in_proj_bias
decoder.decoder.layers.5.self_attn.out_proj.weight
decoder.decoder.layers.5.self_attn.out_proj.bias
decoder.decoder.layers.5.norm1.weight
decoder.decoder.layers.5.norm1.bias
decoder.decoder.layers.5.cross_attn.sampling_offsets.weight
decoder.decoder.layers.5.cross_attn.sampling_offsets.bias
decoder.decoder.layers.5.cross_attn.attention_weights.weight
decoder.decoder.layers.5.cross_attn.attention_weights.bias
decoder.decoder.layers.5.cross_attn.value_proj.weight
decoder.decoder.layers.5.cross_attn.value_proj.bias
decoder.decoder.layers.5.cross_attn.output_proj.weight
decoder.decoder.layers.5.cross_attn.output_proj.bias
decoder.decoder.layers.5.norm2.weight
decoder.decoder.layers.5.norm2.bias
decoder.decoder.layers.5.linear1.weight
decoder.decoder.layers.5.linear1.bias
decoder.decoder.layers.5.linear2.weight
decoder.decoder.layers.5.linear2.bias
decoder.decoder.layers.5.norm3.weight
decoder.decoder.layers.5.norm3.bias
decoder.denoising_class_embed.weight
decoder.query_pos_head.layers.0.weight
decoder.query_pos_head.layers.0.bias
decoder.query_pos_head.layers.1.weight
decoder.query_pos_head.layers.1.bias
decoder.enc_output.0.weight
decoder.enc_output.0.bias
decoder.enc_output.1.weight
decoder.enc_output.1.bias
decoder.enc_score_head.weight
decoder.enc_score_head.bias
decoder.enc_bbox_head.layers.0.weight
decoder.enc_bbox_head.layers.0.bias
decoder.enc_bbox_head.layers.1.weight
decoder.enc_bbox_head.layers.1.bias
decoder.enc_bbox_head.layers.2.weight
decoder.enc_bbox_head.layers.2.bias
decoder.dec_score_head.0.weight
decoder.dec_score_head.0.bias
decoder.dec_score_head.1.weight
decoder.dec_score_head.1.bias
decoder.dec_score_head.2.weight
decoder.dec_score_head.2.bias
decoder.dec_score_head.3.weight
decoder.dec_score_head.3.bias
decoder.dec_score_head.4.weight
decoder.dec_score_head.4.bias
decoder.dec_score_head.5.weight
decoder.dec_score_head.5.bias
decoder.dec_bbox_head.0.layers.0.weight
decoder.dec_bbox_head.0.layers.0.bias
decoder.dec_bbox_head.0.layers.1.weight
decoder.dec_bbox_head.0.layers.1.bias
decoder.dec_bbox_head.0.layers.2.weight
decoder.dec_bbox_head.0.layers.2.bias
decoder.dec_bbox_head.1.layers.0.weight
decoder.dec_bbox_head.1.layers.0.bias
decoder.dec_bbox_head.1.layers.1.weight
decoder.dec_bbox_head.1.layers.1.bias
decoder.dec_bbox_head.1.layers.2.weight
decoder.dec_bbox_head.1.layers.2.bias
decoder.dec_bbox_head.2.layers.0.weight
decoder.dec_bbox_head.2.layers.0.bias
decoder.dec_bbox_head.2.layers.1.weight
decoder.dec_bbox_head.2.layers.1.bias
decoder.dec_bbox_head.2.layers.2.weight
decoder.dec_bbox_head.2.layers.2.bias
decoder.dec_bbox_head.3.layers.0.weight
decoder.dec_bbox_head.3.layers.0.bias
decoder.dec_bbox_head.3.layers.1.weight
decoder.dec_bbox_head.3.layers.1.bias
decoder.dec_bbox_head.3.layers.2.weight
decoder.dec_bbox_head.3.layers.2.bias
decoder.dec_bbox_head.4.layers.0.weight
decoder.dec_bbox_head.4.layers.0.bias
decoder.dec_bbox_head.4.layers.1.weight
decoder.dec_bbox_head.4.layers.1.bias
decoder.dec_bbox_head.4.layers.2.weight
decoder.dec_bbox_head.4.layers.2.bias
decoder.dec_bbox_head.5.layers.0.weight
decoder.dec_bbox_head.5.layers.0.bias
decoder.dec_bbox_head.5.layers.1.weight
decoder.dec_bbox_head.5.layers.1.bias
decoder.dec_bbox_head.5.layers.2.weight
decoder.dec_bbox_head.5.layers.2.bias
encoder.input_proj.0.0.weight
encoder.input_proj.0.1.weight
encoder.input_proj.0.1.bias
encoder.input_proj.1.0.weight
encoder.input_proj.1.1.weight
encoder.input_proj.1.1.bias
encoder.input_proj.2.0.weight
encoder.input_proj.2.1.weight
encoder.input_proj.2.1.bias
encoder.input_proj_swinT.0.0.weight
encoder.input_proj_swinT.0.1.weight
encoder.input_proj_swinT.0.1.bias
encoder.input_proj_swinT.1.0.weight
encoder.input_proj_swinT.1.1.weight
encoder.input_proj_swinT.1.1.bias
encoder.input_proj_swinT.2.0.weight
encoder.input_proj_swinT.2.1.weight
encoder.input_proj_swinT.2.1.bias
encoder.encoder.0.layers.0.self_attn.in_proj_weight
encoder.encoder.0.layers.0.self_attn.in_proj_bias
encoder.encoder.0.layers.0.self_attn.out_proj.weight
encoder.encoder.0.layers.0.self_attn.out_proj.bias
encoder.encoder.0.layers.0.linear1.weight
encoder.encoder.0.layers.0.linear1.bias
encoder.encoder.0.layers.0.linear2.weight
encoder.encoder.0.layers.0.linear2.bias
encoder.encoder.0.layers.0.norm1.weight
encoder.encoder.0.layers.0.norm1.bias
encoder.encoder.0.layers.0.norm2.weight
encoder.encoder.0.layers.0.norm2.bias
encoder.lateral_convs.0.conv.weight
encoder.lateral_convs.0.norm.weight
encoder.lateral_convs.0.norm.bias
encoder.lateral_convs.1.conv.weight
encoder.lateral_convs.1.norm.weight
encoder.lateral_convs.1.norm.bias
encoder.fpn_blocks.0.conv1.conv.weight
encoder.fpn_blocks.0.conv1.norm.weight
encoder.fpn_blocks.0.conv1.norm.bias
encoder.fpn_blocks.0.conv2.conv.weight
encoder.fpn_blocks.0.conv2.norm.weight
encoder.fpn_blocks.0.conv2.norm.bias
encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias
encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias
encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias
encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias
encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias
encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias
encoder.fpn_blocks.1.conv1.conv.weight
encoder.fpn_blocks.1.conv1.norm.weight
encoder.fpn_blocks.1.conv1.norm.bias
encoder.fpn_blocks.1.conv2.conv.weight
encoder.fpn_blocks.1.conv2.norm.weight
encoder.fpn_blocks.1.conv2.norm.bias
encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias
encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias
encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias
encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias
encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias
encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias
encoder.downsample_convs.0.conv.weight
encoder.downsample_convs.0.norm.weight
encoder.downsample_convs.0.norm.bias
encoder.downsample_convs.1.conv.weight
encoder.downsample_convs.1.norm.weight
encoder.downsample_convs.1.norm.bias
encoder.pan_blocks.0.conv1.conv.weight
encoder.pan_blocks.0.conv1.norm.weight
encoder.pan_blocks.0.conv1.norm.bias
encoder.pan_blocks.0.conv2.conv.weight
encoder.pan_blocks.0.conv2.norm.weight
encoder.pan_blocks.0.conv2.norm.bias
encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias
encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias
encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias
encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias
encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias
encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias
encoder.pan_blocks.1.conv1.conv.weight
encoder.pan_blocks.1.conv1.norm.weight
encoder.pan_blocks.1.conv1.norm.bias
encoder.pan_blocks.1.conv2.conv.weight
encoder.pan_blocks.1.conv2.norm.weight
encoder.pan_blocks.1.conv2.norm.bias
encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias
encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias
encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias
encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias
encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias
encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/train2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/val2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'SwinTransformer', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'SwinTransformer': {'num_stages': 4, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
start creating model... (in yaml_config.py)
self.yaml_cfg: {'task': 'detection', 'num_classes': 80, 'remap_mscoco_category': True, 'train_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/train2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_train2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'RandomPhotometricDistort', 'p': 0.5}, {'type': 'RandomZoomOut', 'fill': 0}, {'type': 'RandomIoUCrop', 'p': 0.8}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'RandomHorizontalFlip'}, {'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}, {'type': 'SanitizeBoundingBox', 'min_size': 1}, {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}]}, 'return_masks': False}, 'shuffle': True, 'batch_size': 4, 'num_workers': 4, 'drop_last': True, 'collate_fn': 'default_collate_fn'}, 'val_dataloader': {'type': 'DataLoader', 'dataset': {'type': 'CocoDetection', 'img_folder': '/home/hslee/Desktop/Datasets/COCO/val2017', 'ann_file': '/home/hslee/Desktop/Datasets/COCO/annotations/instances_val2017.json', 'transforms': {'type': 'Compose', 'ops': [{'type': 'Resize', 'size': [640, 640]}, {'type': 'ToImageTensor'}, {'type': 'ConvertDtype'}]}}, 'shuffle': False, 'batch_size': 8, 'num_workers': 4, 'drop_last': False, 'collate_fn': 'default_collate_fn'}, 'sync_bn': True, 'find_unused_parameters': True, 'use_amp': False, 'scaler': {'type': 'GradScaler', 'enabled': True}, 'use_ema': True, 'ema': {'type': 'ModelEMA', 'decay': 0.9999, 'warmups': 2000}, 'epoches': 72, 'clip_max_norm': 0.1, 'optimizer': {'type': 'AdamW', 'params': [{'params': 'backbone', 'lr': 1e-05}, {'params': '^(?=.*encoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}, {'params': '^(?=.*decoder(?=.*bias|.*norm.*weight)).*$', 'weight_decay': 0.0}], 'lr': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.0001}, 'lr_scheduler': {'type': 'MultiStepLR', 'milestones': [1000], 'gamma': 0.1}, 'model': 'RTDETR', 'criterion': 'SetCriterion', 'postprocessor': 'RTDETRPostProcessor', 'RTDETR': {'backbone': 'SwinTransformer', 'encoder': 'HybridEncoder', 'decoder': 'RTDETRTransformer', 'multi_scale': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]}, 'PResNet': {'depth': 50, 'variant': 'd', 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNetADN': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'ResNet': {'depth': 50, 'freeze_at': 0, 'return_idx': [1, 2, 3], 'num_stages': 4, 'freeze_norm': True, 'pretrained': True}, 'SwinTransformer': {'num_stages': 4, 'pretrained': True}, 'HybridEncoder': {'in_channels': [512, 1024, 2048], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'use_encoder_idx': [2], 'num_encoder_layers': 1, 'nhead': 8, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'pe_temperature': 10000, 'expansion': 1.0, 'depth_mult': 1, 'act': 'silu', 'eval_spatial_size': [640, 640]}, 'RTDETRTransformer': {'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'num_levels': 3, 'num_queries': 300, 'num_decoder_layers': 6, 'num_denoising': 100, 'eval_idx': -1, 'eval_spatial_size': [640, 640]}, 'use_focal_loss': True, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'SetCriterion': {'weight_dict': {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}, 'losses': ['vfl', 'boxes'], 'alpha': 0.75, 'gamma': 2.0, 'matcher': {'type': 'HungarianMatcher', 'weight_dict': {'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2}, 'alpha': 0.25, 'gamma': 2.0}}, '__include__': ['../dataset/coco_detection.yml', '../runtime.yml', './include/dataloader.yml', './include/optimizer.yml', './include/rtdetr_r50vd.yml'], 'output_dir': './output/rtdetr_r50vd_6x_coco_ResNetv2', 'resume': None, 'tuning': None}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=6.51s)
creating index...
index created!
loading annotations into memory...
Done (t=0.22s)
creating index...
index created!
(in det_solver.py) model params, #params : 
backbone.pre.0.weight 4608
backbone.pre.0.bias 96
backbone.pre.2.weight 96
backbone.pre.2.bias 96
backbone.features.0.0.norm1.weight 96
backbone.features.0.0.norm1.bias 96
backbone.features.0.0.attn.relative_position_bias_table 507
backbone.features.0.0.attn.qkv.weight 27648
backbone.features.0.0.attn.qkv.bias 288
backbone.features.0.0.attn.proj.weight 9216
backbone.features.0.0.attn.proj.bias 96
backbone.features.0.0.norm2.weight 96
backbone.features.0.0.norm2.bias 96
backbone.features.0.0.mlp.0.weight 36864
backbone.features.0.0.mlp.0.bias 384
backbone.features.0.0.mlp.3.weight 36864
backbone.features.0.0.mlp.3.bias 96
backbone.features.0.0.norm1_skip.weight 96
backbone.features.0.0.norm1_skip.bias 96
backbone.features.0.0.norm2_skip.weight 96
backbone.features.0.0.norm2_skip.bias 96
backbone.features.0.1.norm1.weight 96
backbone.features.0.1.norm1.bias 96
backbone.features.0.1.attn.relative_position_bias_table 507
backbone.features.0.1.attn.qkv.weight 27648
backbone.features.0.1.attn.qkv.bias 288
backbone.features.0.1.attn.proj.weight 9216
backbone.features.0.1.attn.proj.bias 96
backbone.features.0.1.norm2.weight 96
backbone.features.0.1.norm2.bias 96
backbone.features.0.1.mlp.0.weight 36864
backbone.features.0.1.mlp.0.bias 384
backbone.features.0.1.mlp.3.weight 36864
backbone.features.0.1.mlp.3.bias 96
backbone.features.1.reduction.weight 73728
backbone.features.1.norm.weight 384
backbone.features.1.norm.bias 384
backbone.features.1.norm_skip.weight 384
backbone.features.1.norm_skip.bias 384
backbone.features.2.0.norm1.weight 192
backbone.features.2.0.norm1.bias 192
backbone.features.2.0.attn.relative_position_bias_table 1014
backbone.features.2.0.attn.qkv.weight 110592
backbone.features.2.0.attn.qkv.bias 576
backbone.features.2.0.attn.proj.weight 36864
backbone.features.2.0.attn.proj.bias 192
backbone.features.2.0.norm2.weight 192
backbone.features.2.0.norm2.bias 192
backbone.features.2.0.mlp.0.weight 147456
backbone.features.2.0.mlp.0.bias 768
backbone.features.2.0.mlp.3.weight 147456
backbone.features.2.0.mlp.3.bias 192
backbone.features.2.0.norm1_skip.weight 192
backbone.features.2.0.norm1_skip.bias 192
backbone.features.2.0.norm2_skip.weight 192
backbone.features.2.0.norm2_skip.bias 192
backbone.features.2.1.norm1.weight 192
backbone.features.2.1.norm1.bias 192
backbone.features.2.1.attn.relative_position_bias_table 1014
backbone.features.2.1.attn.qkv.weight 110592
backbone.features.2.1.attn.qkv.bias 576
backbone.features.2.1.attn.proj.weight 36864
backbone.features.2.1.attn.proj.bias 192
backbone.features.2.1.norm2.weight 192
backbone.features.2.1.norm2.bias 192
backbone.features.2.1.mlp.0.weight 147456
backbone.features.2.1.mlp.0.bias 768
backbone.features.2.1.mlp.3.weight 147456
backbone.features.2.1.mlp.3.bias 192
backbone.features.3.reduction.weight 294912
backbone.features.3.norm.weight 768
backbone.features.3.norm.bias 768
backbone.features.3.norm_skip.weight 768
backbone.features.3.norm_skip.bias 768
backbone.features.4.0.norm1.weight 384
backbone.features.4.0.norm1.bias 384
backbone.features.4.0.attn.relative_position_bias_table 2028
backbone.features.4.0.attn.qkv.weight 442368
backbone.features.4.0.attn.qkv.bias 1152
backbone.features.4.0.attn.proj.weight 147456
backbone.features.4.0.attn.proj.bias 384
backbone.features.4.0.norm2.weight 384
backbone.features.4.0.norm2.bias 384
backbone.features.4.0.mlp.0.weight 589824
backbone.features.4.0.mlp.0.bias 1536
backbone.features.4.0.mlp.3.weight 589824
backbone.features.4.0.mlp.3.bias 384
backbone.features.4.0.norm1_skip.weight 384
backbone.features.4.0.norm1_skip.bias 384
backbone.features.4.0.norm2_skip.weight 384
backbone.features.4.0.norm2_skip.bias 384
backbone.features.4.1.norm1.weight 384
backbone.features.4.1.norm1.bias 384
backbone.features.4.1.attn.relative_position_bias_table 2028
backbone.features.4.1.attn.qkv.weight 442368
backbone.features.4.1.attn.qkv.bias 1152
backbone.features.4.1.attn.proj.weight 147456
backbone.features.4.1.attn.proj.bias 384
backbone.features.4.1.norm2.weight 384
backbone.features.4.1.norm2.bias 384
backbone.features.4.1.mlp.0.weight 589824
backbone.features.4.1.mlp.0.bias 1536
backbone.features.4.1.mlp.3.weight 589824
backbone.features.4.1.mlp.3.bias 384
backbone.features.4.1.norm1_skip.weight 384
backbone.features.4.1.norm1_skip.bias 384
backbone.features.4.1.norm2_skip.weight 384
backbone.features.4.1.norm2_skip.bias 384
backbone.features.4.2.norm1.weight 384
backbone.features.4.2.norm1.bias 384
backbone.features.4.2.attn.relative_position_bias_table 2028
backbone.features.4.2.attn.qkv.weight 442368
backbone.features.4.2.attn.qkv.bias 1152
backbone.features.4.2.attn.proj.weight 147456
backbone.features.4.2.attn.proj.bias 384
backbone.features.4.2.norm2.weight 384
backbone.features.4.2.norm2.bias 384
backbone.features.4.2.mlp.0.weight 589824
backbone.features.4.2.mlp.0.bias 1536
backbone.features.4.2.mlp.3.weight 589824
backbone.features.4.2.mlp.3.bias 384
backbone.features.4.2.norm1_skip.weight 384
backbone.features.4.2.norm1_skip.bias 384
backbone.features.4.2.norm2_skip.weight 384
backbone.features.4.2.norm2_skip.bias 384
backbone.features.4.3.norm1.weight 384
backbone.features.4.3.norm1.bias 384
backbone.features.4.3.attn.relative_position_bias_table 2028
backbone.features.4.3.attn.qkv.weight 442368
backbone.features.4.3.attn.qkv.bias 1152
backbone.features.4.3.attn.proj.weight 147456
backbone.features.4.3.attn.proj.bias 384
backbone.features.4.3.norm2.weight 384
backbone.features.4.3.norm2.bias 384
backbone.features.4.3.mlp.0.weight 589824
backbone.features.4.3.mlp.0.bias 1536
backbone.features.4.3.mlp.3.weight 589824
backbone.features.4.3.mlp.3.bias 384
backbone.features.4.4.norm1.weight 384
backbone.features.4.4.norm1.bias 384
backbone.features.4.4.attn.relative_position_bias_table 2028
backbone.features.4.4.attn.qkv.weight 442368
backbone.features.4.4.attn.qkv.bias 1152
backbone.features.4.4.attn.proj.weight 147456
backbone.features.4.4.attn.proj.bias 384
backbone.features.4.4.norm2.weight 384
backbone.features.4.4.norm2.bias 384
backbone.features.4.4.mlp.0.weight 589824
backbone.features.4.4.mlp.0.bias 1536
backbone.features.4.4.mlp.3.weight 589824
backbone.features.4.4.mlp.3.bias 384
backbone.features.4.5.norm1.weight 384
backbone.features.4.5.norm1.bias 384
backbone.features.4.5.attn.relative_position_bias_table 2028
backbone.features.4.5.attn.qkv.weight 442368
backbone.features.4.5.attn.qkv.bias 1152
backbone.features.4.5.attn.proj.weight 147456
backbone.features.4.5.attn.proj.bias 384
backbone.features.4.5.norm2.weight 384
backbone.features.4.5.norm2.bias 384
backbone.features.4.5.mlp.0.weight 589824
backbone.features.4.5.mlp.0.bias 1536
backbone.features.4.5.mlp.3.weight 589824
backbone.features.4.5.mlp.3.bias 384
backbone.features.5.reduction.weight 1179648
backbone.features.5.norm.weight 1536
backbone.features.5.norm.bias 1536
backbone.features.5.norm_skip.weight 1536
backbone.features.5.norm_skip.bias 1536
backbone.features.6.0.norm1.weight 768
backbone.features.6.0.norm1.bias 768
backbone.features.6.0.attn.relative_position_bias_table 4056
backbone.features.6.0.attn.qkv.weight 1769472
backbone.features.6.0.attn.qkv.bias 2304
backbone.features.6.0.attn.proj.weight 589824
backbone.features.6.0.attn.proj.bias 768
backbone.features.6.0.norm2.weight 768
backbone.features.6.0.norm2.bias 768
backbone.features.6.0.mlp.0.weight 2359296
backbone.features.6.0.mlp.0.bias 3072
backbone.features.6.0.mlp.3.weight 2359296
backbone.features.6.0.mlp.3.bias 768
backbone.features.6.0.norm1_skip.weight 768
backbone.features.6.0.norm1_skip.bias 768
backbone.features.6.0.norm2_skip.weight 768
backbone.features.6.0.norm2_skip.bias 768
backbone.features.6.1.norm1.weight 768
backbone.features.6.1.norm1.bias 768
backbone.features.6.1.attn.relative_position_bias_table 4056
backbone.features.6.1.attn.qkv.weight 1769472
backbone.features.6.1.attn.qkv.bias 2304
backbone.features.6.1.attn.proj.weight 589824
backbone.features.6.1.attn.proj.bias 768
backbone.features.6.1.norm2.weight 768
backbone.features.6.1.norm2.bias 768
backbone.features.6.1.mlp.0.weight 2359296
backbone.features.6.1.mlp.0.bias 3072
backbone.features.6.1.mlp.3.weight 2359296
backbone.features.6.1.mlp.3.bias 768
backbone.norm.weight 768
backbone.norm.bias 768
decoder.input_proj.0.conv.weight 65536
decoder.input_proj.0.norm.weight 256
decoder.input_proj.0.norm.bias 256
decoder.input_proj.1.conv.weight 65536
decoder.input_proj.1.norm.weight 256
decoder.input_proj.1.norm.bias 256
decoder.input_proj.2.conv.weight 65536
decoder.input_proj.2.norm.weight 256
decoder.input_proj.2.norm.bias 256
decoder.decoder.layers.0.self_attn.in_proj_weight 196608
decoder.decoder.layers.0.self_attn.in_proj_bias 768
decoder.decoder.layers.0.self_attn.out_proj.weight 65536
decoder.decoder.layers.0.self_attn.out_proj.bias 256
decoder.decoder.layers.0.norm1.weight 256
decoder.decoder.layers.0.norm1.bias 256
decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
decoder.decoder.layers.0.cross_attn.value_proj.bias 256
decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
decoder.decoder.layers.0.cross_attn.output_proj.bias 256
decoder.decoder.layers.0.norm2.weight 256
decoder.decoder.layers.0.norm2.bias 256
decoder.decoder.layers.0.linear1.weight 262144
decoder.decoder.layers.0.linear1.bias 1024
decoder.decoder.layers.0.linear2.weight 262144
decoder.decoder.layers.0.linear2.bias 256
decoder.decoder.layers.0.norm3.weight 256
decoder.decoder.layers.0.norm3.bias 256
decoder.decoder.layers.1.self_attn.in_proj_weight 196608
decoder.decoder.layers.1.self_attn.in_proj_bias 768
decoder.decoder.layers.1.self_attn.out_proj.weight 65536
decoder.decoder.layers.1.self_attn.out_proj.bias 256
decoder.decoder.layers.1.norm1.weight 256
decoder.decoder.layers.1.norm1.bias 256
decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
decoder.decoder.layers.1.cross_attn.value_proj.bias 256
decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
decoder.decoder.layers.1.cross_attn.output_proj.bias 256
decoder.decoder.layers.1.norm2.weight 256
decoder.decoder.layers.1.norm2.bias 256
decoder.decoder.layers.1.linear1.weight 262144
decoder.decoder.layers.1.linear1.bias 1024
decoder.decoder.layers.1.linear2.weight 262144
decoder.decoder.layers.1.linear2.bias 256
decoder.decoder.layers.1.norm3.weight 256
decoder.decoder.layers.1.norm3.bias 256
decoder.decoder.layers.2.self_attn.in_proj_weight 196608
decoder.decoder.layers.2.self_attn.in_proj_bias 768
decoder.decoder.layers.2.self_attn.out_proj.weight 65536
decoder.decoder.layers.2.self_attn.out_proj.bias 256
decoder.decoder.layers.2.norm1.weight 256
decoder.decoder.layers.2.norm1.bias 256
decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
decoder.decoder.layers.2.cross_attn.value_proj.bias 256
decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
decoder.decoder.layers.2.cross_attn.output_proj.bias 256
decoder.decoder.layers.2.norm2.weight 256
decoder.decoder.layers.2.norm2.bias 256
decoder.decoder.layers.2.linear1.weight 262144
decoder.decoder.layers.2.linear1.bias 1024
decoder.decoder.layers.2.linear2.weight 262144
decoder.decoder.layers.2.linear2.bias 256
decoder.decoder.layers.2.norm3.weight 256
decoder.decoder.layers.2.norm3.bias 256
decoder.decoder.layers.3.self_attn.in_proj_weight 196608
decoder.decoder.layers.3.self_attn.in_proj_bias 768
decoder.decoder.layers.3.self_attn.out_proj.weight 65536
decoder.decoder.layers.3.self_attn.out_proj.bias 256
decoder.decoder.layers.3.norm1.weight 256
decoder.decoder.layers.3.norm1.bias 256
decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
decoder.decoder.layers.3.cross_attn.value_proj.bias 256
decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
decoder.decoder.layers.3.cross_attn.output_proj.bias 256
decoder.decoder.layers.3.norm2.weight 256
decoder.decoder.layers.3.norm2.bias 256
decoder.decoder.layers.3.linear1.weight 262144
decoder.decoder.layers.3.linear1.bias 1024
decoder.decoder.layers.3.linear2.weight 262144
decoder.decoder.layers.3.linear2.bias 256
decoder.decoder.layers.3.norm3.weight 256
decoder.decoder.layers.3.norm3.bias 256
decoder.decoder.layers.4.self_attn.in_proj_weight 196608
decoder.decoder.layers.4.self_attn.in_proj_bias 768
decoder.decoder.layers.4.self_attn.out_proj.weight 65536
decoder.decoder.layers.4.self_attn.out_proj.bias 256
decoder.decoder.layers.4.norm1.weight 256
decoder.decoder.layers.4.norm1.bias 256
decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
decoder.decoder.layers.4.cross_attn.value_proj.bias 256
decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
decoder.decoder.layers.4.cross_attn.output_proj.bias 256
decoder.decoder.layers.4.norm2.weight 256
decoder.decoder.layers.4.norm2.bias 256
decoder.decoder.layers.4.linear1.weight 262144
decoder.decoder.layers.4.linear1.bias 1024
decoder.decoder.layers.4.linear2.weight 262144
decoder.decoder.layers.4.linear2.bias 256
decoder.decoder.layers.4.norm3.weight 256
decoder.decoder.layers.4.norm3.bias 256
decoder.decoder.layers.5.self_attn.in_proj_weight 196608
decoder.decoder.layers.5.self_attn.in_proj_bias 768
decoder.decoder.layers.5.self_attn.out_proj.weight 65536
decoder.decoder.layers.5.self_attn.out_proj.bias 256
decoder.decoder.layers.5.norm1.weight 256
decoder.decoder.layers.5.norm1.bias 256
decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
decoder.decoder.layers.5.cross_attn.value_proj.bias 256
decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
decoder.decoder.layers.5.cross_attn.output_proj.bias 256
decoder.decoder.layers.5.norm2.weight 256
decoder.decoder.layers.5.norm2.bias 256
decoder.decoder.layers.5.linear1.weight 262144
decoder.decoder.layers.5.linear1.bias 1024
decoder.decoder.layers.5.linear2.weight 262144
decoder.decoder.layers.5.linear2.bias 256
decoder.decoder.layers.5.norm3.weight 256
decoder.decoder.layers.5.norm3.bias 256
decoder.denoising_class_embed.weight 20736
decoder.query_pos_head.layers.0.weight 2048
decoder.query_pos_head.layers.0.bias 512
decoder.query_pos_head.layers.1.weight 131072
decoder.query_pos_head.layers.1.bias 256
decoder.enc_output.0.weight 65536
decoder.enc_output.0.bias 256
decoder.enc_output.1.weight 256
decoder.enc_output.1.bias 256
decoder.enc_score_head.weight 20480
decoder.enc_score_head.bias 80
decoder.enc_bbox_head.layers.0.weight 65536
decoder.enc_bbox_head.layers.0.bias 256
decoder.enc_bbox_head.layers.1.weight 65536
decoder.enc_bbox_head.layers.1.bias 256
decoder.enc_bbox_head.layers.2.weight 1024
decoder.enc_bbox_head.layers.2.bias 4
decoder.dec_score_head.0.weight 20480
decoder.dec_score_head.0.bias 80
decoder.dec_score_head.1.weight 20480
decoder.dec_score_head.1.bias 80
decoder.dec_score_head.2.weight 20480
decoder.dec_score_head.2.bias 80
decoder.dec_score_head.3.weight 20480
decoder.dec_score_head.3.bias 80
decoder.dec_score_head.4.weight 20480
decoder.dec_score_head.4.bias 80
decoder.dec_score_head.5.weight 20480
decoder.dec_score_head.5.bias 80
decoder.dec_bbox_head.0.layers.0.weight 65536
decoder.dec_bbox_head.0.layers.0.bias 256
decoder.dec_bbox_head.0.layers.1.weight 65536
decoder.dec_bbox_head.0.layers.1.bias 256
decoder.dec_bbox_head.0.layers.2.weight 1024
decoder.dec_bbox_head.0.layers.2.bias 4
decoder.dec_bbox_head.1.layers.0.weight 65536
decoder.dec_bbox_head.1.layers.0.bias 256
decoder.dec_bbox_head.1.layers.1.weight 65536
decoder.dec_bbox_head.1.layers.1.bias 256
decoder.dec_bbox_head.1.layers.2.weight 1024
decoder.dec_bbox_head.1.layers.2.bias 4
decoder.dec_bbox_head.2.layers.0.weight 65536
decoder.dec_bbox_head.2.layers.0.bias 256
decoder.dec_bbox_head.2.layers.1.weight 65536
decoder.dec_bbox_head.2.layers.1.bias 256
decoder.dec_bbox_head.2.layers.2.weight 1024
decoder.dec_bbox_head.2.layers.2.bias 4
decoder.dec_bbox_head.3.layers.0.weight 65536
decoder.dec_bbox_head.3.layers.0.bias 256
decoder.dec_bbox_head.3.layers.1.weight 65536
decoder.dec_bbox_head.3.layers.1.bias 256
decoder.dec_bbox_head.3.layers.2.weight 1024
decoder.dec_bbox_head.3.layers.2.bias 4
decoder.dec_bbox_head.4.layers.0.weight 65536
decoder.dec_bbox_head.4.layers.0.bias 256
decoder.dec_bbox_head.4.layers.1.weight 65536
decoder.dec_bbox_head.4.layers.1.bias 256
decoder.dec_bbox_head.4.layers.2.weight 1024
decoder.dec_bbox_head.4.layers.2.bias 4
decoder.dec_bbox_head.5.layers.0.weight 65536
decoder.dec_bbox_head.5.layers.0.bias 256
decoder.dec_bbox_head.5.layers.1.weight 65536
decoder.dec_bbox_head.5.layers.1.bias 256
decoder.dec_bbox_head.5.layers.2.weight 1024
decoder.dec_bbox_head.5.layers.2.bias 4
encoder.input_proj.0.0.weight 131072
encoder.input_proj.0.1.weight 256
encoder.input_proj.0.1.bias 256
encoder.input_proj.1.0.weight 262144
encoder.input_proj.1.1.weight 256
encoder.input_proj.1.1.bias 256
encoder.input_proj.2.0.weight 524288
encoder.input_proj.2.1.weight 256
encoder.input_proj.2.1.bias 256
encoder.input_proj_swinT.0.0.weight 49152
encoder.input_proj_swinT.0.1.weight 256
encoder.input_proj_swinT.0.1.bias 256
encoder.input_proj_swinT.1.0.weight 98304
encoder.input_proj_swinT.1.1.weight 256
encoder.input_proj_swinT.1.1.bias 256
encoder.input_proj_swinT.2.0.weight 196608
encoder.input_proj_swinT.2.1.weight 256
encoder.input_proj_swinT.2.1.bias 256
encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
encoder.encoder.0.layers.0.linear1.weight 262144
encoder.encoder.0.layers.0.linear1.bias 1024
encoder.encoder.0.layers.0.linear2.weight 262144
encoder.encoder.0.layers.0.linear2.bias 256
encoder.encoder.0.layers.0.norm1.weight 256
encoder.encoder.0.layers.0.norm1.bias 256
encoder.encoder.0.layers.0.norm2.weight 256
encoder.encoder.0.layers.0.norm2.bias 256
encoder.lateral_convs.0.conv.weight 65536
encoder.lateral_convs.0.norm.weight 256
encoder.lateral_convs.0.norm.bias 256
encoder.lateral_convs.1.conv.weight 65536
encoder.lateral_convs.1.norm.weight 256
encoder.lateral_convs.1.norm.bias 256
encoder.fpn_blocks.0.conv1.conv.weight 131072
encoder.fpn_blocks.0.conv1.norm.weight 256
encoder.fpn_blocks.0.conv1.norm.bias 256
encoder.fpn_blocks.0.conv2.conv.weight 131072
encoder.fpn_blocks.0.conv2.norm.weight 256
encoder.fpn_blocks.0.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
encoder.fpn_blocks.1.conv1.conv.weight 131072
encoder.fpn_blocks.1.conv1.norm.weight 256
encoder.fpn_blocks.1.conv1.norm.bias 256
encoder.fpn_blocks.1.conv2.conv.weight 131072
encoder.fpn_blocks.1.conv2.norm.weight 256
encoder.fpn_blocks.1.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
encoder.downsample_convs.0.conv.weight 589824
encoder.downsample_convs.0.norm.weight 256
encoder.downsample_convs.0.norm.bias 256
encoder.downsample_convs.1.conv.weight 589824
encoder.downsample_convs.1.norm.weight 256
encoder.downsample_convs.1.norm.bias 256
encoder.pan_blocks.0.conv1.conv.weight 131072
encoder.pan_blocks.0.conv1.norm.weight 256
encoder.pan_blocks.0.conv1.norm.bias 256
encoder.pan_blocks.0.conv2.conv.weight 131072
encoder.pan_blocks.0.conv2.norm.weight 256
encoder.pan_blocks.0.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
encoder.pan_blocks.1.conv1.conv.weight 131072
encoder.pan_blocks.1.conv1.norm.weight 256
encoder.pan_blocks.1.conv1.norm.bias 256
encoder.pan_blocks.1.conv2.conv.weight 131072
encoder.pan_blocks.1.conv2.norm.weight 256
encoder.pan_blocks.1.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 47296518
super_config : [False, False, False, False]
base_config : [True, True, True, True]
Epoch: [0]  [    0/29571]  eta: 9:34:22  lr: 0.000010  loss: 39.3086 (39.3086)  loss_vfl: 0.3585 (0.3585)  loss_bbox: 1.0678 (1.0678)  loss_giou: 1.6738 (1.6738)  loss_vfl_aux_0: 0.3648 (0.3648)  loss_bbox_aux_0: 1.0756 (1.0756)  loss_giou_aux_0: 1.6739 (1.6739)  loss_vfl_aux_1: 0.3934 (0.3934)  loss_bbox_aux_1: 1.0959 (1.0959)  loss_giou_aux_1: 1.6740 (1.6740)  loss_vfl_aux_2: 0.3755 (0.3755)  loss_bbox_aux_2: 1.0778 (1.0778)  loss_giou_aux_2: 1.6710 (1.6710)  loss_vfl_aux_3: 0.3922 (0.3922)  loss_bbox_aux_3: 1.0680 (1.0680)  loss_giou_aux_3: 1.6894 (1.6894)  loss_vfl_aux_4: 0.3757 (0.3757)  loss_bbox_aux_4: 1.0515 (1.0515)  loss_giou_aux_4: 1.6827 (1.6827)  loss_vfl_aux_5: 0.3843 (0.3843)  loss_bbox_aux_5: 1.1044 (1.1044)  loss_giou_aux_5: 1.6708 (1.6708)  loss_vfl_dn_0: 0.7584 (0.7584)  loss_bbox_dn_0: 0.7224 (0.7224)  loss_giou_dn_0: 1.3981 (1.3981)  loss_vfl_dn_1: 0.8233 (0.8233)  loss_bbox_dn_1: 0.7224 (0.7224)  loss_giou_dn_1: 1.3981 (1.3981)  loss_vfl_dn_2: 0.7575 (0.7575)  loss_bbox_dn_2: 0.7224 (0.7224)  loss_giou_dn_2: 1.3981 (1.3981)  loss_vfl_dn_3: 0.7919 (0.7919)  loss_bbox_dn_3: 0.7224 (0.7224)  loss_giou_dn_3: 1.3981 (1.3981)  loss_vfl_dn_4: 0.7856 (0.7856)  loss_bbox_dn_4: 0.7224 (0.7224)  loss_giou_dn_4: 1.3981 (1.3981)  loss_vfl_dn_5: 0.7482 (0.7482)  loss_bbox_dn_5: 0.7224 (0.7224)  loss_giou_dn_5: 1.3981 (1.3981)  time: 1.1654  data: 0.3253  max mem: 7895
Epoch: [0]  [  100/29571]  eta: 2:17:00  lr: 0.000010  loss: 38.2835 (42.0108)  loss_vfl: 0.6072 (0.5672)  loss_bbox: 0.9230 (1.3326)  loss_giou: 1.4748 (1.5503)  loss_vfl_aux_0: 0.5544 (0.4959)  loss_bbox_aux_0: 0.9233 (1.3889)  loss_giou_aux_0: 1.4952 (1.5924)  loss_vfl_aux_1: 0.5610 (0.5242)  loss_bbox_aux_1: 0.9205 (1.3655)  loss_giou_aux_1: 1.4766 (1.5776)  loss_vfl_aux_2: 0.5950 (0.5477)  loss_bbox_aux_2: 0.9049 (1.3516)  loss_giou_aux_2: 1.4583 (1.5651)  loss_vfl_aux_3: 0.5923 (0.5560)  loss_bbox_aux_3: 0.9226 (1.3429)  loss_giou_aux_3: 1.4629 (1.5596)  loss_vfl_aux_4: 0.6174 (0.5701)  loss_bbox_aux_4: 0.9082 (1.3352)  loss_giou_aux_4: 1.4630 (1.5552)  loss_vfl_aux_5: 0.5554 (0.4707)  loss_bbox_aux_5: 0.9560 (1.4458)  loss_giou_aux_5: 1.5094 (1.6123)  loss_vfl_dn_0: 0.5800 (0.6669)  loss_bbox_dn_0: 0.7186 (0.9379)  loss_giou_dn_0: 1.3511 (1.3289)  loss_vfl_dn_1: 0.5552 (0.6478)  loss_bbox_dn_1: 0.7172 (0.9483)  loss_giou_dn_1: 1.3491 (1.3305)  loss_vfl_dn_2: 0.5880 (0.6498)  loss_bbox_dn_2: 0.7166 (0.9582)  loss_giou_dn_2: 1.3453 (1.3349)  loss_vfl_dn_3: 0.5965 (0.6514)  loss_bbox_dn_3: 0.7156 (0.9665)  loss_giou_dn_3: 1.3430 (1.3412)  loss_vfl_dn_4: 0.5873 (0.6441)  loss_bbox_dn_4: 0.7144 (0.9751)  loss_giou_dn_4: 1.3414 (1.3473)  loss_vfl_dn_5: 0.5982 (0.6409)  loss_bbox_dn_5: 0.7124 (0.9816)  loss_giou_dn_5: 1.3437 (1.3527)  time: 0.2708  data: 0.0052  max mem: 9213
Epoch: [0]  [  200/29571]  eta: 2:17:14  lr: 0.000010  loss: 38.3718 (40.9142)  loss_vfl: 0.7715 (0.7117)  loss_bbox: 0.9018 (1.1637)  loss_giou: 1.3889 (1.4625)  loss_vfl_aux_0: 0.6387 (0.6160)  loss_bbox_aux_0: 0.9615 (1.2161)  loss_giou_aux_0: 1.4319 (1.5008)  loss_vfl_aux_1: 0.6614 (0.6526)  loss_bbox_aux_1: 0.9380 (1.1947)  loss_giou_aux_1: 1.4169 (1.4888)  loss_vfl_aux_2: 0.7102 (0.6837)  loss_bbox_aux_2: 0.9293 (1.1819)  loss_giou_aux_2: 1.3968 (1.4759)  loss_vfl_aux_3: 0.7089 (0.6913)  loss_bbox_aux_3: 0.9092 (1.1724)  loss_giou_aux_3: 1.3996 (1.4721)  loss_vfl_aux_4: 0.6955 (0.7078)  loss_bbox_aux_4: 0.9082 (1.1662)  loss_giou_aux_4: 1.3982 (1.4674)  loss_vfl_aux_5: 0.6241 (0.5861)  loss_bbox_aux_5: 0.9652 (1.2618)  loss_giou_aux_5: 1.4639 (1.5198)  loss_vfl_dn_0: 0.5496 (0.6273)  loss_bbox_dn_0: 1.0061 (0.9534)  loss_giou_dn_0: 1.3243 (1.3227)  loss_vfl_dn_1: 0.5421 (0.6148)  loss_bbox_dn_1: 0.9973 (0.9575)  loss_giou_dn_1: 1.3341 (1.3238)  loss_vfl_dn_2: 0.5666 (0.6240)  loss_bbox_dn_2: 0.9900 (0.9607)  loss_giou_dn_2: 1.3588 (1.3290)  loss_vfl_dn_3: 0.5727 (0.6280)  loss_bbox_dn_3: 0.9848 (0.9639)  loss_giou_dn_3: 1.3605 (1.3351)  loss_vfl_dn_4: 0.5907 (0.6299)  loss_bbox_dn_4: 0.9819 (0.9676)  loss_giou_dn_4: 1.3602 (1.3415)  loss_vfl_dn_5: 0.5445 (0.6233)  loss_bbox_dn_5: 0.9801 (0.9703)  loss_giou_dn_5: 1.3598 (1.3484)  time: 0.2833  data: 0.0052  max mem: 9213
Epoch: [0]  [  300/29571]  eta: 2:16:05  lr: 0.000010  loss: 38.0832 (40.2714)  loss_vfl: 0.9767 (0.8189)  loss_bbox: 0.7385 (1.0556)  loss_giou: 1.2698 (1.3938)  loss_vfl_aux_0: 0.7824 (0.6875)  loss_bbox_aux_0: 0.8892 (1.1238)  loss_giou_aux_0: 1.3440 (1.4437)  loss_vfl_aux_1: 0.8601 (0.7347)  loss_bbox_aux_1: 0.7850 (1.0973)  loss_giou_aux_1: 1.2967 (1.4271)  loss_vfl_aux_2: 0.9311 (0.7742)  loss_bbox_aux_2: 0.7427 (1.0791)  loss_giou_aux_2: 1.2712 (1.4102)  loss_vfl_aux_3: 0.8865 (0.7909)  loss_bbox_aux_3: 0.7451 (1.0652)  loss_giou_aux_3: 1.2698 (1.4036)  loss_vfl_aux_4: 0.9712 (0.8100)  loss_bbox_aux_4: 0.7440 (1.0588)  loss_giou_aux_4: 1.2608 (1.3980)  loss_vfl_aux_5: 0.6959 (0.6424)  loss_bbox_aux_5: 0.8763 (1.1703)  loss_giou_aux_5: 1.3760 (1.4707)  loss_vfl_dn_0: 0.5258 (0.6024)  loss_bbox_dn_0: 0.8532 (0.9639)  loss_giou_dn_0: 1.2745 (1.3171)  loss_vfl_dn_1: 0.5611 (0.5974)  loss_bbox_dn_1: 0.8458 (0.9629)  loss_giou_dn_1: 1.2535 (1.3180)  loss_vfl_dn_2: 0.6225 (0.6119)  loss_bbox_dn_2: 0.8285 (0.9616)  loss_giou_dn_2: 1.2554 (1.3241)  loss_vfl_dn_3: 0.6059 (0.6147)  loss_bbox_dn_3: 0.8226 (0.9623)  loss_giou_dn_3: 1.2728 (1.3314)  loss_vfl_dn_4: 0.5940 (0.6189)  loss_bbox_dn_4: 0.8218 (0.9648)  loss_giou_dn_4: 1.2799 (1.3379)  loss_vfl_dn_5: 0.6013 (0.6147)  loss_bbox_dn_5: 0.8216 (0.9665)  loss_giou_dn_5: 1.2840 (1.3448)  time: 0.2625  data: 0.0051  max mem: 9216
Epoch: [0]  [  400/29571]  eta: 2:14:29  lr: 0.000010  loss: 35.7615 (39.6266)  loss_vfl: 1.1527 (0.8868)  loss_bbox: 0.7052 (0.9765)  loss_giou: 1.2138 (1.3504)  loss_vfl_aux_0: 0.9178 (0.7382)  loss_bbox_aux_0: 0.7371 (1.0502)  loss_giou_aux_0: 1.2367 (1.4070)  loss_vfl_aux_1: 0.9296 (0.7958)  loss_bbox_aux_1: 0.7106 (1.0172)  loss_giou_aux_1: 1.2217 (1.3833)  loss_vfl_aux_2: 0.9735 (0.8375)  loss_bbox_aux_2: 0.7049 (0.9979)  loss_giou_aux_2: 1.2408 (1.3661)  loss_vfl_aux_3: 1.0745 (0.8588)  loss_bbox_aux_3: 0.7070 (0.9856)  loss_giou_aux_3: 1.2102 (1.3588)  loss_vfl_aux_4: 1.1035 (0.8767)  loss_bbox_aux_4: 0.7040 (0.9806)  loss_giou_aux_4: 1.1840 (1.3533)  loss_vfl_aux_5: 0.7836 (0.6729)  loss_bbox_aux_5: 0.8048 (1.1093)  loss_giou_aux_5: 1.3259 (1.4495)  loss_vfl_dn_0: 0.4727 (0.5820)  loss_bbox_dn_0: 0.8010 (0.9479)  loss_giou_dn_0: 1.2872 (1.3117)  loss_vfl_dn_1: 0.5086 (0.5845)  loss_bbox_dn_1: 0.7885 (0.9427)  loss_giou_dn_1: 1.2780 (1.3110)  loss_vfl_dn_2: 0.5276 (0.6026)  loss_bbox_dn_2: 0.7849 (0.9396)  loss_giou_dn_2: 1.2806 (1.3160)  loss_vfl_dn_3: 0.5478 (0.6079)  loss_bbox_dn_3: 0.7816 (0.9397)  loss_giou_dn_3: 1.2812 (1.3220)  loss_vfl_dn_4: 0.5395 (0.6118)  loss_bbox_dn_4: 0.7822 (0.9415)  loss_giou_dn_4: 1.2886 (1.3277)  loss_vfl_dn_5: 0.5505 (0.6092)  loss_bbox_dn_5: 0.7826 (0.9428)  loss_giou_dn_5: 1.2907 (1.3336)  time: 0.2790  data: 0.0052  max mem: 9216
Epoch: [0]  [  500/29571]  eta: 2:14:14  lr: 0.000010  loss: 36.7903 (39.1314)  loss_vfl: 1.1127 (0.9468)  loss_bbox: 0.7109 (0.9206)  loss_giou: 1.1084 (1.3135)  loss_vfl_aux_0: 0.8500 (0.7807)  loss_bbox_aux_0: 0.8158 (0.9953)  loss_giou_aux_0: 1.2034 (1.3724)  loss_vfl_aux_1: 0.9675 (0.8449)  loss_bbox_aux_1: 0.7338 (0.9602)  loss_giou_aux_1: 1.1909 (1.3467)  loss_vfl_aux_2: 1.0846 (0.8910)  loss_bbox_aux_2: 0.7136 (0.9408)  loss_giou_aux_2: 1.1401 (1.3297)  loss_vfl_aux_3: 1.0974 (0.9154)  loss_bbox_aux_3: 0.6948 (0.9289)  loss_giou_aux_3: 1.1226 (1.3221)  loss_vfl_aux_4: 1.1065 (0.9355)  loss_bbox_aux_4: 0.7118 (0.9243)  loss_giou_aux_4: 1.1121 (1.3164)  loss_vfl_aux_5: 0.7257 (0.6985)  loss_bbox_aux_5: 0.9277 (1.0642)  loss_giou_aux_5: 1.3160 (1.4262)  loss_vfl_dn_0: 0.4902 (0.5681)  loss_bbox_dn_0: 0.8371 (0.9315)  loss_giou_dn_0: 1.2408 (1.3044)  loss_vfl_dn_1: 0.5310 (0.5779)  loss_bbox_dn_1: 0.8297 (0.9232)  loss_giou_dn_1: 1.2226 (1.3009)  loss_vfl_dn_2: 0.5819 (0.6002)  loss_bbox_dn_2: 0.8410 (0.9188)  loss_giou_dn_2: 1.2035 (1.3037)  loss_vfl_dn_3: 0.6014 (0.6076)  loss_bbox_dn_3: 0.8454 (0.9182)  loss_giou_dn_3: 1.1957 (1.3085)  loss_vfl_dn_4: 0.6002 (0.6113)  loss_bbox_dn_4: 0.8483 (0.9197)  loss_giou_dn_4: 1.1968 (1.3136)  loss_vfl_dn_5: 0.6047 (0.6104)  loss_bbox_dn_5: 0.8494 (0.9207)  loss_giou_dn_5: 1.1981 (1.3191)  time: 0.2823  data: 0.0051  max mem: 9217
Epoch: [0]  [  600/29571]  eta: 2:13:23  lr: 0.000010  loss: 35.5922 (38.6599)  loss_vfl: 1.1722 (0.9967)  loss_bbox: 0.5550 (0.8697)  loss_giou: 1.0489 (1.2786)  loss_vfl_aux_0: 1.0501 (0.8189)  loss_bbox_aux_0: 0.6547 (0.9439)  loss_giou_aux_0: 1.1391 (1.3394)  loss_vfl_aux_1: 1.0856 (0.8854)  loss_bbox_aux_1: 0.6154 (0.9081)  loss_giou_aux_1: 1.1059 (1.3122)  loss_vfl_aux_2: 1.1210 (0.9344)  loss_bbox_aux_2: 0.5678 (0.8888)  loss_giou_aux_2: 1.0834 (1.2950)  loss_vfl_aux_3: 1.1584 (0.9621)  loss_bbox_aux_3: 0.5693 (0.8776)  loss_giou_aux_3: 1.0605 (1.2872)  loss_vfl_aux_4: 1.1694 (0.9840)  loss_bbox_aux_4: 0.5635 (0.8731)  loss_giou_aux_4: 1.0759 (1.2815)  loss_vfl_aux_5: 0.9030 (0.7247)  loss_bbox_aux_5: 0.7570 (1.0213)  loss_giou_aux_5: 1.2192 (1.3993)  loss_vfl_dn_0: 0.4925 (0.5573)  loss_bbox_dn_0: 0.8349 (0.9203)  loss_giou_dn_0: 1.2550 (1.2963)  loss_vfl_dn_1: 0.5312 (0.5733)  loss_bbox_dn_1: 0.7937 (0.9084)  loss_giou_dn_1: 1.2382 (1.2895)  loss_vfl_dn_2: 0.5879 (0.6006)  loss_bbox_dn_2: 0.7811 (0.9025)  loss_giou_dn_2: 1.2494 (1.2899)  loss_vfl_dn_3: 0.5969 (0.6098)  loss_bbox_dn_3: 0.7767 (0.9012)  loss_giou_dn_3: 1.2392 (1.2938)  loss_vfl_dn_4: 0.6148 (0.6138)  loss_bbox_dn_4: 0.7793 (0.9023)  loss_giou_dn_4: 1.2408 (1.2984)  loss_vfl_dn_5: 0.6146 (0.6142)  loss_bbox_dn_5: 0.7815 (0.9032)  loss_giou_dn_5: 1.2459 (1.3033)  time: 0.2530  data: 0.0052  max mem: 9217
Epoch: [0]  [  700/29571]  eta: 2:12:04  lr: 0.000010  loss: 34.3118 (38.3262)  loss_vfl: 1.2600 (1.0432)  loss_bbox: 0.5181 (0.8307)  loss_giou: 1.0507 (1.2529)  loss_vfl_aux_0: 1.0191 (0.8537)  loss_bbox_aux_0: 0.5972 (0.9059)  loss_giou_aux_0: 1.1637 (1.3167)  loss_vfl_aux_1: 1.1252 (0.9237)  loss_bbox_aux_1: 0.5743 (0.8681)  loss_giou_aux_1: 1.0523 (1.2872)  loss_vfl_aux_2: 1.1706 (0.9751)  loss_bbox_aux_2: 0.5716 (0.8494)  loss_giou_aux_2: 1.0558 (1.2693)  loss_vfl_aux_3: 1.1895 (1.0057)  loss_bbox_aux_3: 0.5365 (0.8387)  loss_giou_aux_3: 1.0317 (1.2611)  loss_vfl_aux_4: 1.2271 (1.0280)  loss_bbox_aux_4: 0.5299 (0.8340)  loss_giou_aux_4: 1.0379 (1.2557)  loss_vfl_aux_5: 0.8209 (0.7518)  loss_bbox_aux_5: 0.6508 (0.9853)  loss_giou_aux_5: 1.2781 (1.3808)  loss_vfl_dn_0: 0.5026 (0.5507)  loss_bbox_dn_0: 0.6957 (0.9101)  loss_giou_dn_0: 1.2131 (1.2860)  loss_vfl_dn_1: 0.5524 (0.5735)  loss_bbox_dn_1: 0.6864 (0.8947)  loss_giou_dn_1: 1.1885 (1.2752)  loss_vfl_dn_2: 0.6049 (0.6048)  loss_bbox_dn_2: 0.6812 (0.8875)  loss_giou_dn_2: 1.1749 (1.2729)  loss_vfl_dn_3: 0.6034 (0.6158)  loss_bbox_dn_3: 0.6822 (0.8854)  loss_giou_dn_3: 1.1691 (1.2754)  loss_vfl_dn_4: 0.6129 (0.6198)  loss_bbox_dn_4: 0.6848 (0.8863)  loss_giou_dn_4: 1.1716 (1.2795)  loss_vfl_dn_5: 0.6238 (0.6208)  loss_bbox_dn_5: 0.6846 (0.8870)  loss_giou_dn_5: 1.1730 (1.2838)  time: 0.2541  data: 0.0052  max mem: 9217
Epoch: [0]  [  800/29571]  eta: 2:11:21  lr: 0.000010  loss: 33.5550 (37.9985)  loss_vfl: 1.2747 (1.0740)  loss_bbox: 0.5128 (0.7997)  loss_giou: 0.9640 (1.2270)  loss_vfl_aux_0: 1.0107 (0.8818)  loss_bbox_aux_0: 0.5646 (0.8756)  loss_giou_aux_0: 0.9698 (1.2924)  loss_vfl_aux_1: 1.1102 (0.9510)  loss_bbox_aux_1: 0.5592 (0.8370)  loss_giou_aux_1: 1.0000 (1.2613)  loss_vfl_aux_2: 1.1335 (1.0045)  loss_bbox_aux_2: 0.5555 (0.8180)  loss_giou_aux_2: 0.9713 (1.2432)  loss_vfl_aux_3: 1.2177 (1.0348)  loss_bbox_aux_3: 0.4992 (0.8077)  loss_giou_aux_3: 0.9579 (1.2350)  loss_vfl_aux_4: 1.2138 (1.0571)  loss_bbox_aux_4: 0.5065 (0.8029)  loss_giou_aux_4: 0.9579 (1.2299)  loss_vfl_aux_5: 0.7991 (0.7777)  loss_bbox_aux_5: 0.6636 (0.9554)  loss_giou_aux_5: 1.1409 (1.3608)  loss_vfl_dn_0: 0.4838 (0.5456)  loss_bbox_dn_0: 0.6468 (0.9045)  loss_giou_dn_0: 1.1809 (1.2754)  loss_vfl_dn_1: 0.5307 (0.5732)  loss_bbox_dn_1: 0.6127 (0.8862)  loss_giou_dn_1: 1.1224 (1.2607)  loss_vfl_dn_2: 0.5680 (0.6072)  loss_bbox_dn_2: 0.6160 (0.8783)  loss_giou_dn_2: 1.0998 (1.2567)  loss_vfl_dn_3: 0.5893 (0.6196)  loss_bbox_dn_3: 0.6177 (0.8758)  loss_giou_dn_3: 1.0923 (1.2582)  loss_vfl_dn_4: 0.5828 (0.6237)  loss_bbox_dn_4: 0.6179 (0.8765)  loss_giou_dn_4: 1.0928 (1.2619)  loss_vfl_dn_5: 0.5892 (0.6250)  loss_bbox_dn_5: 0.6186 (0.8772)  loss_giou_dn_5: 1.0946 (1.2658)  time: 0.2649  data: 0.0051  max mem: 9217
Epoch: [0]  [  900/29571]  eta: 2:10:48  lr: 0.000010  loss: 35.3710 (37.7086)  loss_vfl: 1.2128 (1.0975)  loss_bbox: 0.5507 (0.7742)  loss_giou: 0.9305 (1.2081)  loss_vfl_aux_0: 1.0425 (0.9036)  loss_bbox_aux_0: 0.6060 (0.8507)  loss_giou_aux_0: 0.9795 (1.2759)  loss_vfl_aux_1: 1.2560 (0.9738)  loss_bbox_aux_1: 0.5855 (0.8113)  loss_giou_aux_1: 0.9465 (1.2426)  loss_vfl_aux_2: 1.2487 (1.0273)  loss_bbox_aux_2: 0.5954 (0.7929)  loss_giou_aux_2: 0.9472 (1.2240)  loss_vfl_aux_3: 1.2196 (1.0568)  loss_bbox_aux_3: 0.5777 (0.7821)  loss_giou_aux_3: 0.9260 (1.2159)  loss_vfl_aux_4: 1.2473 (1.0787)  loss_bbox_aux_4: 0.5649 (0.7777)  loss_giou_aux_4: 0.9181 (1.2108)  loss_vfl_aux_5: 0.8647 (0.7949)  loss_bbox_aux_5: 0.7685 (0.9324)  loss_giou_aux_5: 1.1835 (1.3488)  loss_vfl_dn_0: 0.4738 (0.5404)  loss_bbox_dn_0: 0.8279 (0.8965)  loss_giou_dn_0: 1.1870 (1.2668)  loss_vfl_dn_1: 0.5138 (0.5718)  loss_bbox_dn_1: 0.7877 (0.8757)  loss_giou_dn_1: 1.1420 (1.2485)  loss_vfl_dn_2: 0.5566 (0.6084)  loss_bbox_dn_2: 0.7888 (0.8673)  loss_giou_dn_2: 1.1219 (1.2426)  loss_vfl_dn_3: 0.5774 (0.6219)  loss_bbox_dn_3: 0.8020 (0.8645)  loss_giou_dn_3: 1.1091 (1.2431)  loss_vfl_dn_4: 0.5780 (0.6263)  loss_bbox_dn_4: 0.8066 (0.8651)  loss_giou_dn_4: 1.1076 (1.2465)  loss_vfl_dn_5: 0.5731 (0.6277)  loss_bbox_dn_5: 0.8098 (0.8656)  loss_giou_dn_5: 1.1090 (1.2501)  time: 0.2738  data: 0.0053  max mem: 9217
Epoch: [0]  [ 1000/29571]  eta: 2:10:06  lr: 0.000010  loss: 35.2246 (37.4266)  loss_vfl: 1.3539 (1.1152)  loss_bbox: 0.5389 (0.7534)  loss_giou: 0.7939 (1.1877)  loss_vfl_aux_0: 1.0771 (0.9216)  loss_bbox_aux_0: 0.6116 (0.8303)  loss_giou_aux_0: 0.9288 (1.2571)  loss_vfl_aux_1: 1.1989 (0.9920)  loss_bbox_aux_1: 0.5656 (0.7902)  loss_giou_aux_1: 0.8448 (1.2225)  loss_vfl_aux_2: 1.2631 (1.0443)  loss_bbox_aux_2: 0.5645 (0.7720)  loss_giou_aux_2: 0.8301 (1.2036)  loss_vfl_aux_3: 1.2889 (1.0737)  loss_bbox_aux_3: 0.5714 (0.7611)  loss_giou_aux_3: 0.8256 (1.1953)  loss_vfl_aux_4: 1.2693 (1.0949)  loss_bbox_aux_4: 0.5214 (0.7569)  loss_giou_aux_4: 0.8298 (1.1905)  loss_vfl_aux_5: 0.9796 (0.8126)  loss_bbox_aux_5: 0.6935 (0.9136)  loss_giou_aux_5: 1.0455 (1.3330)  loss_vfl_dn_0: 0.4874 (0.5357)  loss_bbox_dn_0: 0.8896 (0.8913)  loss_giou_dn_0: 1.1454 (1.2591)  loss_vfl_dn_1: 0.5581 (0.5691)  loss_bbox_dn_1: 0.8093 (0.8687)  loss_giou_dn_1: 1.0885 (1.2379)  loss_vfl_dn_2: 0.6111 (0.6067)  loss_bbox_dn_2: 0.8027 (0.8599)  loss_giou_dn_2: 1.0543 (1.2305)  loss_vfl_dn_3: 0.6340 (0.6209)  loss_bbox_dn_3: 0.8028 (0.8569)  loss_giou_dn_3: 1.0409 (1.2302)  loss_vfl_dn_4: 0.6395 (0.6254)  loss_bbox_dn_4: 0.8047 (0.8574)  loss_giou_dn_4: 1.0404 (1.2334)  loss_vfl_dn_5: 0.6605 (0.6271)  loss_bbox_dn_5: 0.8050 (0.8580)  loss_giou_dn_5: 1.0427 (1.2367)  time: 0.2702  data: 0.0053  max mem: 9219
Epoch: [0]  [ 1100/29571]  eta: 2:09:21  lr: 0.000010  loss: 32.4805 (37.1275)  loss_vfl: 1.1158 (1.1268)  loss_bbox: 0.4354 (0.7324)  loss_giou: 1.0040 (1.1702)  loss_vfl_aux_0: 0.9810 (0.9350)  loss_bbox_aux_0: 0.4794 (0.8089)  loss_giou_aux_0: 1.0917 (1.2415)  loss_vfl_aux_1: 1.0015 (1.0040)  loss_bbox_aux_1: 0.4536 (0.7694)  loss_giou_aux_1: 1.0219 (1.2051)  loss_vfl_aux_2: 0.9926 (1.0561)  loss_bbox_aux_2: 0.4363 (0.7509)  loss_giou_aux_2: 1.0073 (1.1861)  loss_vfl_aux_3: 1.0508 (1.0838)  loss_bbox_aux_3: 0.4372 (0.7401)  loss_giou_aux_3: 1.0063 (1.1776)  loss_vfl_aux_4: 1.0740 (1.1053)  loss_bbox_aux_4: 0.4292 (0.7359)  loss_giou_aux_4: 1.0079 (1.1732)  loss_vfl_aux_5: 0.7967 (0.8256)  loss_bbox_aux_5: 0.5988 (0.8953)  loss_giou_aux_5: 1.2265 (1.3206)  loss_vfl_dn_0: 0.4594 (0.5316)  loss_bbox_dn_0: 0.8044 (0.8846)  loss_giou_dn_0: 1.1910 (1.2524)  loss_vfl_dn_1: 0.4932 (0.5664)  loss_bbox_dn_1: 0.7538 (0.8605)  loss_giou_dn_1: 1.1434 (1.2286)  loss_vfl_dn_2: 0.5294 (0.6053)  loss_bbox_dn_2: 0.7477 (0.8516)  loss_giou_dn_2: 1.1252 (1.2201)  loss_vfl_dn_3: 0.5586 (0.6196)  loss_bbox_dn_3: 0.7368 (0.8485)  loss_giou_dn_3: 1.1210 (1.2190)  loss_vfl_dn_4: 0.5617 (0.6244)  loss_bbox_dn_4: 0.7372 (0.8489)  loss_giou_dn_4: 1.1189 (1.2219)  loss_vfl_dn_5: 0.5635 (0.6260)  loss_bbox_dn_5: 0.7378 (0.8494)  loss_giou_dn_5: 1.1185 (1.2250)  time: 0.2760  data: 0.0051  max mem: 9219
Epoch: [0]  [ 1200/29571]  eta: 2:09:02  lr: 0.000010  loss: 31.0418 (36.8236)  loss_vfl: 0.9970 (1.1354)  loss_bbox: 0.4184 (0.7122)  loss_giou: 1.0564 (1.1559)  loss_vfl_aux_0: 0.8750 (0.9456)  loss_bbox_aux_0: 0.4991 (0.7884)  loss_giou_aux_0: 1.1288 (1.2289)  loss_vfl_aux_1: 0.9085 (1.0151)  loss_bbox_aux_1: 0.4519 (0.7486)  loss_giou_aux_1: 1.0745 (1.1909)  loss_vfl_aux_2: 1.0081 (1.0671)  loss_bbox_aux_2: 0.4190 (0.7306)  loss_giou_aux_2: 1.0606 (1.1713)  loss_vfl_aux_3: 1.0074 (1.0934)  loss_bbox_aux_3: 0.4195 (0.7198)  loss_giou_aux_3: 1.0531 (1.1630)  loss_vfl_aux_4: 0.9950 (1.1136)  loss_bbox_aux_4: 0.4167 (0.7156)  loss_giou_aux_4: 1.0566 (1.1588)  loss_vfl_aux_5: 0.7680 (0.8337)  loss_bbox_aux_5: 0.6344 (0.8767)  loss_giou_aux_5: 1.2428 (1.3122)  loss_vfl_dn_0: 0.4718 (0.5276)  loss_bbox_dn_0: 0.5666 (0.8743)  loss_giou_dn_0: 1.2252 (1.2467)  loss_vfl_dn_1: 0.4905 (0.5639)  loss_bbox_dn_1: 0.5627 (0.8489)  loss_giou_dn_1: 1.1849 (1.2202)  loss_vfl_dn_2: 0.5357 (0.6038)  loss_bbox_dn_2: 0.5722 (0.8397)  loss_giou_dn_2: 1.1572 (1.2105)  loss_vfl_dn_3: 0.5491 (0.6185)  loss_bbox_dn_3: 0.5690 (0.8364)  loss_giou_dn_3: 1.1458 (1.2087)  loss_vfl_dn_4: 0.5593 (0.6233)  loss_bbox_dn_4: 0.5715 (0.8368)  loss_giou_dn_4: 1.1441 (1.2113)  loss_vfl_dn_5: 0.5545 (0.6250)  loss_bbox_dn_5: 0.5722 (0.8372)  loss_giou_dn_5: 1.1454 (1.2141)  time: 0.2743  data: 0.0052  max mem: 9219
Epoch: [0]  [ 1300/29571]  eta: 2:08:20  lr: 0.000010  loss: 32.0577 (36.5895)  loss_vfl: 1.0457 (1.1484)  loss_bbox: 0.4689 (0.6953)  loss_giou: 0.7475 (1.1387)  loss_vfl_aux_0: 0.9769 (0.9598)  loss_bbox_aux_0: 0.5253 (0.7720)  loss_giou_aux_0: 0.8514 (1.2135)  loss_vfl_aux_1: 0.9783 (1.0296)  loss_bbox_aux_1: 0.4928 (0.7312)  loss_giou_aux_1: 0.7763 (1.1739)  loss_vfl_aux_2: 1.0198 (1.0804)  loss_bbox_aux_2: 0.4852 (0.7133)  loss_giou_aux_2: 0.7309 (1.1542)  loss_vfl_aux_3: 1.0521 (1.1062)  loss_bbox_aux_3: 0.4717 (0.7029)  loss_giou_aux_3: 0.7325 (1.1457)  loss_vfl_aux_4: 1.0201 (1.1257)  loss_bbox_aux_4: 0.4669 (0.6988)  loss_giou_aux_4: 0.7535 (1.1416)  loss_vfl_aux_5: 0.8629 (0.8482)  loss_bbox_aux_5: 0.5974 (0.8624)  loss_giou_aux_5: 1.0046 (1.2994)  loss_vfl_dn_0: 0.4564 (0.5246)  loss_bbox_dn_0: 0.8845 (0.8694)  loss_giou_dn_0: 1.1450 (1.2409)  loss_vfl_dn_1: 0.5042 (0.5617)  loss_bbox_dn_1: 0.8805 (0.8426)  loss_giou_dn_1: 1.0966 (1.2122)  loss_vfl_dn_2: 0.5383 (0.6024)  loss_bbox_dn_2: 0.8800 (0.8331)  loss_giou_dn_2: 1.0686 (1.2016)  loss_vfl_dn_3: 0.5471 (0.6180)  loss_bbox_dn_3: 0.8838 (0.8296)  loss_giou_dn_3: 1.0481 (1.1991)  loss_vfl_dn_4: 0.5432 (0.6226)  loss_bbox_dn_4: 0.8861 (0.8299)  loss_giou_dn_4: 1.0444 (1.2015)  loss_vfl_dn_5: 0.5512 (0.6246)  loss_bbox_dn_5: 0.8877 (0.8304)  loss_giou_dn_5: 1.0441 (1.2042)  time: 0.2661  data: 0.0052  max mem: 9219
Epoch: [0]  [ 1400/29571]  eta: 2:07:48  lr: 0.000010  loss: 31.5413 (36.3741)  loss_vfl: 1.3201 (1.1615)  loss_bbox: 0.4503 (0.6790)  loss_giou: 0.7860 (1.1227)  loss_vfl_aux_0: 1.1747 (0.9742)  loss_bbox_aux_0: 0.5262 (0.7562)  loss_giou_aux_0: 0.9229 (1.1990)  loss_vfl_aux_1: 1.2631 (1.0447)  loss_bbox_aux_1: 0.5012 (0.7146)  loss_giou_aux_1: 0.8489 (1.1583)  loss_vfl_aux_2: 1.2944 (1.0951)  loss_bbox_aux_2: 0.4805 (0.6966)  loss_giou_aux_2: 0.8249 (1.1382)  loss_vfl_aux_3: 1.2548 (1.1195)  loss_bbox_aux_3: 0.4775 (0.6864)  loss_giou_aux_3: 0.8207 (1.1297)  loss_vfl_aux_4: 1.3170 (1.1388)  loss_bbox_aux_4: 0.4607 (0.6825)  loss_giou_aux_4: 0.8236 (1.1256)  loss_vfl_aux_5: 1.0756 (0.8608)  loss_bbox_aux_5: 0.6098 (0.8489)  loss_giou_aux_5: 1.0547 (1.2892)  loss_vfl_dn_0: 0.4757 (0.5222)  loss_bbox_dn_0: 0.7449 (0.8650)  loss_giou_dn_0: 1.1436 (1.2348)  loss_vfl_dn_1: 0.5209 (0.5605)  loss_bbox_dn_1: 0.6587 (0.8365)  loss_giou_dn_1: 1.0842 (1.2034)  loss_vfl_dn_2: 0.5754 (0.6018)  loss_bbox_dn_2: 0.6085 (0.8266)  loss_giou_dn_2: 1.0527 (1.1921)  loss_vfl_dn_3: 0.5806 (0.6182)  loss_bbox_dn_3: 0.5920 (0.8229)  loss_giou_dn_3: 1.0506 (1.1890)  loss_vfl_dn_4: 0.5976 (0.6229)  loss_bbox_dn_4: 0.5886 (0.8232)  loss_giou_dn_4: 1.0518 (1.1911)  loss_vfl_dn_5: 0.5953 (0.6247)  loss_bbox_dn_5: 0.5884 (0.8236)  loss_giou_dn_5: 1.0523 (1.1937)  time: 0.2874  data: 0.0051  max mem: 9219
Epoch: [0]  [ 1500/29571]  eta: 2:07:24  lr: 0.000010  loss: 32.2485 (36.1624)  loss_vfl: 1.0539 (1.1625)  loss_bbox: 0.4635 (0.6680)  loss_giou: 0.9593 (1.1141)  loss_vfl_aux_0: 0.9386 (0.9784)  loss_bbox_aux_0: 0.5704 (0.7454)  loss_giou_aux_0: 1.0485 (1.1917)  loss_vfl_aux_1: 0.9251 (1.0491)  loss_bbox_aux_1: 0.4996 (0.7031)  loss_giou_aux_1: 0.9872 (1.1499)  loss_vfl_aux_2: 0.9850 (1.0987)  loss_bbox_aux_2: 0.4647 (0.6852)  loss_giou_aux_2: 0.9732 (1.1297)  loss_vfl_aux_3: 1.0500 (1.1220)  loss_bbox_aux_3: 0.4585 (0.6752)  loss_giou_aux_3: 0.9591 (1.1211)  loss_vfl_aux_4: 1.1152 (1.1404)  loss_bbox_aux_4: 0.4624 (0.6714)  loss_giou_aux_4: 0.9491 (1.1170)  loss_vfl_aux_5: 0.9013 (0.8666)  loss_bbox_aux_5: 0.6142 (0.8391)  loss_giou_aux_5: 1.1610 (1.2837)  loss_vfl_dn_0: 0.4798 (0.5198)  loss_bbox_dn_0: 0.7630 (0.8583)  loss_giou_dn_0: 1.1325 (1.2298)  loss_vfl_dn_1: 0.5322 (0.5584)  loss_bbox_dn_1: 0.7009 (0.8286)  loss_giou_dn_1: 1.0797 (1.1963)  loss_vfl_dn_2: 0.5600 (0.6000)  loss_bbox_dn_2: 0.6760 (0.8185)  loss_giou_dn_2: 1.0331 (1.1842)  loss_vfl_dn_3: 0.5779 (0.6170)  loss_bbox_dn_3: 0.6668 (0.8147)  loss_giou_dn_3: 1.0238 (1.1807)  loss_vfl_dn_4: 0.5953 (0.6219)  loss_bbox_dn_4: 0.6656 (0.8150)  loss_giou_dn_4: 1.0218 (1.1827)  loss_vfl_dn_5: 0.5755 (0.6234)  loss_bbox_dn_5: 0.6663 (0.8154)  loss_giou_dn_5: 1.0214 (1.1852)  time: 0.2604  data: 0.0051  max mem: 9221
Epoch: [0]  [ 1600/29571]  eta: 2:06:56  lr: 0.000010  loss: 30.7867 (35.9421)  loss_vfl: 0.8806 (1.1660)  loss_bbox: 0.4128 (0.6551)  loss_giou: 0.9645 (1.1043)  loss_vfl_aux_0: 0.8588 (0.9847)  loss_bbox_aux_0: 0.5050 (0.7321)  loss_giou_aux_0: 1.0538 (1.1822)  loss_vfl_aux_1: 0.8909 (1.0548)  loss_bbox_aux_1: 0.4234 (0.6900)  loss_giou_aux_1: 1.0207 (1.1401)  loss_vfl_aux_2: 0.9038 (1.1029)  loss_bbox_aux_2: 0.4106 (0.6720)  loss_giou_aux_2: 0.9709 (1.1199)  loss_vfl_aux_3: 0.9303 (1.1262)  loss_bbox_aux_3: 0.4024 (0.6622)  loss_giou_aux_3: 0.9732 (1.1111)  loss_vfl_aux_4: 0.8884 (1.1435)  loss_bbox_aux_4: 0.4093 (0.6585)  loss_giou_aux_4: 0.9755 (1.1072)  loss_vfl_aux_5: 0.8109 (0.8757)  loss_bbox_aux_5: 0.5934 (0.8265)  loss_giou_aux_5: 1.1870 (1.2754)  loss_vfl_dn_0: 0.4658 (0.5178)  loss_bbox_dn_0: 0.5838 (0.8513)  loss_giou_dn_0: 1.1533 (1.2254)  loss_vfl_dn_1: 0.5091 (0.5566)  loss_bbox_dn_1: 0.5207 (0.8206)  loss_giou_dn_1: 1.0808 (1.1898)  loss_vfl_dn_2: 0.5643 (0.5986)  loss_bbox_dn_2: 0.5194 (0.8101)  loss_giou_dn_2: 1.0827 (1.1771)  loss_vfl_dn_3: 0.5700 (0.6161)  loss_bbox_dn_3: 0.5195 (0.8062)  loss_giou_dn_3: 1.1023 (1.1730)  loss_vfl_dn_4: 0.5920 (0.6212)  loss_bbox_dn_4: 0.5221 (0.8064)  loss_giou_dn_4: 1.1037 (1.1749)  loss_vfl_dn_5: 0.6044 (0.6225)  loss_bbox_dn_5: 0.5228 (0.8068)  loss_giou_dn_5: 1.1048 (1.1772)  time: 0.2681  data: 0.0051  max mem: 9221
Epoch: [0]  [ 1700/29571]  eta: 2:06:24  lr: 0.000010  loss: 30.7653 (35.7764)  loss_vfl: 0.9667 (1.1722)  loss_bbox: 0.3503 (0.6446)  loss_giou: 0.9733 (1.0946)  loss_vfl_aux_0: 0.8888 (0.9925)  loss_bbox_aux_0: 0.4375 (0.7216)  loss_giou_aux_0: 1.0340 (1.1736)  loss_vfl_aux_1: 0.9076 (1.0627)  loss_bbox_aux_1: 0.3861 (0.6796)  loss_giou_aux_1: 1.0064 (1.1309)  loss_vfl_aux_2: 0.9628 (1.1100)  loss_bbox_aux_2: 0.3613 (0.6614)  loss_giou_aux_2: 1.0025 (1.1101)  loss_vfl_aux_3: 0.9716 (1.1328)  loss_bbox_aux_3: 0.3517 (0.6517)  loss_giou_aux_3: 0.9775 (1.1013)  loss_vfl_aux_4: 0.9652 (1.1499)  loss_bbox_aux_4: 0.3507 (0.6481)  loss_giou_aux_4: 0.9747 (1.0973)  loss_vfl_aux_5: 0.8417 (0.8855)  loss_bbox_aux_5: 0.5032 (0.8166)  loss_giou_aux_5: 1.1650 (1.2680)  loss_vfl_dn_0: 0.4584 (0.5163)  loss_bbox_dn_0: 0.4837 (0.8467)  loss_giou_dn_0: 1.1670 (1.2208)  loss_vfl_dn_1: 0.4910 (0.5556)  loss_bbox_dn_1: 0.4412 (0.8148)  loss_giou_dn_1: 1.0899 (1.1831)  loss_vfl_dn_2: 0.5350 (0.5979)  loss_bbox_dn_2: 0.4200 (0.8042)  loss_giou_dn_2: 1.0626 (1.1695)  loss_vfl_dn_3: 0.5645 (0.6160)  loss_bbox_dn_3: 0.4165 (0.8003)  loss_giou_dn_3: 1.0445 (1.1651)  loss_vfl_dn_4: 0.5599 (0.6214)  loss_bbox_dn_4: 0.4157 (0.8005)  loss_giou_dn_4: 1.0418 (1.1668)  loss_vfl_dn_5: 0.5772 (0.6225)  loss_bbox_dn_5: 0.4156 (0.8008)  loss_giou_dn_5: 1.0415 (1.1690)  time: 0.2717  data: 0.0054  max mem: 9221
Epoch: [0]  [ 1800/29571]  eta: 2:06:05  lr: 0.000010  loss: 33.0265 (35.6083)  loss_vfl: 1.4495 (1.1792)  loss_bbox: 0.4481 (0.6342)  loss_giou: 0.7385 (1.0819)  loss_vfl_aux_0: 1.2672 (1.0035)  loss_bbox_aux_0: 0.5247 (0.7108)  loss_giou_aux_0: 0.8424 (1.1610)  loss_vfl_aux_1: 1.3823 (1.0731)  loss_bbox_aux_1: 0.4561 (0.6686)  loss_giou_aux_1: 0.7589 (1.1180)  loss_vfl_aux_2: 1.3925 (1.1187)  loss_bbox_aux_2: 0.4599 (0.6506)  loss_giou_aux_2: 0.7527 (1.0974)  loss_vfl_aux_3: 1.4679 (1.1405)  loss_bbox_aux_3: 0.4445 (0.6413)  loss_giou_aux_3: 0.7531 (1.0885)  loss_vfl_aux_4: 1.4386 (1.1571)  loss_bbox_aux_4: 0.4446 (0.6374)  loss_giou_aux_4: 0.7396 (1.0846)  loss_vfl_aux_5: 1.1800 (0.8985)  loss_bbox_aux_5: 0.7011 (0.8071)  loss_giou_aux_5: 0.9989 (1.2573)  loss_vfl_dn_0: 0.5051 (0.5151)  loss_bbox_dn_0: 0.9458 (0.8446)  loss_giou_dn_0: 1.1215 (1.2159)  loss_vfl_dn_1: 0.5627 (0.5543)  loss_bbox_dn_1: 0.8763 (0.8114)  loss_giou_dn_1: 1.0374 (1.1761)  loss_vfl_dn_2: 0.5878 (0.5965)  loss_bbox_dn_2: 0.8501 (0.8003)  loss_giou_dn_2: 0.9923 (1.1617)  loss_vfl_dn_3: 0.6070 (0.6153)  loss_bbox_dn_3: 0.8382 (0.7962)  loss_giou_dn_3: 0.9701 (1.1567)  loss_vfl_dn_4: 0.6288 (0.6211)  loss_bbox_dn_4: 0.8323 (0.7964)  loss_giou_dn_4: 0.9665 (1.1582)  loss_vfl_dn_5: 0.6340 (0.6221)  loss_bbox_dn_5: 0.8316 (0.7968)  loss_giou_dn_5: 0.9669 (1.1603)  time: 0.2914  data: 0.0051  max mem: 9221
Epoch: [0]  [ 1900/29571]  eta: 2:05:41  lr: 0.000010  loss: 31.4440 (35.4723)  loss_vfl: 1.1227 (1.1853)  loss_bbox: 0.3707 (0.6255)  loss_giou: 0.8608 (1.0716)  loss_vfl_aux_0: 0.9693 (1.0117)  loss_bbox_aux_0: 0.4789 (0.7024)  loss_giou_aux_0: 0.9534 (1.1514)  loss_vfl_aux_1: 1.0048 (1.0805)  loss_bbox_aux_1: 0.4351 (0.6599)  loss_giou_aux_1: 0.9139 (1.1078)  loss_vfl_aux_2: 1.0508 (1.1256)  loss_bbox_aux_2: 0.4179 (0.6417)  loss_giou_aux_2: 0.8827 (1.0871)  loss_vfl_aux_3: 1.0685 (1.1469)  loss_bbox_aux_3: 0.3826 (0.6324)  loss_giou_aux_3: 0.8646 (1.0782)  loss_vfl_aux_4: 1.1203 (1.1634)  loss_bbox_aux_4: 0.3809 (0.6287)  loss_giou_aux_4: 0.8592 (1.0742)  loss_vfl_aux_5: 0.9288 (0.9071)  loss_bbox_aux_5: 0.6144 (0.8001)  loss_giou_aux_5: 1.0656 (1.2496)  loss_vfl_dn_0: 0.4740 (0.5138)  loss_bbox_dn_0: 0.7351 (0.8433)  loss_giou_dn_0: 1.1384 (1.2116)  loss_vfl_dn_1: 0.5039 (0.5529)  loss_bbox_dn_1: 0.6984 (0.8091)  loss_giou_dn_1: 1.0712 (1.1699)  loss_vfl_dn_2: 0.5426 (0.5953)  loss_bbox_dn_2: 0.6827 (0.7979)  loss_giou_dn_2: 1.0491 (1.1548)  loss_vfl_dn_3: 0.5864 (0.6148)  loss_bbox_dn_3: 0.6821 (0.7937)  loss_giou_dn_3: 1.0423 (1.1495)  loss_vfl_dn_4: 0.5971 (0.6209)  loss_bbox_dn_4: 0.6814 (0.7938)  loss_giou_dn_4: 1.0417 (1.1509)  loss_vfl_dn_5: 0.5880 (0.6219)  loss_bbox_dn_5: 0.6811 (0.7942)  loss_giou_dn_5: 1.0409 (1.1529)  time: 0.2722  data: 0.0051  max mem: 9221
Epoch: [0]  [ 2000/29571]  eta: 2:05:32  lr: 0.000010  loss: 32.1613 (35.3269)  loss_vfl: 1.3001 (1.1932)  loss_bbox: 0.3992 (0.6153)  loss_giou: 0.8211 (1.0590)  loss_vfl_aux_0: 1.2288 (1.0203)  loss_bbox_aux_0: 0.5061 (0.6930)  loss_giou_aux_0: 0.8688 (1.1400)  loss_vfl_aux_1: 1.2179 (1.0898)  loss_bbox_aux_1: 0.4141 (0.6498)  loss_giou_aux_1: 0.8254 (1.0956)  loss_vfl_aux_2: 1.2619 (1.1345)  loss_bbox_aux_2: 0.3863 (0.6313)  loss_giou_aux_2: 0.8329 (1.0746)  loss_vfl_aux_3: 1.2946 (1.1552)  loss_bbox_aux_3: 0.4037 (0.6221)  loss_giou_aux_3: 0.8113 (1.0654)  loss_vfl_aux_4: 1.2772 (1.1715)  loss_bbox_aux_4: 0.3986 (0.6184)  loss_giou_aux_4: 0.8128 (1.0615)  loss_vfl_aux_5: 1.0813 (0.9172)  loss_bbox_aux_5: 0.6374 (0.7925)  loss_giou_aux_5: 1.0413 (1.2402)  loss_vfl_dn_0: 0.4962 (0.5131)  loss_bbox_dn_0: 0.8562 (0.8429)  loss_giou_dn_0: 1.1002 (1.2068)  loss_vfl_dn_1: 0.5177 (0.5521)  loss_bbox_dn_1: 0.7807 (0.8074)  loss_giou_dn_1: 1.0200 (1.1633)  loss_vfl_dn_2: 0.5566 (0.5945)  loss_bbox_dn_2: 0.7423 (0.7958)  loss_giou_dn_2: 0.9803 (1.1475)  loss_vfl_dn_3: 0.6059 (0.6147)  loss_bbox_dn_3: 0.7254 (0.7915)  loss_giou_dn_3: 0.9589 (1.1418)  loss_vfl_dn_4: 0.6032 (0.6212)  loss_bbox_dn_4: 0.7214 (0.7916)  loss_giou_dn_4: 0.9560 (1.1431)  loss_vfl_dn_5: 0.6112 (0.6222)  loss_bbox_dn_5: 0.7205 (0.7920)  loss_giou_dn_5: 0.9553 (1.1450)  time: 0.2951  data: 0.0052  max mem: 9221
Epoch: [0]  [ 2100/29571]  eta: 2:05:01  lr: 0.000010  loss: 30.2578 (35.1365)  loss_vfl: 1.1137 (1.1926)  loss_bbox: 0.3936 (0.6061)  loss_giou: 0.8866 (1.0519)  loss_vfl_aux_0: 0.9653 (1.0223)  loss_bbox_aux_0: 0.4574 (0.6834)  loss_giou_aux_0: 1.0028 (1.1333)  loss_vfl_aux_1: 0.9909 (1.0909)  loss_bbox_aux_1: 0.4112 (0.6403)  loss_giou_aux_1: 0.9226 (1.0885)  loss_vfl_aux_2: 1.0301 (1.1350)  loss_bbox_aux_2: 0.4217 (0.6218)  loss_giou_aux_2: 0.9125 (1.0675)  loss_vfl_aux_3: 1.0282 (1.1554)  loss_bbox_aux_3: 0.4334 (0.6128)  loss_giou_aux_3: 0.8876 (1.0582)  loss_vfl_aux_4: 1.0945 (1.1711)  loss_bbox_aux_4: 0.4279 (0.6091)  loss_giou_aux_4: 0.9164 (1.0543)  loss_vfl_aux_5: 0.8819 (0.9213)  loss_bbox_aux_5: 0.4980 (0.7830)  loss_giou_aux_5: 1.1035 (1.2344)  loss_vfl_dn_0: 0.4702 (0.5118)  loss_bbox_dn_0: 0.6078 (0.8361)  loss_giou_dn_0: 1.1496 (1.2033)  loss_vfl_dn_1: 0.5133 (0.5504)  loss_bbox_dn_1: 0.5471 (0.8001)  loss_giou_dn_1: 1.0732 (1.1584)  loss_vfl_dn_2: 0.5591 (0.5927)  loss_bbox_dn_2: 0.5475 (0.7885)  loss_giou_dn_2: 1.0366 (1.1421)  loss_vfl_dn_3: 0.5917 (0.6133)  loss_bbox_dn_3: 0.5470 (0.7842)  loss_giou_dn_3: 1.0283 (1.1361)  loss_vfl_dn_4: 0.5936 (0.6199)  loss_bbox_dn_4: 0.5443 (0.7843)  loss_giou_dn_4: 1.0282 (1.1373)  loss_vfl_dn_5: 0.5946 (0.6209)  loss_bbox_dn_5: 0.5443 (0.7846)  loss_giou_dn_5: 1.0287 (1.1391)  time: 0.2667  data: 0.0052  max mem: 9221
Epoch: [0]  [ 2200/29571]  eta: 2:04:21  lr: 0.000010  loss: 30.4522 (34.9870)  loss_vfl: 1.3486 (1.1953)  loss_bbox: 0.3200 (0.5986)  loss_giou: 0.7710 (1.0444)  loss_vfl_aux_0: 1.1769 (1.0264)  loss_bbox_aux_0: 0.4270 (0.6754)  loss_giou_aux_0: 0.8911 (1.1263)  loss_vfl_aux_1: 1.2267 (1.0945)  loss_bbox_aux_1: 0.3372 (0.6324)  loss_giou_aux_1: 0.8117 (1.0813)  loss_vfl_aux_2: 1.2818 (1.1381)  loss_bbox_aux_2: 0.3187 (0.6143)  loss_giou_aux_2: 0.7944 (1.0601)  loss_vfl_aux_3: 1.3125 (1.1585)  loss_bbox_aux_3: 0.3217 (0.6052)  loss_giou_aux_3: 0.7782 (1.0507)  loss_vfl_aux_4: 1.2653 (1.1742)  loss_bbox_aux_4: 0.3230 (0.6015)  loss_giou_aux_4: 0.7705 (1.0468)  loss_vfl_aux_5: 1.1642 (0.9287)  loss_bbox_aux_5: 0.5004 (0.7750)  loss_giou_aux_5: 0.9586 (1.2279)  loss_vfl_dn_0: 0.4824 (0.5104)  loss_bbox_dn_0: 0.6657 (0.8312)  loss_giou_dn_0: 1.1011 (1.2000)  loss_vfl_dn_1: 0.5176 (0.5489)  loss_bbox_dn_1: 0.6011 (0.7945)  loss_giou_dn_1: 0.9970 (1.1536)  loss_vfl_dn_2: 0.5665 (0.5913)  loss_bbox_dn_2: 0.5853 (0.7827)  loss_giou_dn_2: 0.9663 (1.1368)  loss_vfl_dn_3: 0.6074 (0.6122)  loss_bbox_dn_3: 0.5803 (0.7783)  loss_giou_dn_3: 0.9414 (1.1305)  loss_vfl_dn_4: 0.6174 (0.6190)  loss_bbox_dn_4: 0.5779 (0.7784)  loss_giou_dn_4: 0.9427 (1.1316)  loss_vfl_dn_5: 0.6367 (0.6201)  loss_bbox_dn_5: 0.5779 (0.7787)  loss_giou_dn_5: 0.9442 (1.1333)  time: 0.2732  data: 0.0051  max mem: 9221
Epoch: [0]  [ 2300/29571]  eta: 2:03:50  lr: 0.000010  loss: 31.6174 (34.8401)  loss_vfl: 1.3795 (1.2010)  loss_bbox: 0.3895 (0.5892)  loss_giou: 0.6620 (1.0347)  loss_vfl_aux_0: 1.2445 (1.0336)  loss_bbox_aux_0: 0.4459 (0.6663)  loss_giou_aux_0: 0.7814 (1.1168)  loss_vfl_aux_1: 1.2739 (1.1012)  loss_bbox_aux_1: 0.4379 (0.6232)  loss_giou_aux_1: 0.7296 (1.0718)  loss_vfl_aux_2: 1.3417 (1.1439)  loss_bbox_aux_2: 0.4229 (0.6050)  loss_giou_aux_2: 0.6696 (1.0505)  loss_vfl_aux_3: 1.3697 (1.1643)  loss_bbox_aux_3: 0.3901 (0.5959)  loss_giou_aux_3: 0.6689 (1.0410)  loss_vfl_aux_4: 1.4156 (1.1799)  loss_bbox_aux_4: 0.3938 (0.5922)  loss_giou_aux_4: 0.6618 (1.0371)  loss_vfl_aux_5: 1.1052 (0.9370)  loss_bbox_aux_5: 0.6569 (0.7667)  loss_giou_aux_5: 0.9513 (1.2199)  loss_vfl_dn_0: 0.4955 (0.5096)  loss_bbox_dn_0: 0.7627 (0.8282)  loss_giou_dn_0: 1.0767 (1.1962)  loss_vfl_dn_1: 0.5185 (0.5478)  loss_bbox_dn_1: 0.6333 (0.7903)  loss_giou_dn_1: 1.0182 (1.1485)  loss_vfl_dn_2: 0.5567 (0.5900)  loss_bbox_dn_2: 0.5847 (0.7782)  loss_giou_dn_2: 0.9737 (1.1311)  loss_vfl_dn_3: 0.5699 (0.6115)  loss_bbox_dn_3: 0.5890 (0.7738)  loss_giou_dn_3: 0.9572 (1.1246)  loss_vfl_dn_4: 0.5878 (0.6186)  loss_bbox_dn_4: 0.5933 (0.7738)  loss_giou_dn_4: 0.9562 (1.1255)  loss_vfl_dn_5: 0.5972 (0.6200)  loss_bbox_dn_5: 0.5935 (0.7741)  loss_giou_dn_5: 0.9551 (1.1272)  time: 0.2716  data: 0.0053  max mem: 9222
Epoch: [0]  [ 2400/29571]  eta: 2:03:16  lr: 0.000010  loss: 29.0390 (34.6729)  loss_vfl: 1.1498 (1.2029)  loss_bbox: 0.3175 (0.5804)  loss_giou: 0.8289 (1.0264)  loss_vfl_aux_0: 1.0208 (1.0374)  loss_bbox_aux_0: 0.3793 (0.6574)  loss_giou_aux_0: 0.9071 (1.1086)  loss_vfl_aux_1: 1.0817 (1.1045)  loss_bbox_aux_1: 0.3406 (0.6142)  loss_giou_aux_1: 0.8780 (1.0634)  loss_vfl_aux_2: 1.0556 (1.1466)  loss_bbox_aux_2: 0.3185 (0.5960)  loss_giou_aux_2: 0.8587 (1.0422)  loss_vfl_aux_3: 1.1047 (1.1665)  loss_bbox_aux_3: 0.3182 (0.5869)  loss_giou_aux_3: 0.8327 (1.0327)  loss_vfl_aux_4: 1.1356 (1.1819)  loss_bbox_aux_4: 0.3213 (0.5833)  loss_giou_aux_4: 0.8416 (1.0288)  loss_vfl_aux_5: 0.9802 (0.9433)  loss_bbox_aux_5: 0.4356 (0.7581)  loss_giou_aux_5: 1.0172 (1.2127)  loss_vfl_dn_0: 0.4643 (0.5085)  loss_bbox_dn_0: 0.5358 (0.8231)  loss_giou_dn_0: 1.0904 (1.1931)  loss_vfl_dn_1: 0.4815 (0.5462)  loss_bbox_dn_1: 0.5048 (0.7846)  loss_giou_dn_1: 1.0120 (1.1442)  loss_vfl_dn_2: 0.5238 (0.5884)  loss_bbox_dn_2: 0.4903 (0.7723)  loss_giou_dn_2: 1.0204 (1.1264)  loss_vfl_dn_3: 0.5452 (0.6099)  loss_bbox_dn_3: 0.4990 (0.7679)  loss_giou_dn_3: 1.0083 (1.1196)  loss_vfl_dn_4: 0.5478 (0.6172)  loss_bbox_dn_4: 0.5039 (0.7679)  loss_giou_dn_4: 1.0105 (1.1205)  loss_vfl_dn_5: 0.5612 (0.6187)  loss_bbox_dn_5: 0.5043 (0.7682)  loss_giou_dn_5: 1.0100 (1.1221)  time: 0.2667  data: 0.0053  max mem: 9222
Epoch: [0]  [ 2500/29571]  eta: 2:02:47  lr: 0.000010  loss: 30.9297 (34.5344)  loss_vfl: 1.1650 (1.2065)  loss_bbox: 0.3795 (0.5730)  loss_giou: 0.8335 (1.0182)  loss_vfl_aux_0: 1.0073 (1.0422)  loss_bbox_aux_0: 0.4815 (0.6500)  loss_giou_aux_0: 0.9024 (1.1008)  loss_vfl_aux_1: 1.1092 (1.1087)  loss_bbox_aux_1: 0.4155 (0.6068)  loss_giou_aux_1: 0.8649 (1.0552)  loss_vfl_aux_2: 1.1449 (1.1502)  loss_bbox_aux_2: 0.3940 (0.5886)  loss_giou_aux_2: 0.8469 (1.0340)  loss_vfl_aux_3: 1.1732 (1.1702)  loss_bbox_aux_3: 0.4080 (0.5795)  loss_giou_aux_3: 0.8401 (1.0245)  loss_vfl_aux_4: 1.1628 (1.1851)  loss_bbox_aux_4: 0.3862 (0.5758)  loss_giou_aux_4: 0.8337 (1.0205)  loss_vfl_aux_5: 1.0600 (0.9522)  loss_bbox_aux_5: 0.5638 (0.7511)  loss_giou_aux_5: 1.0214 (1.2055)  loss_vfl_dn_0: 0.4694 (0.5079)  loss_bbox_dn_0: 0.7407 (0.8201)  loss_giou_dn_0: 1.1004 (1.1892)  loss_vfl_dn_1: 0.4966 (0.5451)  loss_bbox_dn_1: 0.6562 (0.7807)  loss_giou_dn_1: 1.0293 (1.1390)  loss_vfl_dn_2: 0.5303 (0.5870)  loss_bbox_dn_2: 0.6245 (0.7681)  loss_giou_dn_2: 0.9993 (1.1206)  loss_vfl_dn_3: 0.5622 (0.6089)  loss_bbox_dn_3: 0.6088 (0.7636)  loss_giou_dn_3: 0.9971 (1.1136)  loss_vfl_dn_4: 0.5765 (0.6164)  loss_bbox_dn_4: 0.6060 (0.7636)  loss_giou_dn_4: 0.9944 (1.1144)  loss_vfl_dn_5: 0.5937 (0.6181)  loss_bbox_dn_5: 0.6060 (0.7639)  loss_giou_dn_5: 0.9941 (1.1159)  time: 0.2742  data: 0.0051  max mem: 9222
Epoch: [0]  [ 2600/29571]  eta: 2:02:18  lr: 0.000010  loss: 30.3786 (34.3979)  loss_vfl: 1.0768 (1.2080)  loss_bbox: 0.3351 (0.5661)  loss_giou: 0.8047 (1.0108)  loss_vfl_aux_0: 0.9259 (1.0453)  loss_bbox_aux_0: 0.4221 (0.6431)  loss_giou_aux_0: 0.9055 (1.0937)  loss_vfl_aux_1: 1.0456 (1.1115)  loss_bbox_aux_1: 0.3611 (0.5998)  loss_giou_aux_1: 0.8431 (1.0479)  loss_vfl_aux_2: 1.0293 (1.1526)  loss_bbox_aux_2: 0.3332 (0.5816)  loss_giou_aux_2: 0.8320 (1.0266)  loss_vfl_aux_3: 1.0529 (1.1724)  loss_bbox_aux_3: 0.3251 (0.5725)  loss_giou_aux_3: 0.8143 (1.0171)  loss_vfl_aux_4: 1.0672 (1.1868)  loss_bbox_aux_4: 0.3348 (0.5689)  loss_giou_aux_4: 0.8054 (1.0131)  loss_vfl_aux_5: 0.8788 (0.9576)  loss_bbox_aux_5: 0.5735 (0.7447)  loss_giou_aux_5: 1.0507 (1.1995)  loss_vfl_dn_0: 0.4554 (0.5070)  loss_bbox_dn_0: 0.7330 (0.8170)  loss_giou_dn_0: 1.1219 (1.1862)  loss_vfl_dn_1: 0.4707 (0.5439)  loss_bbox_dn_1: 0.6500 (0.7768)  loss_giou_dn_1: 1.0513 (1.1347)  loss_vfl_dn_2: 0.5100 (0.5855)  loss_bbox_dn_2: 0.6194 (0.7641)  loss_giou_dn_2: 1.0279 (1.1158)  loss_vfl_dn_3: 0.5228 (0.6077)  loss_bbox_dn_3: 0.6069 (0.7595)  loss_giou_dn_3: 1.0061 (1.1085)  loss_vfl_dn_4: 0.5287 (0.6154)  loss_bbox_dn_4: 0.6065 (0.7594)  loss_giou_dn_4: 1.0022 (1.1092)  loss_vfl_dn_5: 0.5248 (0.6172)  loss_bbox_dn_5: 0.6065 (0.7597)  loss_giou_dn_5: 1.0050 (1.1107)  time: 0.2615  data: 0.0054  max mem: 9222
Epoch: [0]  [ 2700/29571]  eta: 2:01:52  lr: 0.000010  loss: 30.7615 (34.2638)  loss_vfl: 1.1230 (1.2090)  loss_bbox: 0.4058 (0.5595)  loss_giou: 0.8423 (1.0047)  loss_vfl_aux_0: 1.0598 (1.0480)  loss_bbox_aux_0: 0.4672 (0.6364)  loss_giou_aux_0: 0.9621 (1.0880)  loss_vfl_aux_1: 1.0614 (1.1138)  loss_bbox_aux_1: 0.4523 (0.5930)  loss_giou_aux_1: 0.9150 (1.0418)  loss_vfl_aux_2: 1.1008 (1.1547)  loss_bbox_aux_2: 0.4147 (0.5748)  loss_giou_aux_2: 0.8747 (1.0205)  loss_vfl_aux_3: 1.1129 (1.1746)  loss_bbox_aux_3: 0.3778 (0.5657)  loss_giou_aux_3: 0.8579 (1.0108)  loss_vfl_aux_4: 1.1046 (1.1883)  loss_bbox_aux_4: 0.4067 (0.5622)  loss_giou_aux_4: 0.8509 (1.0070)  loss_vfl_aux_5: 1.0671 (0.9635)  loss_bbox_aux_5: 0.6801 (0.7384)  loss_giou_aux_5: 1.1069 (1.1945)  loss_vfl_dn_0: 0.4884 (0.5063)  loss_bbox_dn_0: 0.6726 (0.8131)  loss_giou_dn_0: 1.1146 (1.1831)  loss_vfl_dn_1: 0.5042 (0.5426)  loss_bbox_dn_1: 0.6278 (0.7719)  loss_giou_dn_1: 1.0092 (1.1304)  loss_vfl_dn_2: 0.5512 (0.5839)  loss_bbox_dn_2: 0.6092 (0.7588)  loss_giou_dn_2: 0.9765 (1.1112)  loss_vfl_dn_3: 0.5843 (0.6064)  loss_bbox_dn_3: 0.5948 (0.7542)  loss_giou_dn_3: 0.9700 (1.1037)  loss_vfl_dn_4: 0.5943 (0.6143)  loss_bbox_dn_4: 0.5898 (0.7541)  loss_giou_dn_4: 0.9638 (1.1043)  loss_vfl_dn_5: 0.6032 (0.6162)  loss_bbox_dn_5: 0.5897 (0.7543)  loss_giou_dn_5: 0.9626 (1.1058)  time: 0.2874  data: 0.0052  max mem: 9222
Epoch: [0]  [ 2800/29571]  eta: 2:01:24  lr: 0.000010  loss: 29.7813 (34.1335)  loss_vfl: 1.2595 (1.2107)  loss_bbox: 0.3029 (0.5529)  loss_giou: 0.7023 (0.9974)  loss_vfl_aux_0: 1.1784 (1.0506)  loss_bbox_aux_0: 0.3620 (0.6298)  loss_giou_aux_0: 0.7726 (1.0809)  loss_vfl_aux_1: 1.1628 (1.1168)  loss_bbox_aux_1: 0.3191 (0.5863)  loss_giou_aux_1: 0.6950 (1.0345)  loss_vfl_aux_2: 1.2407 (1.1576)  loss_bbox_aux_2: 0.3117 (0.5681)  loss_giou_aux_2: 0.7172 (1.0131)  loss_vfl_aux_3: 1.3034 (1.1767)  loss_bbox_aux_3: 0.3071 (0.5591)  loss_giou_aux_3: 0.7055 (1.0035)  loss_vfl_aux_4: 1.2459 (1.1901)  loss_bbox_aux_4: 0.3103 (0.5556)  loss_giou_aux_4: 0.7018 (0.9996)  loss_vfl_aux_5: 1.0849 (0.9694)  loss_bbox_aux_5: 0.4819 (0.7325)  loss_giou_aux_5: 0.9055 (1.1886)  loss_vfl_dn_0: 0.4821 (0.5058)  loss_bbox_dn_0: 0.7739 (0.8106)  loss_giou_dn_0: 1.0754 (1.1798)  loss_vfl_dn_1: 0.5053 (0.5418)  loss_bbox_dn_1: 0.6702 (0.7686)  loss_giou_dn_1: 0.9684 (1.1258)  loss_vfl_dn_2: 0.5492 (0.5828)  loss_bbox_dn_2: 0.6761 (0.7553)  loss_giou_dn_2: 0.9537 (1.1060)  loss_vfl_dn_3: 0.5697 (0.6053)  loss_bbox_dn_3: 0.6695 (0.7505)  loss_giou_dn_3: 0.9525 (1.0983)  loss_vfl_dn_4: 0.5774 (0.6134)  loss_bbox_dn_4: 0.6700 (0.7504)  loss_giou_dn_4: 0.9533 (1.0988)  loss_vfl_dn_5: 0.5833 (0.6155)  loss_bbox_dn_5: 0.6711 (0.7507)  loss_giou_dn_5: 0.9535 (1.1002)  time: 0.2809  data: 0.0052  max mem: 9222
Epoch: [0]  [ 2900/29571]  eta: 2:00:57  lr: 0.000010  loss: 30.9559 (34.0127)  loss_vfl: 1.2012 (1.2134)  loss_bbox: 0.3866 (0.5464)  loss_giou: 0.7366 (0.9902)  loss_vfl_aux_0: 1.1204 (1.0553)  loss_bbox_aux_0: 0.4593 (0.6229)  loss_giou_aux_0: 0.8221 (1.0739)  loss_vfl_aux_1: 1.1612 (1.1207)  loss_bbox_aux_1: 0.4383 (0.5796)  loss_giou_aux_1: 0.8031 (1.0275)  loss_vfl_aux_2: 1.2632 (1.1611)  loss_bbox_aux_2: 0.3925 (0.5615)  loss_giou_aux_2: 0.7547 (1.0060)  loss_vfl_aux_3: 1.2338 (1.1801)  loss_bbox_aux_3: 0.3504 (0.5525)  loss_giou_aux_3: 0.7416 (0.9963)  loss_vfl_aux_4: 1.2398 (1.1928)  loss_bbox_aux_4: 0.3762 (0.5490)  loss_giou_aux_4: 0.7394 (0.9924)  loss_vfl_aux_5: 1.1551 (0.9762)  loss_bbox_aux_5: 0.6253 (0.7259)  loss_giou_aux_5: 0.9836 (1.1823)  loss_vfl_dn_0: 0.4908 (0.5054)  loss_bbox_dn_0: 0.8546 (0.8077)  loss_giou_dn_0: 1.0729 (1.1767)  loss_vfl_dn_1: 0.5240 (0.5412)  loss_bbox_dn_1: 0.7838 (0.7649)  loss_giou_dn_1: 0.9755 (1.1216)  loss_vfl_dn_2: 0.5511 (0.5819)  loss_bbox_dn_2: 0.7678 (0.7514)  loss_giou_dn_2: 0.9516 (1.1013)  loss_vfl_dn_3: 0.5782 (0.6047)  loss_bbox_dn_3: 0.7451 (0.7466)  loss_giou_dn_3: 0.9261 (1.0933)  loss_vfl_dn_4: 0.5864 (0.6129)  loss_bbox_dn_4: 0.7454 (0.7465)  loss_giou_dn_4: 0.9239 (1.0938)  loss_vfl_dn_5: 0.5936 (0.6151)  loss_bbox_dn_5: 0.7453 (0.7467)  loss_giou_dn_5: 0.9239 (1.0952)  time: 0.2745  data: 0.0052  max mem: 9222
Epoch: [0]  [ 3000/29571]  eta: 2:00:33  lr: 0.000010  loss: 29.6270 (33.8848)  loss_vfl: 1.2528 (1.2133)  loss_bbox: 0.3244 (0.5404)  loss_giou: 0.6701 (0.9848)  loss_vfl_aux_0: 1.1264 (1.0572)  loss_bbox_aux_0: 0.4368 (0.6162)  loss_giou_aux_0: 0.7756 (1.0684)  loss_vfl_aux_1: 1.2242 (1.1219)  loss_bbox_aux_1: 0.3791 (0.5734)  loss_giou_aux_1: 0.7108 (1.0220)  loss_vfl_aux_2: 1.2178 (1.1620)  loss_bbox_aux_2: 0.3423 (0.5552)  loss_giou_aux_2: 0.6974 (1.0006)  loss_vfl_aux_3: 1.3088 (1.1809)  loss_bbox_aux_3: 0.3796 (0.5464)  loss_giou_aux_3: 0.6754 (0.9909)  loss_vfl_aux_4: 1.2989 (1.1931)  loss_bbox_aux_4: 0.3655 (0.5430)  loss_giou_aux_4: 0.6732 (0.9870)  loss_vfl_aux_5: 1.1919 (0.9803)  loss_bbox_aux_5: 0.4992 (0.7192)  loss_giou_aux_5: 0.9326 (1.1774)  loss_vfl_dn_0: 0.5072 (0.5048)  loss_bbox_dn_0: 0.6575 (0.8035)  loss_giou_dn_0: 1.0612 (1.1741)  loss_vfl_dn_1: 0.5311 (0.5404)  loss_bbox_dn_1: 0.6161 (0.7603)  loss_giou_dn_1: 0.9581 (1.1180)  loss_vfl_dn_2: 0.5650 (0.5808)  loss_bbox_dn_2: 0.5997 (0.7466)  loss_giou_dn_2: 0.9144 (1.0973)  loss_vfl_dn_3: 0.5747 (0.6037)  loss_bbox_dn_3: 0.6074 (0.7418)  loss_giou_dn_3: 0.8987 (1.0892)  loss_vfl_dn_4: 0.5823 (0.6120)  loss_bbox_dn_4: 0.6088 (0.7417)  loss_giou_dn_4: 0.8945 (1.0896)  loss_vfl_dn_5: 0.5802 (0.6144)  loss_bbox_dn_5: 0.6091 (0.7419)  loss_giou_dn_5: 0.8958 (1.0910)  time: 0.2663  data: 0.0051  max mem: 9222
Epoch: [0]  [ 3100/29571]  eta: 2:00:05  lr: 0.000010  loss: 28.8993 (33.7735)  loss_vfl: 1.1334 (1.2160)  loss_bbox: 0.3649 (0.5342)  loss_giou: 0.7904 (0.9776)  loss_vfl_aux_0: 1.0592 (1.0618)  loss_bbox_aux_0: 0.4637 (0.6101)  loss_giou_aux_0: 0.8696 (1.0613)  loss_vfl_aux_1: 1.0771 (1.1263)  loss_bbox_aux_1: 0.3566 (0.5672)  loss_giou_aux_1: 0.8125 (1.0148)  loss_vfl_aux_2: 1.0638 (1.1660)  loss_bbox_aux_2: 0.3619 (0.5490)  loss_giou_aux_2: 0.7914 (0.9932)  loss_vfl_aux_3: 1.1564 (1.1844)  loss_bbox_aux_3: 0.3596 (0.5402)  loss_giou_aux_3: 0.7787 (0.9837)  loss_vfl_aux_4: 1.1519 (1.1961)  loss_bbox_aux_4: 0.3674 (0.5370)  loss_giou_aux_4: 0.7882 (0.9798)  loss_vfl_aux_5: 1.0547 (0.9873)  loss_bbox_aux_5: 0.4947 (0.7132)  loss_giou_aux_5: 1.0409 (1.1712)  loss_vfl_dn_0: 0.4845 (0.5046)  loss_bbox_dn_0: 0.6158 (0.8018)  loss_giou_dn_0: 1.0822 (1.1708)  loss_vfl_dn_1: 0.5158 (0.5401)  loss_bbox_dn_1: 0.5701 (0.7576)  loss_giou_dn_1: 0.9892 (1.1133)  loss_vfl_dn_2: 0.5370 (0.5803)  loss_bbox_dn_2: 0.5410 (0.7436)  loss_giou_dn_2: 0.9554 (1.0921)  loss_vfl_dn_3: 0.5519 (0.6033)  loss_bbox_dn_3: 0.5309 (0.7387)  loss_giou_dn_3: 0.9533 (1.0838)  loss_vfl_dn_4: 0.5622 (0.6119)  loss_bbox_dn_4: 0.5330 (0.7385)  loss_giou_dn_4: 0.9512 (1.0842)  loss_vfl_dn_5: 0.5650 (0.6144)  loss_bbox_dn_5: 0.5330 (0.7387)  loss_giou_dn_5: 0.9517 (1.0855)  time: 0.2790  data: 0.0053  max mem: 9222
Epoch: [0]  [ 3200/29571]  eta: 1:59:40  lr: 0.000010  loss: 29.5608 (33.6502)  loss_vfl: 1.2881 (1.2166)  loss_bbox: 0.3020 (0.5282)  loss_giou: 0.7495 (0.9719)  loss_vfl_aux_0: 1.1892 (1.0638)  loss_bbox_aux_0: 0.3643 (0.6038)  loss_giou_aux_0: 0.8582 (1.0556)  loss_vfl_aux_1: 1.2091 (1.1273)  loss_bbox_aux_1: 0.3346 (0.5611)  loss_giou_aux_1: 0.7914 (1.0092)  loss_vfl_aux_2: 1.2018 (1.1671)  loss_bbox_aux_2: 0.3101 (0.5429)  loss_giou_aux_2: 0.7610 (0.9876)  loss_vfl_aux_3: 1.2357 (1.1854)  loss_bbox_aux_3: 0.2957 (0.5341)  loss_giou_aux_3: 0.7802 (0.9780)  loss_vfl_aux_4: 1.2394 (1.1969)  loss_bbox_aux_4: 0.3078 (0.5309)  loss_giou_aux_4: 0.7511 (0.9741)  loss_vfl_aux_5: 1.2223 (0.9914)  loss_bbox_aux_5: 0.4589 (0.7071)  loss_giou_aux_5: 0.9627 (1.1662)  loss_vfl_dn_0: 0.4842 (0.5041)  loss_bbox_dn_0: 0.6710 (0.7984)  loss_giou_dn_0: 1.0639 (1.1682)  loss_vfl_dn_1: 0.5023 (0.5392)  loss_bbox_dn_1: 0.6281 (0.7537)  loss_giou_dn_1: 1.0087 (1.1098)  loss_vfl_dn_2: 0.5349 (0.5790)  loss_bbox_dn_2: 0.6271 (0.7395)  loss_giou_dn_2: 0.9803 (1.0881)  loss_vfl_dn_3: 0.5635 (0.6020)  loss_bbox_dn_3: 0.6217 (0.7347)  loss_giou_dn_3: 0.9622 (1.0798)  loss_vfl_dn_4: 0.5794 (0.6107)  loss_bbox_dn_4: 0.6223 (0.7345)  loss_giou_dn_4: 0.9495 (1.0800)  loss_vfl_dn_5: 0.5942 (0.6133)  loss_bbox_dn_5: 0.6224 (0.7347)  loss_giou_dn_5: 0.9492 (1.0814)  time: 0.2578  data: 0.0052  max mem: 9222
Epoch: [0]  [ 3300/29571]  eta: 1:59:07  lr: 0.000010  loss: 29.6816 (33.5462)  loss_vfl: 1.2455 (1.2178)  loss_bbox: 0.3485 (0.5231)  loss_giou: 0.7306 (0.9665)  loss_vfl_aux_0: 1.2296 (1.0665)  loss_bbox_aux_0: 0.4300 (0.5986)  loss_giou_aux_0: 0.8279 (1.0502)  loss_vfl_aux_1: 1.2625 (1.1298)  loss_bbox_aux_1: 0.3856 (0.5559)  loss_giou_aux_1: 0.7730 (1.0035)  loss_vfl_aux_2: 1.2488 (1.1697)  loss_bbox_aux_2: 0.3477 (0.5375)  loss_giou_aux_2: 0.7339 (0.9821)  loss_vfl_aux_3: 1.2815 (1.1872)  loss_bbox_aux_3: 0.3430 (0.5290)  loss_giou_aux_3: 0.7497 (0.9725)  loss_vfl_aux_4: 1.2655 (1.1983)  loss_bbox_aux_4: 0.3435 (0.5258)  loss_giou_aux_4: 0.7339 (0.9686)  loss_vfl_aux_5: 1.0680 (0.9962)  loss_bbox_aux_5: 0.6002 (0.7024)  loss_giou_aux_5: 0.9171 (1.1614)  loss_vfl_dn_0: 0.4836 (0.5038)  loss_bbox_dn_0: 0.7710 (0.7960)  loss_giou_dn_0: 1.0468 (1.1654)  loss_vfl_dn_1: 0.5261 (0.5386)  loss_bbox_dn_1: 0.6118 (0.7505)  loss_giou_dn_1: 0.9323 (1.1060)  loss_vfl_dn_2: 0.5664 (0.5783)  loss_bbox_dn_2: 0.5813 (0.7360)  loss_giou_dn_2: 0.8989 (1.0839)  loss_vfl_dn_3: 0.5824 (0.6014)  loss_bbox_dn_3: 0.5750 (0.7310)  loss_giou_dn_3: 0.8746 (1.0754)  loss_vfl_dn_4: 0.5746 (0.6102)  loss_bbox_dn_4: 0.5645 (0.7308)  loss_giou_dn_4: 0.8712 (1.0755)  loss_vfl_dn_5: 0.5792 (0.6129)  loss_bbox_dn_5: 0.5645 (0.7311)  loss_giou_dn_5: 0.8715 (1.0768)  time: 0.2733  data: 0.0051  max mem: 9222
Epoch: [0]  [ 3400/29571]  eta: 1:58:41  lr: 0.000010  loss: 28.9364 (33.4508)  loss_vfl: 1.1249 (1.2193)  loss_bbox: 0.3260 (0.5183)  loss_giou: 0.7117 (0.9609)  loss_vfl_aux_0: 1.0655 (1.0690)  loss_bbox_aux_0: 0.3921 (0.5936)  loss_giou_aux_0: 0.7596 (1.0447)  loss_vfl_aux_1: 1.0688 (1.1324)  loss_bbox_aux_1: 0.3552 (0.5509)  loss_giou_aux_1: 0.7232 (0.9978)  loss_vfl_aux_2: 1.1438 (1.1715)  loss_bbox_aux_2: 0.3415 (0.5327)  loss_giou_aux_2: 0.7240 (0.9764)  loss_vfl_aux_3: 1.1080 (1.1891)  loss_bbox_aux_3: 0.3331 (0.5241)  loss_giou_aux_3: 0.7257 (0.9668)  loss_vfl_aux_4: 1.1372 (1.1999)  loss_bbox_aux_4: 0.3312 (0.5210)  loss_giou_aux_4: 0.7164 (0.9630)  loss_vfl_aux_5: 0.9902 (1.0004)  loss_bbox_aux_5: 0.4879 (0.6978)  loss_giou_aux_5: 0.9362 (1.1566)  loss_vfl_dn_0: 0.4909 (0.5034)  loss_bbox_dn_0: 0.5735 (0.7946)  loss_giou_dn_0: 1.0578 (1.1629)  loss_vfl_dn_1: 0.5135 (0.5382)  loss_bbox_dn_1: 0.4898 (0.7483)  loss_giou_dn_1: 0.9742 (1.1024)  loss_vfl_dn_2: 0.5542 (0.5776)  loss_bbox_dn_2: 0.4630 (0.7336)  loss_giou_dn_2: 0.9250 (1.0798)  loss_vfl_dn_3: 0.5813 (0.6009)  loss_bbox_dn_3: 0.4481 (0.7286)  loss_giou_dn_3: 0.9086 (1.0711)  loss_vfl_dn_4: 0.5959 (0.6099)  loss_bbox_dn_4: 0.4447 (0.7283)  loss_giou_dn_4: 0.9016 (1.0712)  loss_vfl_dn_5: 0.5897 (0.6127)  loss_bbox_dn_5: 0.4444 (0.7285)  loss_giou_dn_5: 0.9012 (1.0725)  time: 0.2812  data: 0.0053  max mem: 9222
Epoch: [0]  [ 3500/29571]  eta: 1:58:11  lr: 0.000010  loss: 30.8209 (33.3614)  loss_vfl: 1.2343 (1.2196)  loss_bbox: 0.3566 (0.5143)  loss_giou: 0.8033 (0.9565)  loss_vfl_aux_0: 1.0692 (1.0709)  loss_bbox_aux_0: 0.4556 (0.5894)  loss_giou_aux_0: 0.8601 (1.0404)  loss_vfl_aux_1: 1.1290 (1.1342)  loss_bbox_aux_1: 0.3413 (0.5467)  loss_giou_aux_1: 0.8142 (0.9933)  loss_vfl_aux_2: 1.1958 (1.1726)  loss_bbox_aux_2: 0.3467 (0.5286)  loss_giou_aux_2: 0.8020 (0.9718)  loss_vfl_aux_3: 1.1625 (1.1897)  loss_bbox_aux_3: 0.3811 (0.5202)  loss_giou_aux_3: 0.8002 (0.9624)  loss_vfl_aux_4: 1.1440 (1.2002)  loss_bbox_aux_4: 0.3663 (0.5171)  loss_giou_aux_4: 0.7981 (0.9587)  loss_vfl_aux_5: 1.0962 (1.0037)  loss_bbox_aux_5: 0.5880 (0.6941)  loss_giou_aux_5: 0.9374 (1.1529)  loss_vfl_dn_0: 0.4829 (0.5031)  loss_bbox_dn_0: 0.7952 (0.7927)  loss_giou_dn_0: 1.0592 (1.1606)  loss_vfl_dn_1: 0.5180 (0.5376)  loss_bbox_dn_1: 0.7205 (0.7457)  loss_giou_dn_1: 0.9834 (1.0991)  loss_vfl_dn_2: 0.5611 (0.5768)  loss_bbox_dn_2: 0.6991 (0.7307)  loss_giou_dn_2: 0.9570 (1.0761)  loss_vfl_dn_3: 0.5910 (0.6003)  loss_bbox_dn_3: 0.6725 (0.7256)  loss_giou_dn_3: 0.9542 (1.0673)  loss_vfl_dn_4: 0.6095 (0.6095)  loss_bbox_dn_4: 0.6632 (0.7253)  loss_giou_dn_4: 0.9557 (1.0673)  loss_vfl_dn_5: 0.6340 (0.6125)  loss_bbox_dn_5: 0.6625 (0.7256)  loss_giou_dn_5: 0.9574 (1.0686)  time: 0.2694  data: 0.0054  max mem: 9222
Epoch: [0]  [ 3600/29571]  eta: 1:57:41  lr: 0.000010  loss: 27.4224 (33.2490)  loss_vfl: 1.1885 (1.2201)  loss_bbox: 0.2407 (0.5092)  loss_giou: 0.7496 (0.9514)  loss_vfl_aux_0: 1.0752 (1.0727)  loss_bbox_aux_0: 0.3126 (0.5837)  loss_giou_aux_0: 0.8511 (1.0352)  loss_vfl_aux_1: 1.0637 (1.1356)  loss_bbox_aux_1: 0.2765 (0.5412)  loss_giou_aux_1: 0.7934 (0.9882)  loss_vfl_aux_2: 1.1373 (1.1738)  loss_bbox_aux_2: 0.2558 (0.5233)  loss_giou_aux_2: 0.7577 (0.9667)  loss_vfl_aux_3: 1.1044 (1.1909)  loss_bbox_aux_3: 0.2445 (0.5149)  loss_giou_aux_3: 0.7535 (0.9573)  loss_vfl_aux_4: 1.1088 (1.2006)  loss_bbox_aux_4: 0.2386 (0.5120)  loss_giou_aux_4: 0.7585 (0.9537)  loss_vfl_aux_5: 1.0341 (1.0079)  loss_bbox_aux_5: 0.3784 (0.6882)  loss_giou_aux_5: 0.9647 (1.1480)  loss_vfl_dn_0: 0.4960 (0.5028)  loss_bbox_dn_0: 0.5252 (0.7890)  loss_giou_dn_0: 1.0715 (1.1581)  loss_vfl_dn_1: 0.5160 (0.5372)  loss_bbox_dn_1: 0.4655 (0.7415)  loss_giou_dn_1: 0.9770 (1.0956)  loss_vfl_dn_2: 0.5378 (0.5759)  loss_bbox_dn_2: 0.4414 (0.7264)  loss_giou_dn_2: 0.9391 (1.0723)  loss_vfl_dn_3: 0.5570 (0.5995)  loss_bbox_dn_3: 0.4388 (0.7213)  loss_giou_dn_3: 0.9261 (1.0634)  loss_vfl_dn_4: 0.5626 (0.6088)  loss_bbox_dn_4: 0.4353 (0.7211)  loss_giou_dn_4: 0.9267 (1.0634)  loss_vfl_dn_5: 0.5880 (0.6120)  loss_bbox_dn_5: 0.4353 (0.7213)  loss_giou_dn_5: 0.9269 (1.0646)  time: 0.2534  data: 0.0052  max mem: 9222
Epoch: [0]  [ 3700/29571]  eta: 1:57:15  lr: 0.000010  loss: 29.1941 (33.1496)  loss_vfl: 1.1686 (1.2209)  loss_bbox: 0.3187 (0.5049)  loss_giou: 0.7618 (0.9461)  loss_vfl_aux_0: 1.1169 (1.0745)  loss_bbox_aux_0: 0.3860 (0.5793)  loss_giou_aux_0: 0.8243 (1.0301)  loss_vfl_aux_1: 1.1417 (1.1369)  loss_bbox_aux_1: 0.3551 (0.5368)  loss_giou_aux_1: 0.7895 (0.9828)  loss_vfl_aux_2: 1.1017 (1.1748)  loss_bbox_aux_2: 0.3228 (0.5189)  loss_giou_aux_2: 0.7682 (0.9615)  loss_vfl_aux_3: 1.1313 (1.1915)  loss_bbox_aux_3: 0.3214 (0.5106)  loss_giou_aux_3: 0.7661 (0.9520)  loss_vfl_aux_4: 1.1070 (1.2015)  loss_bbox_aux_4: 0.3216 (0.5076)  loss_giou_aux_4: 0.7603 (0.9485)  loss_vfl_aux_5: 1.1104 (1.0124)  loss_bbox_aux_5: 0.5385 (0.6840)  loss_giou_aux_5: 0.9726 (1.1430)  loss_vfl_dn_0: 0.4926 (0.5026)  loss_bbox_dn_0: 0.6773 (0.7869)  loss_giou_dn_0: 1.0702 (1.1557)  loss_vfl_dn_1: 0.5008 (0.5365)  loss_bbox_dn_1: 0.5934 (0.7387)  loss_giou_dn_1: 0.9890 (1.0923)  loss_vfl_dn_2: 0.5232 (0.5750)  loss_bbox_dn_2: 0.5682 (0.7233)  loss_giou_dn_2: 0.9370 (1.0685)  loss_vfl_dn_3: 0.5479 (0.5986)  loss_bbox_dn_3: 0.5743 (0.7182)  loss_giou_dn_3: 0.9239 (1.0594)  loss_vfl_dn_4: 0.5645 (0.6081)  loss_bbox_dn_4: 0.5755 (0.7179)  loss_giou_dn_4: 0.9163 (1.0593)  loss_vfl_dn_5: 0.5655 (0.6114)  loss_bbox_dn_5: 0.5756 (0.7181)  loss_giou_dn_5: 0.9160 (1.0605)  time: 0.2822  data: 0.0052  max mem: 9222
Epoch: [0]  [ 3800/29571]  eta: 1:56:50  lr: 0.000010  loss: 29.6599 (33.0613)  loss_vfl: 1.0809 (1.2200)  loss_bbox: 0.3699 (0.5015)  loss_giou: 0.8329 (0.9423)  loss_vfl_aux_0: 1.0455 (1.0757)  loss_bbox_aux_0: 0.4136 (0.5756)  loss_giou_aux_0: 0.8826 (1.0262)  loss_vfl_aux_1: 1.0964 (1.1375)  loss_bbox_aux_1: 0.3639 (0.5332)  loss_giou_aux_1: 0.8658 (0.9790)  loss_vfl_aux_2: 1.0741 (1.1750)  loss_bbox_aux_2: 0.3527 (0.5153)  loss_giou_aux_2: 0.8474 (0.9576)  loss_vfl_aux_3: 1.0469 (1.1913)  loss_bbox_aux_3: 0.3718 (0.5071)  loss_giou_aux_3: 0.8434 (0.9482)  loss_vfl_aux_4: 1.0447 (1.2008)  loss_bbox_aux_4: 0.3685 (0.5042)  loss_giou_aux_4: 0.8375 (0.9447)  loss_vfl_aux_5: 1.0582 (1.0153)  loss_bbox_aux_5: 0.4872 (0.6805)  loss_giou_aux_5: 1.0182 (1.1395)  loss_vfl_dn_0: 0.4748 (0.5022)  loss_bbox_dn_0: 0.6181 (0.7843)  loss_giou_dn_0: 1.1036 (1.1536)  loss_vfl_dn_1: 0.4883 (0.5360)  loss_bbox_dn_1: 0.5118 (0.7356)  loss_giou_dn_1: 1.0084 (1.0896)  loss_vfl_dn_2: 0.5093 (0.5741)  loss_bbox_dn_2: 0.5014 (0.7201)  loss_giou_dn_2: 0.9726 (1.0655)  loss_vfl_dn_3: 0.5208 (0.5977)  loss_bbox_dn_3: 0.4982 (0.7149)  loss_giou_dn_3: 0.9760 (1.0563)  loss_vfl_dn_4: 0.5558 (0.6074)  loss_bbox_dn_4: 0.5007 (0.7146)  loss_giou_dn_4: 0.9770 (1.0561)  loss_vfl_dn_5: 0.5562 (0.6108)  loss_bbox_dn_5: 0.5010 (0.7148)  loss_giou_dn_5: 0.9775 (1.0572)  time: 0.2538  data: 0.0053  max mem: 9222
Epoch: [0]  [ 3900/29571]  eta: 1:56:23  lr: 0.000010  loss: 28.3989 (32.9647)  loss_vfl: 1.1984 (1.2194)  loss_bbox: 0.2241 (0.4973)  loss_giou: 0.8037 (0.9388)  loss_vfl_aux_0: 1.0327 (1.0761)  loss_bbox_aux_0: 0.2997 (0.5710)  loss_giou_aux_0: 0.9002 (1.0225)  loss_vfl_aux_1: 1.0189 (1.1378)  loss_bbox_aux_1: 0.2628 (0.5288)  loss_giou_aux_1: 0.8427 (0.9753)  loss_vfl_aux_2: 1.0762 (1.1753)  loss_bbox_aux_2: 0.2518 (0.5110)  loss_giou_aux_2: 0.8138 (0.9540)  loss_vfl_aux_3: 1.0785 (1.1911)  loss_bbox_aux_3: 0.2398 (0.5029)  loss_giou_aux_3: 0.7995 (0.9446)  loss_vfl_aux_4: 1.1244 (1.2006)  loss_bbox_aux_4: 0.2367 (0.4999)  loss_giou_aux_4: 0.7928 (0.9412)  loss_vfl_aux_5: 0.9353 (1.0176)  loss_bbox_aux_5: 0.4905 (0.6759)  loss_giou_aux_5: 1.0215 (1.1362)  loss_vfl_dn_0: 0.4637 (0.5018)  loss_bbox_dn_0: 0.5396 (0.7810)  loss_giou_dn_0: 1.1258 (1.1517)  loss_vfl_dn_1: 0.4773 (0.5353)  loss_bbox_dn_1: 0.4530 (0.7319)  loss_giou_dn_1: 1.0136 (1.0868)  loss_vfl_dn_2: 0.5026 (0.5732)  loss_bbox_dn_2: 0.4466 (0.7162)  loss_giou_dn_2: 0.9688 (1.0624)  loss_vfl_dn_3: 0.5207 (0.5969)  loss_bbox_dn_3: 0.4494 (0.7111)  loss_giou_dn_3: 0.9612 (1.0532)  loss_vfl_dn_4: 0.5286 (0.6067)  loss_bbox_dn_4: 0.4491 (0.7108)  loss_giou_dn_4: 0.9622 (1.0529)  loss_vfl_dn_5: 0.5271 (0.6102)  loss_bbox_dn_5: 0.4491 (0.7110)  loss_giou_dn_5: 0.9619 (1.0541)  time: 0.2893  data: 0.0053  max mem: 9222
