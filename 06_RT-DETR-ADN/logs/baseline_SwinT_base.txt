WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
create: RTDETR
create: SwinTransformer
name : pre.0.weight, param : torch.Size([96, 3, 4, 4])
name : pre.0.bias, param : torch.Size([96])
name : pre.2.weight, param : torch.Size([96])
name : pre.2.bias, param : torch.Size([96])
name : features.0.0.norm1.weight, param : torch.Size([96])
name : features.0.0.norm1.bias, param : torch.Size([96])
name : features.0.0.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.0.attn.relative_position_index, param : torch.Size([2401])
name : features.0.0.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.0.attn.qkv.bias, param : torch.Size([288])
name : features.0.0.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.0.attn.proj.bias, param : torch.Size([96])
name : features.0.0.norm2.weight, param : torch.Size([96])
name : features.0.0.norm2.bias, param : torch.Size([96])
name : features.0.0.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.0.mlp.0.bias, param : torch.Size([384])
name : features.0.0.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.0.mlp.3.bias, param : torch.Size([96])
name : features.0.0.norm1_skip.weight, param : torch.Size([96])
name : features.0.0.norm1_skip.bias, param : torch.Size([96])
name : features.0.0.norm2_skip.weight, param : torch.Size([96])
name : features.0.0.norm2_skip.bias, param : torch.Size([96])
name : features.0.1.norm1.weight, param : torch.Size([96])
name : features.0.1.norm1.bias, param : torch.Size([96])
name : features.0.1.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.1.attn.relative_position_index, param : torch.Size([2401])
name : features.0.1.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.1.attn.qkv.bias, param : torch.Size([288])
name : features.0.1.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.1.attn.proj.bias, param : torch.Size([96])
name : features.0.1.norm2.weight, param : torch.Size([96])
name : features.0.1.norm2.bias, param : torch.Size([96])
name : features.0.1.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.1.mlp.0.bias, param : torch.Size([384])
name : features.0.1.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.1.mlp.3.bias, param : torch.Size([96])
name : features.1.reduction.weight, param : torch.Size([192, 384])
name : features.1.norm.weight, param : torch.Size([384])
name : features.1.norm.bias, param : torch.Size([384])
name : features.1.norm_skip.weight, param : torch.Size([384])
name : features.1.norm_skip.bias, param : torch.Size([384])
name : features.2.0.norm1.weight, param : torch.Size([192])
name : features.2.0.norm1.bias, param : torch.Size([192])
name : features.2.0.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.0.attn.relative_position_index, param : torch.Size([2401])
name : features.2.0.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.0.attn.qkv.bias, param : torch.Size([576])
name : features.2.0.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.0.attn.proj.bias, param : torch.Size([192])
name : features.2.0.norm2.weight, param : torch.Size([192])
name : features.2.0.norm2.bias, param : torch.Size([192])
name : features.2.0.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.0.mlp.0.bias, param : torch.Size([768])
name : features.2.0.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.0.mlp.3.bias, param : torch.Size([192])
name : features.2.0.norm1_skip.weight, param : torch.Size([192])
name : features.2.0.norm1_skip.bias, param : torch.Size([192])
name : features.2.0.norm2_skip.weight, param : torch.Size([192])
name : features.2.0.norm2_skip.bias, param : torch.Size([192])
name : features.2.1.norm1.weight, param : torch.Size([192])
name : features.2.1.norm1.bias, param : torch.Size([192])
name : features.2.1.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.1.attn.relative_position_index, param : torch.Size([2401])
name : features.2.1.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.1.attn.qkv.bias, param : torch.Size([576])
name : features.2.1.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.1.attn.proj.bias, param : torch.Size([192])
name : features.2.1.norm2.weight, param : torch.Size([192])
name : features.2.1.norm2.bias, param : torch.Size([192])
name : features.2.1.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.1.mlp.0.bias, param : torch.Size([768])
name : features.2.1.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.1.mlp.3.bias, param : torch.Size([192])
name : features.3.reduction.weight, param : torch.Size([384, 768])
name : features.3.norm.weight, param : torch.Size([768])
name : features.3.norm.bias, param : torch.Size([768])
name : features.3.norm_skip.weight, param : torch.Size([768])
name : features.3.norm_skip.bias, param : torch.Size([768])
name : features.4.0.norm1.weight, param : torch.Size([384])
name : features.4.0.norm1.bias, param : torch.Size([384])
name : features.4.0.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.0.attn.relative_position_index, param : torch.Size([2401])
name : features.4.0.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.0.attn.qkv.bias, param : torch.Size([1152])
name : features.4.0.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.0.attn.proj.bias, param : torch.Size([384])
name : features.4.0.norm2.weight, param : torch.Size([384])
name : features.4.0.norm2.bias, param : torch.Size([384])
name : features.4.0.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.0.mlp.0.bias, param : torch.Size([1536])
name : features.4.0.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.0.mlp.3.bias, param : torch.Size([384])
name : features.4.0.norm1_skip.weight, param : torch.Size([384])
name : features.4.0.norm1_skip.bias, param : torch.Size([384])
name : features.4.0.norm2_skip.weight, param : torch.Size([384])
name : features.4.0.norm2_skip.bias, param : torch.Size([384])
name : features.4.1.norm1.weight, param : torch.Size([384])
name : features.4.1.norm1.bias, param : torch.Size([384])
name : features.4.1.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.1.attn.relative_position_index, param : torch.Size([2401])
name : features.4.1.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.1.attn.qkv.bias, param : torch.Size([1152])
name : features.4.1.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.1.attn.proj.bias, param : torch.Size([384])
name : features.4.1.norm2.weight, param : torch.Size([384])
name : features.4.1.norm2.bias, param : torch.Size([384])
name : features.4.1.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.1.mlp.0.bias, param : torch.Size([1536])
name : features.4.1.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.1.mlp.3.bias, param : torch.Size([384])
name : features.4.1.norm1_skip.weight, param : torch.Size([384])
name : features.4.1.norm1_skip.bias, param : torch.Size([384])
name : features.4.1.norm2_skip.weight, param : torch.Size([384])
name : features.4.1.norm2_skip.bias, param : torch.Size([384])
name : features.4.2.norm1.weight, param : torch.Size([384])
name : features.4.2.norm1.bias, param : torch.Size([384])
name : features.4.2.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.2.attn.relative_position_index, param : torch.Size([2401])
name : features.4.2.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.2.attn.qkv.bias, param : torch.Size([1152])
name : features.4.2.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.2.attn.proj.bias, param : torch.Size([384])
name : features.4.2.norm2.weight, param : torch.Size([384])
name : features.4.2.norm2.bias, param : torch.Size([384])
name : features.4.2.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.2.mlp.0.bias, param : torch.Size([1536])
name : features.4.2.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.2.mlp.3.bias, param : torch.Size([384])
name : features.4.2.norm1_skip.weight, param : torch.Size([384])
name : features.4.2.norm1_skip.bias, param : torch.Size([384])
name : features.4.2.norm2_skip.weight, param : torch.Size([384])
name : features.4.2.norm2_skip.bias, param : torch.Size([384])
name : features.4.3.norm1.weight, param : torch.Size([384])
name : features.4.3.norm1.bias, param : torch.Size([384])
name : features.4.3.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.3.attn.relative_position_index, param : torch.Size([2401])
name : features.4.3.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.3.attn.qkv.bias, param : torch.Size([1152])
name : features.4.3.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.3.attn.proj.bias, param : torch.Size([384])
name : features.4.3.norm2.weight, param : torch.Size([384])
name : features.4.3.norm2.bias, param : torch.Size([384])
name : features.4.3.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.3.mlp.0.bias, param : torch.Size([1536])
name : features.4.3.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.3.mlp.3.bias, param : torch.Size([384])
name : features.4.4.norm1.weight, param : torch.Size([384])
name : features.4.4.norm1.bias, param : torch.Size([384])
name : features.4.4.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.4.attn.relative_position_index, param : torch.Size([2401])
name : features.4.4.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.4.attn.qkv.bias, param : torch.Size([1152])
name : features.4.4.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.4.attn.proj.bias, param : torch.Size([384])
name : features.4.4.norm2.weight, param : torch.Size([384])
name : features.4.4.norm2.bias, param : torch.Size([384])
name : features.4.4.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.4.mlp.0.bias, param : torch.Size([1536])
name : features.4.4.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.4.mlp.3.bias, param : torch.Size([384])
name : features.4.5.norm1.weight, param : torch.Size([384])
name : features.4.5.norm1.bias, param : torch.Size([384])
name : features.4.5.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.5.attn.relative_position_index, param : torch.Size([2401])
name : features.4.5.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.5.attn.qkv.bias, param : torch.Size([1152])
name : features.4.5.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.5.attn.proj.bias, param : torch.Size([384])
name : features.4.5.norm2.weight, param : torch.Size([384])
name : features.4.5.norm2.bias, param : torch.Size([384])
name : features.4.5.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.5.mlp.0.bias, param : torch.Size([1536])
name : features.4.5.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.5.mlp.3.bias, param : torch.Size([384])
name : features.5.reduction.weight, param : torch.Size([768, 1536])
name : features.5.norm.weight, param : torch.Size([1536])
name : features.5.norm.bias, param : torch.Size([1536])
name : features.5.norm_skip.weight, param : torch.Size([1536])
name : features.5.norm_skip.bias, param : torch.Size([1536])
name : features.6.0.norm1.weight, param : torch.Size([768])
name : features.6.0.norm1.bias, param : torch.Size([768])
name : features.6.0.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.0.attn.relative_position_index, param : torch.Size([2401])
name : features.6.0.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.0.attn.qkv.bias, param : torch.Size([2304])
name : features.6.0.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.0.attn.proj.bias, param : torch.Size([768])
name : features.6.0.norm2.weight, param : torch.Size([768])
name : features.6.0.norm2.bias, param : torch.Size([768])
name : features.6.0.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.0.mlp.0.bias, param : torch.Size([3072])
name : features.6.0.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.0.mlp.3.bias, param : torch.Size([768])
name : features.6.0.norm1_skip.weight, param : torch.Size([768])
name : features.6.0.norm1_skip.bias, param : torch.Size([768])
name : features.6.0.norm2_skip.weight, param : torch.Size([768])
name : features.6.0.norm2_skip.bias, param : torch.Size([768])
name : features.6.1.norm1.weight, param : torch.Size([768])
name : features.6.1.norm1.bias, param : torch.Size([768])
name : features.6.1.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.1.attn.relative_position_index, param : torch.Size([2401])
name : features.6.1.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.1.attn.qkv.bias, param : torch.Size([2304])
name : features.6.1.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.1.attn.proj.bias, param : torch.Size([768])
name : features.6.1.norm2.weight, param : torch.Size([768])
name : features.6.1.norm2.bias, param : torch.Size([768])
name : features.6.1.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.1.mlp.0.bias, param : torch.Size([3072])
name : features.6.1.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.1.mlp.3.bias, param : torch.Size([768])
name : norm.weight, param : torch.Size([768])
name : norm.bias, param : torch.Size([768])
Load state_dict from /home/hslee/Desktop/RetinaNet-ADN/02_AdaptiveDepthNetwork/pretrained/checkpoint_swin-t-epoch297.pth
create: HybridEncoder
backbone : None
create: RTDETRTransformer
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): SwinTransformer(
      (pre): Sequential(
        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): Permute()
        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (features): SkippableSequentialStages(
        (0): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.009090909090909092, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PatchMerging(
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (2): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.02727272727272728, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.045454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.06363636363636364, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.08181818181818182, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (6): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.1, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
create: SetCriterion
create: HungarianMatcher
create: HungarianMatcher
create: RTDETRPostProcessor
start creating model... (in yaml_config.py)
create: ema
create: ModelEMA
start creating model... (in yaml_config.py)
create: optimizer
create: AdamW
create: lr_scheduler
create: MultiStepLR
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
create: train_dataloader
create: DataLoader
create: CocoDetection
create: CocoDetection
create: Compose
create: Compose
loading annotations into memory...
Done (t=11.33s)
creating index...
index created!
create: val_dataloader
create: DataLoader
create: CocoDetection
create: CocoDetection
create: Compose
create: Compose
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
module.backbone.features.0.0.norm1.weight 96
module.backbone.features.0.0.norm1.bias 96
module.backbone.features.0.0.attn.relative_position_bias_table 507
module.backbone.features.0.0.attn.qkv.weight 27648
module.backbone.features.0.0.attn.qkv.bias 288
module.backbone.features.0.0.attn.proj.weight 9216
module.backbone.features.0.0.attn.proj.bias 96
module.backbone.features.0.0.norm2.weight 96
module.backbone.features.0.0.norm2.bias 96
module.backbone.features.0.0.mlp.0.weight 36864
module.backbone.features.0.0.mlp.0.bias 384
module.backbone.features.0.0.mlp.3.weight 36864
module.backbone.features.0.0.mlp.3.bias 96
module.backbone.features.0.0.norm1_skip.weight 96
module.backbone.features.0.0.norm1_skip.bias 96
module.backbone.features.0.0.norm2_skip.weight 96
module.backbone.features.0.0.norm2_skip.bias 96
module.backbone.features.0.1.norm1.weight 96
module.backbone.features.0.1.norm1.bias 96
module.backbone.features.0.1.attn.relative_position_bias_table 507
module.backbone.features.0.1.attn.qkv.weight 27648
module.backbone.features.0.1.attn.qkv.bias 288
module.backbone.features.0.1.attn.proj.weight 9216
module.backbone.features.0.1.attn.proj.bias 96
module.backbone.features.0.1.norm2.weight 96
module.backbone.features.0.1.norm2.bias 96
module.backbone.features.0.1.mlp.0.weight 36864
module.backbone.features.0.1.mlp.0.bias 384
module.backbone.features.0.1.mlp.3.weight 36864
module.backbone.features.0.1.mlp.3.bias 96
module.backbone.features.1.reduction.weight 73728
module.backbone.features.1.norm.weight 384
module.backbone.features.1.norm.bias 384
module.backbone.features.1.norm_skip.weight 384
module.backbone.features.1.norm_skip.bias 384
module.backbone.features.2.0.norm1.weight 192
module.backbone.features.2.0.norm1.bias 192
module.backbone.features.2.0.attn.relative_position_bias_table 1014
module.backbone.features.2.0.attn.qkv.weight 110592
module.backbone.features.2.0.attn.qkv.bias 576
module.backbone.features.2.0.attn.proj.weight 36864
module.backbone.features.2.0.attn.proj.bias 192
module.backbone.features.2.0.norm2.weight 192
module.backbone.features.2.0.norm2.bias 192
module.backbone.features.2.0.mlp.0.weight 147456
module.backbone.features.2.0.mlp.0.bias 768
module.backbone.features.2.0.mlp.3.weight 147456
module.backbone.features.2.0.mlp.3.bias 192
module.backbone.features.2.0.norm1_skip.weight 192
module.backbone.features.2.0.norm1_skip.bias 192
module.backbone.features.2.0.norm2_skip.weight 192
module.backbone.features.2.0.norm2_skip.bias 192
module.backbone.features.2.1.norm1.weight 192
module.backbone.features.2.1.norm1.bias 192
module.backbone.features.2.1.attn.relative_position_bias_table 1014
module.backbone.features.2.1.attn.qkv.weight 110592
module.backbone.features.2.1.attn.qkv.bias 576
module.backbone.features.2.1.attn.proj.weight 36864
module.backbone.features.2.1.attn.proj.bias 192
module.backbone.features.2.1.norm2.weight 192
module.backbone.features.2.1.norm2.bias 192
module.backbone.features.2.1.mlp.0.weight 147456
module.backbone.features.2.1.mlp.0.bias 768
module.backbone.features.2.1.mlp.3.weight 147456
module.backbone.features.2.1.mlp.3.bias 192
module.backbone.features.3.reduction.weight 294912
module.backbone.features.3.norm.weight 768
module.backbone.features.3.norm.bias 768
module.backbone.features.3.norm_skip.weight 768
module.backbone.features.3.norm_skip.bias 768
module.backbone.features.4.0.norm1.weight 384
module.backbone.features.4.0.norm1.bias 384
module.backbone.features.4.0.attn.relative_position_bias_table 2028
module.backbone.features.4.0.attn.qkv.weight 442368
module.backbone.features.4.0.attn.qkv.bias 1152
module.backbone.features.4.0.attn.proj.weight 147456
module.backbone.features.4.0.attn.proj.bias 384
module.backbone.features.4.0.norm2.weight 384
module.backbone.features.4.0.norm2.bias 384
module.backbone.features.4.0.mlp.0.weight 589824
module.backbone.features.4.0.mlp.0.bias 1536
module.backbone.features.4.0.mlp.3.weight 589824
module.backbone.features.4.0.mlp.3.bias 384
module.backbone.features.4.0.norm1_skip.weight 384
module.backbone.features.4.0.norm1_skip.bias 384
module.backbone.features.4.0.norm2_skip.weight 384
module.backbone.features.4.0.norm2_skip.bias 384
module.backbone.features.4.1.norm1.weight 384
module.backbone.features.4.1.norm1.bias 384
module.backbone.features.4.1.attn.relative_position_bias_table 2028
module.backbone.features.4.1.attn.qkv.weight 442368
module.backbone.features.4.1.attn.qkv.bias 1152
module.backbone.features.4.1.attn.proj.weight 147456
module.backbone.features.4.1.attn.proj.bias 384
module.backbone.features.4.1.norm2.weight 384
module.backbone.features.4.1.norm2.bias 384
module.backbone.features.4.1.mlp.0.weight 589824
module.backbone.features.4.1.mlp.0.bias 1536
module.backbone.features.4.1.mlp.3.weight 589824
module.backbone.features.4.1.mlp.3.bias 384
module.backbone.features.4.1.norm1_skip.weight 384
module.backbone.features.4.1.norm1_skip.bias 384
module.backbone.features.4.1.norm2_skip.weight 384
module.backbone.features.4.1.norm2_skip.bias 384
module.backbone.features.4.2.norm1.weight 384
module.backbone.features.4.2.norm1.bias 384
module.backbone.features.4.2.attn.relative_position_bias_table 2028
module.backbone.features.4.2.attn.qkv.weight 442368
module.backbone.features.4.2.attn.qkv.bias 1152
module.backbone.features.4.2.attn.proj.weight 147456
module.backbone.features.4.2.attn.proj.bias 384
module.backbone.features.4.2.norm2.weight 384
module.backbone.features.4.2.norm2.bias 384
module.backbone.features.4.2.mlp.0.weight 589824
module.backbone.features.4.2.mlp.0.bias 1536
module.backbone.features.4.2.mlp.3.weight 589824
module.backbone.features.4.2.mlp.3.bias 384
module.backbone.features.4.2.norm1_skip.weight 384
module.backbone.features.4.2.norm1_skip.bias 384
module.backbone.features.4.2.norm2_skip.weight 384
module.backbone.features.4.2.norm2_skip.bias 384
module.backbone.features.4.3.norm1.weight 384
module.backbone.features.4.3.norm1.bias 384
module.backbone.features.4.3.attn.relative_position_bias_table 2028
module.backbone.features.4.3.attn.qkv.weight 442368
module.backbone.features.4.3.attn.qkv.bias 1152
module.backbone.features.4.3.attn.proj.weight 147456
module.backbone.features.4.3.attn.proj.bias 384
module.backbone.features.4.3.norm2.weight 384
module.backbone.features.4.3.norm2.bias 384
module.backbone.features.4.3.mlp.0.weight 589824
module.backbone.features.4.3.mlp.0.bias 1536
module.backbone.features.4.3.mlp.3.weight 589824
module.backbone.features.4.3.mlp.3.bias 384
module.backbone.features.4.4.norm1.weight 384
module.backbone.features.4.4.norm1.bias 384
module.backbone.features.4.4.attn.relative_position_bias_table 2028
module.backbone.features.4.4.attn.qkv.weight 442368
module.backbone.features.4.4.attn.qkv.bias 1152
module.backbone.features.4.4.attn.proj.weight 147456
module.backbone.features.4.4.attn.proj.bias 384
module.backbone.features.4.4.norm2.weight 384
module.backbone.features.4.4.norm2.bias 384
module.backbone.features.4.4.mlp.0.weight 589824
module.backbone.features.4.4.mlp.0.bias 1536
module.backbone.features.4.4.mlp.3.weight 589824
module.backbone.features.4.4.mlp.3.bias 384
module.backbone.features.4.5.norm1.weight 384
module.backbone.features.4.5.norm1.bias 384
module.backbone.features.4.5.attn.relative_position_bias_table 2028
module.backbone.features.4.5.attn.qkv.weight 442368
module.backbone.features.4.5.attn.qkv.bias 1152
module.backbone.features.4.5.attn.proj.weight 147456
module.backbone.features.4.5.attn.proj.bias 384
module.backbone.features.4.5.norm2.weight 384
module.backbone.features.4.5.norm2.bias 384
module.backbone.features.4.5.mlp.0.weight 589824
module.backbone.features.4.5.mlp.0.bias 1536
module.backbone.features.4.5.mlp.3.weight 589824
module.backbone.features.4.5.mlp.3.bias 384
module.backbone.features.5.reduction.weight 1179648
module.backbone.features.5.norm.weight 1536
module.backbone.features.5.norm.bias 1536
module.backbone.features.5.norm_skip.weight 1536
module.backbone.features.5.norm_skip.bias 1536
module.backbone.features.6.0.norm1.weight 768
module.backbone.features.6.0.norm1.bias 768
module.backbone.features.6.0.attn.relative_position_bias_table 4056
module.backbone.features.6.0.attn.qkv.weight 1769472
module.backbone.features.6.0.attn.qkv.bias 2304
module.backbone.features.6.0.attn.proj.weight 589824
module.backbone.features.6.0.attn.proj.bias 768
module.backbone.features.6.0.norm2.weight 768
module.backbone.features.6.0.norm2.bias 768
module.backbone.features.6.0.mlp.0.weight 2359296
module.backbone.features.6.0.mlp.0.bias 3072
module.backbone.features.6.0.mlp.3.weight 2359296
module.backbone.features.6.0.mlp.3.bias 768
module.backbone.features.6.0.norm1_skip.weight 768
module.backbone.features.6.0.norm1_skip.bias 768
module.backbone.features.6.0.norm2_skip.weight 768
module.backbone.features.6.0.norm2_skip.bias 768
module.backbone.features.6.1.norm1.weight 768
module.backbone.features.6.1.norm1.bias 768
module.backbone.features.6.1.attn.relative_position_bias_table 4056
module.backbone.features.6.1.attn.qkv.weight 1769472
module.backbone.features.6.1.attn.qkv.bias 2304
module.backbone.features.6.1.attn.proj.weight 589824
module.backbone.features.6.1.attn.proj.bias 768
module.backbone.features.6.1.norm2.weight 768
module.backbone.features.6.1.norm2.bias 768
module.backbone.features.6.1.mlp.0.weight 2359296
module.backbone.features.6.1.mlp.0.bias 3072
module.backbone.features.6.1.mlp.3.weight 2359296
module.backbone.features.6.1.mlp.3.bias 768
module.backbone.norm.weight 768
module.backbone.norm.bias 768
module.decoder.input_proj.0.conv.weight 65536
module.decoder.input_proj.0.norm.weight 256
module.decoder.input_proj.0.norm.bias 256
module.decoder.input_proj.1.conv.weight 65536
module.decoder.input_proj.1.norm.weight 256
module.decoder.input_proj.1.norm.bias 256
module.decoder.input_proj.2.conv.weight 65536
module.decoder.input_proj.2.norm.weight 256
module.decoder.input_proj.2.norm.bias 256
module.decoder.decoder.layers.0.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.0.self_attn.in_proj_bias 768
module.decoder.decoder.layers.0.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.0.self_attn.out_proj.bias 256
module.decoder.decoder.layers.0.norm1.weight 256
module.decoder.decoder.layers.0.norm1.bias 256
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.0.norm2.weight 256
module.decoder.decoder.layers.0.norm2.bias 256
module.decoder.decoder.layers.0.linear1.weight 262144
module.decoder.decoder.layers.0.linear1.bias 1024
module.decoder.decoder.layers.0.linear2.weight 262144
module.decoder.decoder.layers.0.linear2.bias 256
module.decoder.decoder.layers.0.norm3.weight 256
module.decoder.decoder.layers.0.norm3.bias 256
module.decoder.decoder.layers.1.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.1.self_attn.in_proj_bias 768
module.decoder.decoder.layers.1.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.1.self_attn.out_proj.bias 256
module.decoder.decoder.layers.1.norm1.weight 256
module.decoder.decoder.layers.1.norm1.bias 256
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.1.norm2.weight 256
module.decoder.decoder.layers.1.norm2.bias 256
module.decoder.decoder.layers.1.linear1.weight 262144
module.decoder.decoder.layers.1.linear1.bias 1024
module.decoder.decoder.layers.1.linear2.weight 262144
module.decoder.decoder.layers.1.linear2.bias 256
module.decoder.decoder.layers.1.norm3.weight 256
module.decoder.decoder.layers.1.norm3.bias 256
module.decoder.decoder.layers.2.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.2.self_attn.in_proj_bias 768
module.decoder.decoder.layers.2.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.2.self_attn.out_proj.bias 256
module.decoder.decoder.layers.2.norm1.weight 256
module.decoder.decoder.layers.2.norm1.bias 256
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.2.norm2.weight 256
module.decoder.decoder.layers.2.norm2.bias 256
module.decoder.decoder.layers.2.linear1.weight 262144
module.decoder.decoder.layers.2.linear1.bias 1024
module.decoder.decoder.layers.2.linear2.weight 262144
module.decoder.decoder.layers.2.linear2.bias 256
module.decoder.decoder.layers.2.norm3.weight 256
module.decoder.decoder.layers.2.norm3.bias 256
module.decoder.decoder.layers.3.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.3.self_attn.in_proj_bias 768
module.decoder.decoder.layers.3.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.3.self_attn.out_proj.bias 256
module.decoder.decoder.layers.3.norm1.weight 256
module.decoder.decoder.layers.3.norm1.bias 256
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.3.norm2.weight 256
module.decoder.decoder.layers.3.norm2.bias 256
module.decoder.decoder.layers.3.linear1.weight 262144
module.decoder.decoder.layers.3.linear1.bias 1024
module.decoder.decoder.layers.3.linear2.weight 262144
module.decoder.decoder.layers.3.linear2.bias 256
module.decoder.decoder.layers.3.norm3.weight 256
module.decoder.decoder.layers.3.norm3.bias 256
module.decoder.decoder.layers.4.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.4.self_attn.in_proj_bias 768
module.decoder.decoder.layers.4.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.4.self_attn.out_proj.bias 256
module.decoder.decoder.layers.4.norm1.weight 256
module.decoder.decoder.layers.4.norm1.bias 256
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.4.norm2.weight 256
module.decoder.decoder.layers.4.norm2.bias 256
module.decoder.decoder.layers.4.linear1.weight 262144
module.decoder.decoder.layers.4.linear1.bias 1024
module.decoder.decoder.layers.4.linear2.weight 262144
module.decoder.decoder.layers.4.linear2.bias 256
module.decoder.decoder.layers.4.norm3.weight 256
module.decoder.decoder.layers.4.norm3.bias 256
module.decoder.decoder.layers.5.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.5.self_attn.in_proj_bias 768
module.decoder.decoder.layers.5.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.5.self_attn.out_proj.bias 256
module.decoder.decoder.layers.5.norm1.weight 256
module.decoder.decoder.layers.5.norm1.bias 256
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.5.norm2.weight 256
module.decoder.decoder.layers.5.norm2.bias 256
module.decoder.decoder.layers.5.linear1.weight 262144
module.decoder.decoder.layers.5.linear1.bias 1024
module.decoder.decoder.layers.5.linear2.weight 262144
module.decoder.decoder.layers.5.linear2.bias 256
module.decoder.decoder.layers.5.norm3.weight 256
module.decoder.decoder.layers.5.norm3.bias 256
module.decoder.denoising_class_embed.weight 20736
module.decoder.query_pos_head.layers.0.weight 2048
module.decoder.query_pos_head.layers.0.bias 512
module.decoder.query_pos_head.layers.1.weight 131072
module.decoder.query_pos_head.layers.1.bias 256
module.decoder.enc_output.0.weight 65536
module.decoder.enc_output.0.bias 256
module.decoder.enc_output.1.weight 256
module.decoder.enc_output.1.bias 256
module.decoder.enc_score_head.weight 20480
module.decoder.enc_score_head.bias 80
module.decoder.enc_bbox_head.layers.0.weight 65536
module.decoder.enc_bbox_head.layers.0.bias 256
module.decoder.enc_bbox_head.layers.1.weight 65536
module.decoder.enc_bbox_head.layers.1.bias 256
module.decoder.enc_bbox_head.layers.2.weight 1024
module.decoder.enc_bbox_head.layers.2.bias 4
module.decoder.dec_score_head.0.weight 20480
module.decoder.dec_score_head.0.bias 80
module.decoder.dec_score_head.1.weight 20480
module.decoder.dec_score_head.1.bias 80
module.decoder.dec_score_head.2.weight 20480
module.decoder.dec_score_head.2.bias 80
module.decoder.dec_score_head.3.weight 20480
module.decoder.dec_score_head.3.bias 80
module.decoder.dec_score_head.4.weight 20480
module.decoder.dec_score_head.4.bias 80
module.decoder.dec_score_head.5.weight 20480
module.decoder.dec_score_head.5.bias 80
module.decoder.dec_bbox_head.0.layers.0.weight 65536
module.decoder.dec_bbox_head.0.layers.0.bias 256
module.decoder.dec_bbox_head.0.layers.1.weight 65536
module.decoder.dec_bbox_head.0.layers.1.bias 256
module.decoder.dec_bbox_head.0.layers.2.weight 1024
module.decoder.dec_bbox_head.0.layers.2.bias 4
module.decoder.dec_bbox_head.1.layers.0.weight 65536
module.decoder.dec_bbox_head.1.layers.0.bias 256
module.decoder.dec_bbox_head.1.layers.1.weight 65536
module.decoder.dec_bbox_head.1.layers.1.bias 256
module.decoder.dec_bbox_head.1.layers.2.weight 1024
module.decoder.dec_bbox_head.1.layers.2.bias 4
module.decoder.dec_bbox_head.2.layers.0.weight 65536
module.decoder.dec_bbox_head.2.layers.0.bias 256
module.decoder.dec_bbox_head.2.layers.1.weight 65536
module.decoder.dec_bbox_head.2.layers.1.bias 256
module.decoder.dec_bbox_head.2.layers.2.weight 1024
module.decoder.dec_bbox_head.2.layers.2.bias 4
module.decoder.dec_bbox_head.3.layers.0.weight 65536
module.decoder.dec_bbox_head.3.layers.0.bias 256
module.decoder.dec_bbox_head.3.layers.1.weight 65536
module.decoder.dec_bbox_head.3.layers.1.bias 256
module.decoder.dec_bbox_head.3.layers.2.weight 1024
module.decoder.dec_bbox_head.3.layers.2.bias 4
module.decoder.dec_bbox_head.4.layers.0.weight 65536
module.decoder.dec_bbox_head.4.layers.0.bias 256
module.decoder.dec_bbox_head.4.layers.1.weight 65536
module.decoder.dec_bbox_head.4.layers.1.bias 256
module.decoder.dec_bbox_head.4.layers.2.weight 1024
module.decoder.dec_bbox_head.4.layers.2.bias 4
module.decoder.dec_bbox_head.5.layers.0.weight 65536
module.decoder.dec_bbox_head.5.layers.0.bias 256
module.decoder.dec_bbox_head.5.layers.1.weight 65536
module.decoder.dec_bbox_head.5.layers.1.bias 256
module.decoder.dec_bbox_head.5.layers.2.weight 1024
module.decoder.dec_bbox_head.5.layers.2.bias 4
module.encoder.input_proj.0.0.weight 131072
module.encoder.input_proj.0.1.weight 256
module.encoder.input_proj.0.1.bias 256
module.encoder.input_proj.1.0.weight 262144
module.encoder.input_proj.1.1.weight 256
module.encoder.input_proj.1.1.bias 256
module.encoder.input_proj.2.0.weight 524288
module.encoder.input_proj.2.1.weight 256
module.encoder.input_proj.2.1.bias 256
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
module.encoder.encoder.0.layers.0.linear1.weight 262144
module.encoder.encoder.0.layers.0.linear1.bias 1024
module.encoder.encoder.0.layers.0.linear2.weight 262144
module.encoder.encoder.0.layers.0.linear2.bias 256
module.encoder.encoder.0.layers.0.norm1.weight 256
module.encoder.encoder.0.layers.0.norm1.bias 256
module.encoder.encoder.0.layers.0.norm2.weight 256
module.encoder.encoder.0.layers.0.norm2.bias 256
module.encoder.lateral_convs.0.conv.weight 65536
module.encoder.lateral_convs.0.norm.weight 256
module.encoder.lateral_convs.0.norm.bias 256
module.encoder.lateral_convs.1.conv.weight 65536
module.encoder.lateral_convs.1.norm.weight 256
module.encoder.lateral_convs.1.norm.bias 256
module.encoder.fpn_blocks.0.conv1.conv.weight 131072
module.encoder.fpn_blocks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.conv2.conv.weight 131072
module.encoder.fpn_blocks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.fpn_blocks.1.conv1.conv.weight 131072
module.encoder.fpn_blocks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.conv2.conv.weight 131072
module.encoder.fpn_blocks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
module.encoder.downsample_convs.0.conv.weight 589824
module.encoder.downsample_convs.0.norm.weight 256
module.encoder.downsample_convs.0.norm.bias 256
module.encoder.downsample_convs.1.conv.weight 589824
module.encoder.downsample_convs.1.norm.weight 256
module.encoder.downsample_convs.1.norm.bias 256
module.encoder.pan_blocks.0.conv1.conv.weight 131072
module.encoder.pan_blocks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.conv2.conv.weight 131072
module.encoder.pan_blocks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.pan_blocks.1.conv1.conv.weight 131072
module.encoder.pan_blocks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.conv2.conv.weight 131072
module.encoder.pan_blocks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 46946022
super_config : [False, False, False, False]
base_config : [True, True, True, True]
Traceback (most recent call last):
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/train.py", line 63, in <module>
    main(args)
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/train.py", line 49, in main
    solver.fit() # training
    ^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/solver/det_solver.py", line 50, in fit
    train_stats = train_one_epoch(
                  ^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/solver/det_engine.py", line 68, in train_one_epoch
    outputs_super = model(samples, targets, skip=base_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/rtdetr.py", line 55, in forward
    x = self.encoder(x)        
        ^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/hybrid_encoder.py", line 357, in forward
    proj_feats = [self.input_proj_swinT[i](feat) for i, feat in enumerate(feats)]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/hybrid_encoder.py", line 357, in <listcomp>
    proj_feats = [self.input_proj_swinT[i](feat) for i, feat in enumerate(feats)]
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'HybridEncoder' object has no attribute 'input_proj_swinT'
Traceback (most recent call last):
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/train.py", line 63, in <module>
    main(args)
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/train.py", line 49, in main
    solver.fit() # training
    ^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/solver/det_solver.py", line 50, in fit
    train_stats = train_one_epoch(
                  ^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/solver/det_engine.py", line 68, in train_one_epoch
    outputs_super = model(samples, targets, skip=base_config)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/rtdetr.py", line 55, in forward
    x = self.encoder(x)        
        ^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/hybrid_encoder.py", line 357, in forward
    proj_feats = [self.input_proj_swinT[i](feat) for i, feat in enumerate(feats)]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/RT-DETR-ADN/06_RT-DETR-ADN/tools/../src/zoo/rtdetr/hybrid_encoder.py", line 357, in <listcomp>
    proj_feats = [self.input_proj_swinT[i](feat) for i, feat in enumerate(feats)]
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'HybridEncoder' object has no attribute 'input_proj_swinT'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3274583) of binary: /home/hslee/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-25_22:43:34
  host      : ada
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3274584)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-25_22:43:34
  host      : ada
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3274583)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
