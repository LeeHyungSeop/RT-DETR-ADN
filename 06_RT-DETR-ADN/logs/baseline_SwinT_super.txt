WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
start creating model... (in yaml_config.py)
name : pre.0.weight, param : torch.Size([96, 3, 4, 4])
name : pre.0.bias, param : torch.Size([96])
name : pre.2.weight, param : torch.Size([96])
name : pre.2.bias, param : torch.Size([96])
name : features.0.0.norm1.weight, param : torch.Size([96])
name : features.0.0.norm1.bias, param : torch.Size([96])
name : features.0.0.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.0.attn.relative_position_index, param : torch.Size([2401])
name : features.0.0.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.0.attn.qkv.bias, param : torch.Size([288])
name : features.0.0.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.0.attn.proj.bias, param : torch.Size([96])
name : features.0.0.norm2.weight, param : torch.Size([96])
name : features.0.0.norm2.bias, param : torch.Size([96])
name : features.0.0.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.0.mlp.0.bias, param : torch.Size([384])
name : features.0.0.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.0.mlp.3.bias, param : torch.Size([96])
name : features.0.0.norm1_skip.weight, param : torch.Size([96])
name : features.0.0.norm1_skip.bias, param : torch.Size([96])
name : features.0.0.norm2_skip.weight, param : torch.Size([96])
name : features.0.0.norm2_skip.bias, param : torch.Size([96])
name : features.0.1.norm1.weight, param : torch.Size([96])
name : features.0.1.norm1.bias, param : torch.Size([96])
name : features.0.1.attn.relative_position_bias_table, param : torch.Size([169, 3])
name : features.0.1.attn.relative_position_index, param : torch.Size([2401])
name : features.0.1.attn.qkv.weight, param : torch.Size([288, 96])
name : features.0.1.attn.qkv.bias, param : torch.Size([288])
name : features.0.1.attn.proj.weight, param : torch.Size([96, 96])
name : features.0.1.attn.proj.bias, param : torch.Size([96])
name : features.0.1.norm2.weight, param : torch.Size([96])
name : features.0.1.norm2.bias, param : torch.Size([96])
name : features.0.1.mlp.0.weight, param : torch.Size([384, 96])
name : features.0.1.mlp.0.bias, param : torch.Size([384])
name : features.0.1.mlp.3.weight, param : torch.Size([96, 384])
name : features.0.1.mlp.3.bias, param : torch.Size([96])
name : features.1.reduction.weight, param : torch.Size([192, 384])
name : features.1.norm.weight, param : torch.Size([384])
name : features.1.norm.bias, param : torch.Size([384])
name : features.1.norm_skip.weight, param : torch.Size([384])
name : features.1.norm_skip.bias, param : torch.Size([384])
name : features.2.0.norm1.weight, param : torch.Size([192])
name : features.2.0.norm1.bias, param : torch.Size([192])
name : features.2.0.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.0.attn.relative_position_index, param : torch.Size([2401])
name : features.2.0.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.0.attn.qkv.bias, param : torch.Size([576])
name : features.2.0.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.0.attn.proj.bias, param : torch.Size([192])
name : features.2.0.norm2.weight, param : torch.Size([192])
name : features.2.0.norm2.bias, param : torch.Size([192])
name : features.2.0.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.0.mlp.0.bias, param : torch.Size([768])
name : features.2.0.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.0.mlp.3.bias, param : torch.Size([192])
name : features.2.0.norm1_skip.weight, param : torch.Size([192])
name : features.2.0.norm1_skip.bias, param : torch.Size([192])
name : features.2.0.norm2_skip.weight, param : torch.Size([192])
name : features.2.0.norm2_skip.bias, param : torch.Size([192])
name : features.2.1.norm1.weight, param : torch.Size([192])
name : features.2.1.norm1.bias, param : torch.Size([192])
name : features.2.1.attn.relative_position_bias_table, param : torch.Size([169, 6])
name : features.2.1.attn.relative_position_index, param : torch.Size([2401])
name : features.2.1.attn.qkv.weight, param : torch.Size([576, 192])
name : features.2.1.attn.qkv.bias, param : torch.Size([576])
name : features.2.1.attn.proj.weight, param : torch.Size([192, 192])
name : features.2.1.attn.proj.bias, param : torch.Size([192])
name : features.2.1.norm2.weight, param : torch.Size([192])
name : features.2.1.norm2.bias, param : torch.Size([192])
name : features.2.1.mlp.0.weight, param : torch.Size([768, 192])
name : features.2.1.mlp.0.bias, param : torch.Size([768])
name : features.2.1.mlp.3.weight, param : torch.Size([192, 768])
name : features.2.1.mlp.3.bias, param : torch.Size([192])
name : features.3.reduction.weight, param : torch.Size([384, 768])
name : features.3.norm.weight, param : torch.Size([768])
name : features.3.norm.bias, param : torch.Size([768])
name : features.3.norm_skip.weight, param : torch.Size([768])
name : features.3.norm_skip.bias, param : torch.Size([768])
name : features.4.0.norm1.weight, param : torch.Size([384])
name : features.4.0.norm1.bias, param : torch.Size([384])
name : features.4.0.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.0.attn.relative_position_index, param : torch.Size([2401])
name : features.4.0.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.0.attn.qkv.bias, param : torch.Size([1152])
name : features.4.0.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.0.attn.proj.bias, param : torch.Size([384])
name : features.4.0.norm2.weight, param : torch.Size([384])
name : features.4.0.norm2.bias, param : torch.Size([384])
name : features.4.0.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.0.mlp.0.bias, param : torch.Size([1536])
name : features.4.0.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.0.mlp.3.bias, param : torch.Size([384])
name : features.4.0.norm1_skip.weight, param : torch.Size([384])
name : features.4.0.norm1_skip.bias, param : torch.Size([384])
name : features.4.0.norm2_skip.weight, param : torch.Size([384])
name : features.4.0.norm2_skip.bias, param : torch.Size([384])
name : features.4.1.norm1.weight, param : torch.Size([384])
name : features.4.1.norm1.bias, param : torch.Size([384])
name : features.4.1.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.1.attn.relative_position_index, param : torch.Size([2401])
name : features.4.1.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.1.attn.qkv.bias, param : torch.Size([1152])
name : features.4.1.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.1.attn.proj.bias, param : torch.Size([384])
name : features.4.1.norm2.weight, param : torch.Size([384])
name : features.4.1.norm2.bias, param : torch.Size([384])
name : features.4.1.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.1.mlp.0.bias, param : torch.Size([1536])
name : features.4.1.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.1.mlp.3.bias, param : torch.Size([384])
name : features.4.1.norm1_skip.weight, param : torch.Size([384])
name : features.4.1.norm1_skip.bias, param : torch.Size([384])
name : features.4.1.norm2_skip.weight, param : torch.Size([384])
name : features.4.1.norm2_skip.bias, param : torch.Size([384])
name : features.4.2.norm1.weight, param : torch.Size([384])
name : features.4.2.norm1.bias, param : torch.Size([384])
name : features.4.2.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.2.attn.relative_position_index, param : torch.Size([2401])
name : features.4.2.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.2.attn.qkv.bias, param : torch.Size([1152])
name : features.4.2.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.2.attn.proj.bias, param : torch.Size([384])
name : features.4.2.norm2.weight, param : torch.Size([384])
name : features.4.2.norm2.bias, param : torch.Size([384])
name : features.4.2.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.2.mlp.0.bias, param : torch.Size([1536])
name : features.4.2.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.2.mlp.3.bias, param : torch.Size([384])
name : features.4.2.norm1_skip.weight, param : torch.Size([384])
name : features.4.2.norm1_skip.bias, param : torch.Size([384])
name : features.4.2.norm2_skip.weight, param : torch.Size([384])
name : features.4.2.norm2_skip.bias, param : torch.Size([384])
name : features.4.3.norm1.weight, param : torch.Size([384])
name : features.4.3.norm1.bias, param : torch.Size([384])
name : features.4.3.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.3.attn.relative_position_index, param : torch.Size([2401])
name : features.4.3.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.3.attn.qkv.bias, param : torch.Size([1152])
name : features.4.3.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.3.attn.proj.bias, param : torch.Size([384])
name : features.4.3.norm2.weight, param : torch.Size([384])
name : features.4.3.norm2.bias, param : torch.Size([384])
name : features.4.3.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.3.mlp.0.bias, param : torch.Size([1536])
name : features.4.3.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.3.mlp.3.bias, param : torch.Size([384])
name : features.4.4.norm1.weight, param : torch.Size([384])
name : features.4.4.norm1.bias, param : torch.Size([384])
name : features.4.4.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.4.attn.relative_position_index, param : torch.Size([2401])
name : features.4.4.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.4.attn.qkv.bias, param : torch.Size([1152])
name : features.4.4.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.4.attn.proj.bias, param : torch.Size([384])
name : features.4.4.norm2.weight, param : torch.Size([384])
name : features.4.4.norm2.bias, param : torch.Size([384])
name : features.4.4.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.4.mlp.0.bias, param : torch.Size([1536])
name : features.4.4.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.4.mlp.3.bias, param : torch.Size([384])
name : features.4.5.norm1.weight, param : torch.Size([384])
name : features.4.5.norm1.bias, param : torch.Size([384])
name : features.4.5.attn.relative_position_bias_table, param : torch.Size([169, 12])
name : features.4.5.attn.relative_position_index, param : torch.Size([2401])
name : features.4.5.attn.qkv.weight, param : torch.Size([1152, 384])
name : features.4.5.attn.qkv.bias, param : torch.Size([1152])
name : features.4.5.attn.proj.weight, param : torch.Size([384, 384])
name : features.4.5.attn.proj.bias, param : torch.Size([384])
name : features.4.5.norm2.weight, param : torch.Size([384])
name : features.4.5.norm2.bias, param : torch.Size([384])
name : features.4.5.mlp.0.weight, param : torch.Size([1536, 384])
name : features.4.5.mlp.0.bias, param : torch.Size([1536])
name : features.4.5.mlp.3.weight, param : torch.Size([384, 1536])
name : features.4.5.mlp.3.bias, param : torch.Size([384])
name : features.5.reduction.weight, param : torch.Size([768, 1536])
name : features.5.norm.weight, param : torch.Size([1536])
name : features.5.norm.bias, param : torch.Size([1536])
name : features.5.norm_skip.weight, param : torch.Size([1536])
name : features.5.norm_skip.bias, param : torch.Size([1536])
name : features.6.0.norm1.weight, param : torch.Size([768])
name : features.6.0.norm1.bias, param : torch.Size([768])
name : features.6.0.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.0.attn.relative_position_index, param : torch.Size([2401])
name : features.6.0.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.0.attn.qkv.bias, param : torch.Size([2304])
name : features.6.0.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.0.attn.proj.bias, param : torch.Size([768])
name : features.6.0.norm2.weight, param : torch.Size([768])
name : features.6.0.norm2.bias, param : torch.Size([768])
name : features.6.0.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.0.mlp.0.bias, param : torch.Size([3072])
name : features.6.0.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.0.mlp.3.bias, param : torch.Size([768])
name : features.6.0.norm1_skip.weight, param : torch.Size([768])
name : features.6.0.norm1_skip.bias, param : torch.Size([768])
name : features.6.0.norm2_skip.weight, param : torch.Size([768])
name : features.6.0.norm2_skip.bias, param : torch.Size([768])
name : features.6.1.norm1.weight, param : torch.Size([768])
name : features.6.1.norm1.bias, param : torch.Size([768])
name : features.6.1.attn.relative_position_bias_table, param : torch.Size([169, 24])
name : features.6.1.attn.relative_position_index, param : torch.Size([2401])
name : features.6.1.attn.qkv.weight, param : torch.Size([2304, 768])
name : features.6.1.attn.qkv.bias, param : torch.Size([2304])
name : features.6.1.attn.proj.weight, param : torch.Size([768, 768])
name : features.6.1.attn.proj.bias, param : torch.Size([768])
name : features.6.1.norm2.weight, param : torch.Size([768])
name : features.6.1.norm2.bias, param : torch.Size([768])
name : features.6.1.mlp.0.weight, param : torch.Size([3072, 768])
name : features.6.1.mlp.0.bias, param : torch.Size([3072])
name : features.6.1.mlp.3.weight, param : torch.Size([768, 3072])
name : features.6.1.mlp.3.bias, param : torch.Size([768])
name : norm.weight, param : torch.Size([768])
name : norm.bias, param : torch.Size([768])
Load state_dict from /home/hslee/Desktop/RetinaNet-ADN/02_AdaptiveDepthNetwork/pretrained/checkpoint_swin-t-epoch297.pth
self.model (in solver.py): 
DistributedDataParallel(
  (module): RTDETR(
    (backbone): SwinTransformer(
      (pre): Sequential(
        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (1): Permute()
        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (features): SkippableSequentialStages(
        (0): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.009090909090909092, mode=row)
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=96, out_features=384, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=384, out_features=96, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PatchMerging(
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (2): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.02727272727272728, mode=row)
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=192, out_features=768, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=768, out_features=192, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.045454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.06363636363636364, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.08181818181818182, mode=row)
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=384, out_features=1536, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1536, out_features=384, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (norm_skip): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (6): SkippableSequentialBlocks(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
            (norm1_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (norm2_skip): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.1, mode=row)
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=3072, out_features=768, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
    )
    (decoder): RTDETRTransformer(
      (input_proj): ModuleList(
        (0-2): 3 x Sequential(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (decoder): TransformerDecoder(
        (layers): ModuleList(
          (0-5): 6 x TransformerDecoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (cross_attn): MSDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout2): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (denoising_class_embed): Embedding(81, 256, padding_idx=80)
      (query_pos_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=4, out_features=512, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (enc_output): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
      (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
      (enc_bbox_head): MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
      (dec_score_head): ModuleList(
        (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
      )
      (dec_bbox_head): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
          (act): ReLU(inplace=True)
        )
      )
    )
    (encoder): HybridEncoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (input_proj_swinT): ModuleList(
        (0): Sequential(
          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (encoder): ModuleList(
        (0): TransformerEncoder(
          (layers): ModuleList(
            (0): TransformerEncoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.0, inplace=False)
              (dropout2): Dropout(p=0.0, inplace=False)
              (activation): GELU(approximate='none')
            )
          )
        )
      )
      (lateral_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (fpn_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
      (downsample_convs): ModuleList(
        (0-1): 2 x ConvNormLayer(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
      )
      (pan_blocks): ModuleList(
        (0-1): 2 x CSPRepLayer(
          (conv1): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (conv2): ConvNormLayer(
            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): SiLU(inplace=True)
          )
          (bottlenecks): Sequential(
            (0): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (1): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
            (2): RepVggBlock(
              (conv1): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (conv2): ConvNormLayer(
                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (act): Identity()
              )
              (act): SiLU(inplace=True)
            )
          )
          (conv3): Identity()
        )
      )
    )
  )
)
module.backbone.pre.0.weight (not requires_grad)
module.backbone.pre.0.bias (not requires_grad)
module.backbone.pre.2.weight (not requires_grad)
module.backbone.pre.2.bias (not requires_grad)
module.backbone.features.0.0.norm1.weight
module.backbone.features.0.0.norm1.bias
module.backbone.features.0.0.attn.relative_position_bias_table
module.backbone.features.0.0.attn.qkv.weight
module.backbone.features.0.0.attn.qkv.bias
module.backbone.features.0.0.attn.proj.weight
module.backbone.features.0.0.attn.proj.bias
module.backbone.features.0.0.norm2.weight
module.backbone.features.0.0.norm2.bias
module.backbone.features.0.0.mlp.0.weight
module.backbone.features.0.0.mlp.0.bias
module.backbone.features.0.0.mlp.3.weight
module.backbone.features.0.0.mlp.3.bias
module.backbone.features.0.0.norm1_skip.weight
module.backbone.features.0.0.norm1_skip.bias
module.backbone.features.0.0.norm2_skip.weight
module.backbone.features.0.0.norm2_skip.bias
module.backbone.features.0.1.norm1.weight
module.backbone.features.0.1.norm1.bias
module.backbone.features.0.1.attn.relative_position_bias_table
module.backbone.features.0.1.attn.qkv.weight
module.backbone.features.0.1.attn.qkv.bias
module.backbone.features.0.1.attn.proj.weight
module.backbone.features.0.1.attn.proj.bias
module.backbone.features.0.1.norm2.weight
module.backbone.features.0.1.norm2.bias
module.backbone.features.0.1.mlp.0.weight
module.backbone.features.0.1.mlp.0.bias
module.backbone.features.0.1.mlp.3.weight
module.backbone.features.0.1.mlp.3.bias
module.backbone.features.1.reduction.weight
module.backbone.features.1.norm.weight
module.backbone.features.1.norm.bias
module.backbone.features.1.norm_skip.weight
module.backbone.features.1.norm_skip.bias
module.backbone.features.2.0.norm1.weight
module.backbone.features.2.0.norm1.bias
module.backbone.features.2.0.attn.relative_position_bias_table
module.backbone.features.2.0.attn.qkv.weight
module.backbone.features.2.0.attn.qkv.bias
module.backbone.features.2.0.attn.proj.weight
module.backbone.features.2.0.attn.proj.bias
module.backbone.features.2.0.norm2.weight
module.backbone.features.2.0.norm2.bias
module.backbone.features.2.0.mlp.0.weight
module.backbone.features.2.0.mlp.0.bias
module.backbone.features.2.0.mlp.3.weight
module.backbone.features.2.0.mlp.3.bias
module.backbone.features.2.0.norm1_skip.weight
module.backbone.features.2.0.norm1_skip.bias
module.backbone.features.2.0.norm2_skip.weight
module.backbone.features.2.0.norm2_skip.bias
module.backbone.features.2.1.norm1.weight
module.backbone.features.2.1.norm1.bias
module.backbone.features.2.1.attn.relative_position_bias_table
module.backbone.features.2.1.attn.qkv.weight
module.backbone.features.2.1.attn.qkv.bias
module.backbone.features.2.1.attn.proj.weight
module.backbone.features.2.1.attn.proj.bias
module.backbone.features.2.1.norm2.weight
module.backbone.features.2.1.norm2.bias
module.backbone.features.2.1.mlp.0.weight
module.backbone.features.2.1.mlp.0.bias
module.backbone.features.2.1.mlp.3.weight
module.backbone.features.2.1.mlp.3.bias
module.backbone.features.3.reduction.weight
module.backbone.features.3.norm.weight
module.backbone.features.3.norm.bias
module.backbone.features.3.norm_skip.weight
module.backbone.features.3.norm_skip.bias
module.backbone.features.4.0.norm1.weight
module.backbone.features.4.0.norm1.bias
module.backbone.features.4.0.attn.relative_position_bias_table
module.backbone.features.4.0.attn.qkv.weight
module.backbone.features.4.0.attn.qkv.bias
module.backbone.features.4.0.attn.proj.weight
module.backbone.features.4.0.attn.proj.bias
module.backbone.features.4.0.norm2.weight
module.backbone.features.4.0.norm2.bias
module.backbone.features.4.0.mlp.0.weight
module.backbone.features.4.0.mlp.0.bias
module.backbone.features.4.0.mlp.3.weight
module.backbone.features.4.0.mlp.3.bias
module.backbone.features.4.0.norm1_skip.weight
module.backbone.features.4.0.norm1_skip.bias
module.backbone.features.4.0.norm2_skip.weight
module.backbone.features.4.0.norm2_skip.bias
module.backbone.features.4.1.norm1.weight
module.backbone.features.4.1.norm1.bias
module.backbone.features.4.1.attn.relative_position_bias_table
module.backbone.features.4.1.attn.qkv.weight
module.backbone.features.4.1.attn.qkv.bias
module.backbone.features.4.1.attn.proj.weight
module.backbone.features.4.1.attn.proj.bias
module.backbone.features.4.1.norm2.weight
module.backbone.features.4.1.norm2.bias
module.backbone.features.4.1.mlp.0.weight
module.backbone.features.4.1.mlp.0.bias
module.backbone.features.4.1.mlp.3.weight
module.backbone.features.4.1.mlp.3.bias
module.backbone.features.4.1.norm1_skip.weight
module.backbone.features.4.1.norm1_skip.bias
module.backbone.features.4.1.norm2_skip.weight
module.backbone.features.4.1.norm2_skip.bias
module.backbone.features.4.2.norm1.weight
module.backbone.features.4.2.norm1.bias
module.backbone.features.4.2.attn.relative_position_bias_table
module.backbone.features.4.2.attn.qkv.weight
module.backbone.features.4.2.attn.qkv.bias
module.backbone.features.4.2.attn.proj.weight
module.backbone.features.4.2.attn.proj.bias
module.backbone.features.4.2.norm2.weight
module.backbone.features.4.2.norm2.bias
module.backbone.features.4.2.mlp.0.weight
module.backbone.features.4.2.mlp.0.bias
module.backbone.features.4.2.mlp.3.weight
module.backbone.features.4.2.mlp.3.bias
module.backbone.features.4.2.norm1_skip.weight
module.backbone.features.4.2.norm1_skip.bias
module.backbone.features.4.2.norm2_skip.weight
module.backbone.features.4.2.norm2_skip.bias
module.backbone.features.4.3.norm1.weight
module.backbone.features.4.3.norm1.bias
module.backbone.features.4.3.attn.relative_position_bias_table
module.backbone.features.4.3.attn.qkv.weight
module.backbone.features.4.3.attn.qkv.bias
module.backbone.features.4.3.attn.proj.weight
module.backbone.features.4.3.attn.proj.bias
module.backbone.features.4.3.norm2.weight
module.backbone.features.4.3.norm2.bias
module.backbone.features.4.3.mlp.0.weight
module.backbone.features.4.3.mlp.0.bias
module.backbone.features.4.3.mlp.3.weight
module.backbone.features.4.3.mlp.3.bias
module.backbone.features.4.4.norm1.weight
module.backbone.features.4.4.norm1.bias
module.backbone.features.4.4.attn.relative_position_bias_table
module.backbone.features.4.4.attn.qkv.weight
module.backbone.features.4.4.attn.qkv.bias
module.backbone.features.4.4.attn.proj.weight
module.backbone.features.4.4.attn.proj.bias
module.backbone.features.4.4.norm2.weight
module.backbone.features.4.4.norm2.bias
module.backbone.features.4.4.mlp.0.weight
module.backbone.features.4.4.mlp.0.bias
module.backbone.features.4.4.mlp.3.weight
module.backbone.features.4.4.mlp.3.bias
module.backbone.features.4.5.norm1.weight
module.backbone.features.4.5.norm1.bias
module.backbone.features.4.5.attn.relative_position_bias_table
module.backbone.features.4.5.attn.qkv.weight
module.backbone.features.4.5.attn.qkv.bias
module.backbone.features.4.5.attn.proj.weight
module.backbone.features.4.5.attn.proj.bias
module.backbone.features.4.5.norm2.weight
module.backbone.features.4.5.norm2.bias
module.backbone.features.4.5.mlp.0.weight
module.backbone.features.4.5.mlp.0.bias
module.backbone.features.4.5.mlp.3.weight
module.backbone.features.4.5.mlp.3.bias
module.backbone.features.5.reduction.weight
module.backbone.features.5.norm.weight
module.backbone.features.5.norm.bias
module.backbone.features.5.norm_skip.weight
module.backbone.features.5.norm_skip.bias
module.backbone.features.6.0.norm1.weight
module.backbone.features.6.0.norm1.bias
module.backbone.features.6.0.attn.relative_position_bias_table
module.backbone.features.6.0.attn.qkv.weight
module.backbone.features.6.0.attn.qkv.bias
module.backbone.features.6.0.attn.proj.weight
module.backbone.features.6.0.attn.proj.bias
module.backbone.features.6.0.norm2.weight
module.backbone.features.6.0.norm2.bias
module.backbone.features.6.0.mlp.0.weight
module.backbone.features.6.0.mlp.0.bias
module.backbone.features.6.0.mlp.3.weight
module.backbone.features.6.0.mlp.3.bias
module.backbone.features.6.0.norm1_skip.weight
module.backbone.features.6.0.norm1_skip.bias
module.backbone.features.6.0.norm2_skip.weight
module.backbone.features.6.0.norm2_skip.bias
module.backbone.features.6.1.norm1.weight
module.backbone.features.6.1.norm1.bias
module.backbone.features.6.1.attn.relative_position_bias_table
module.backbone.features.6.1.attn.qkv.weight
module.backbone.features.6.1.attn.qkv.bias
module.backbone.features.6.1.attn.proj.weight
module.backbone.features.6.1.attn.proj.bias
module.backbone.features.6.1.norm2.weight
module.backbone.features.6.1.norm2.bias
module.backbone.features.6.1.mlp.0.weight
module.backbone.features.6.1.mlp.0.bias
module.backbone.features.6.1.mlp.3.weight
module.backbone.features.6.1.mlp.3.bias
module.backbone.norm.weight
module.backbone.norm.bias
module.decoder.input_proj.0.conv.weight
module.decoder.input_proj.0.norm.weight
module.decoder.input_proj.0.norm.bias
module.decoder.input_proj.1.conv.weight
module.decoder.input_proj.1.norm.weight
module.decoder.input_proj.1.norm.bias
module.decoder.input_proj.2.conv.weight
module.decoder.input_proj.2.norm.weight
module.decoder.input_proj.2.norm.bias
module.decoder.decoder.layers.0.self_attn.in_proj_weight
module.decoder.decoder.layers.0.self_attn.in_proj_bias
module.decoder.decoder.layers.0.self_attn.out_proj.weight
module.decoder.decoder.layers.0.self_attn.out_proj.bias
module.decoder.decoder.layers.0.norm1.weight
module.decoder.decoder.layers.0.norm1.bias
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias
module.decoder.decoder.layers.0.cross_attn.value_proj.weight
module.decoder.decoder.layers.0.cross_attn.value_proj.bias
module.decoder.decoder.layers.0.cross_attn.output_proj.weight
module.decoder.decoder.layers.0.cross_attn.output_proj.bias
module.decoder.decoder.layers.0.norm2.weight
module.decoder.decoder.layers.0.norm2.bias
module.decoder.decoder.layers.0.linear1.weight
module.decoder.decoder.layers.0.linear1.bias
module.decoder.decoder.layers.0.linear2.weight
module.decoder.decoder.layers.0.linear2.bias
module.decoder.decoder.layers.0.norm3.weight
module.decoder.decoder.layers.0.norm3.bias
module.decoder.decoder.layers.1.self_attn.in_proj_weight
module.decoder.decoder.layers.1.self_attn.in_proj_bias
module.decoder.decoder.layers.1.self_attn.out_proj.weight
module.decoder.decoder.layers.1.self_attn.out_proj.bias
module.decoder.decoder.layers.1.norm1.weight
module.decoder.decoder.layers.1.norm1.bias
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias
module.decoder.decoder.layers.1.cross_attn.value_proj.weight
module.decoder.decoder.layers.1.cross_attn.value_proj.bias
module.decoder.decoder.layers.1.cross_attn.output_proj.weight
module.decoder.decoder.layers.1.cross_attn.output_proj.bias
module.decoder.decoder.layers.1.norm2.weight
module.decoder.decoder.layers.1.norm2.bias
module.decoder.decoder.layers.1.linear1.weight
module.decoder.decoder.layers.1.linear1.bias
module.decoder.decoder.layers.1.linear2.weight
module.decoder.decoder.layers.1.linear2.bias
module.decoder.decoder.layers.1.norm3.weight
module.decoder.decoder.layers.1.norm3.bias
module.decoder.decoder.layers.2.self_attn.in_proj_weight
module.decoder.decoder.layers.2.self_attn.in_proj_bias
module.decoder.decoder.layers.2.self_attn.out_proj.weight
module.decoder.decoder.layers.2.self_attn.out_proj.bias
module.decoder.decoder.layers.2.norm1.weight
module.decoder.decoder.layers.2.norm1.bias
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias
module.decoder.decoder.layers.2.cross_attn.value_proj.weight
module.decoder.decoder.layers.2.cross_attn.value_proj.bias
module.decoder.decoder.layers.2.cross_attn.output_proj.weight
module.decoder.decoder.layers.2.cross_attn.output_proj.bias
module.decoder.decoder.layers.2.norm2.weight
module.decoder.decoder.layers.2.norm2.bias
module.decoder.decoder.layers.2.linear1.weight
module.decoder.decoder.layers.2.linear1.bias
module.decoder.decoder.layers.2.linear2.weight
module.decoder.decoder.layers.2.linear2.bias
module.decoder.decoder.layers.2.norm3.weight
module.decoder.decoder.layers.2.norm3.bias
module.decoder.decoder.layers.3.self_attn.in_proj_weight
module.decoder.decoder.layers.3.self_attn.in_proj_bias
module.decoder.decoder.layers.3.self_attn.out_proj.weight
module.decoder.decoder.layers.3.self_attn.out_proj.bias
module.decoder.decoder.layers.3.norm1.weight
module.decoder.decoder.layers.3.norm1.bias
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias
module.decoder.decoder.layers.3.cross_attn.value_proj.weight
module.decoder.decoder.layers.3.cross_attn.value_proj.bias
module.decoder.decoder.layers.3.cross_attn.output_proj.weight
module.decoder.decoder.layers.3.cross_attn.output_proj.bias
module.decoder.decoder.layers.3.norm2.weight
module.decoder.decoder.layers.3.norm2.bias
module.decoder.decoder.layers.3.linear1.weight
module.decoder.decoder.layers.3.linear1.bias
module.decoder.decoder.layers.3.linear2.weight
module.decoder.decoder.layers.3.linear2.bias
module.decoder.decoder.layers.3.norm3.weight
module.decoder.decoder.layers.3.norm3.bias
module.decoder.decoder.layers.4.self_attn.in_proj_weight
module.decoder.decoder.layers.4.self_attn.in_proj_bias
module.decoder.decoder.layers.4.self_attn.out_proj.weight
module.decoder.decoder.layers.4.self_attn.out_proj.bias
module.decoder.decoder.layers.4.norm1.weight
module.decoder.decoder.layers.4.norm1.bias
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias
module.decoder.decoder.layers.4.cross_attn.value_proj.weight
module.decoder.decoder.layers.4.cross_attn.value_proj.bias
module.decoder.decoder.layers.4.cross_attn.output_proj.weight
module.decoder.decoder.layers.4.cross_attn.output_proj.bias
module.decoder.decoder.layers.4.norm2.weight
module.decoder.decoder.layers.4.norm2.bias
module.decoder.decoder.layers.4.linear1.weight
module.decoder.decoder.layers.4.linear1.bias
module.decoder.decoder.layers.4.linear2.weight
module.decoder.decoder.layers.4.linear2.bias
module.decoder.decoder.layers.4.norm3.weight
module.decoder.decoder.layers.4.norm3.bias
module.decoder.decoder.layers.5.self_attn.in_proj_weight
module.decoder.decoder.layers.5.self_attn.in_proj_bias
module.decoder.decoder.layers.5.self_attn.out_proj.weight
module.decoder.decoder.layers.5.self_attn.out_proj.bias
module.decoder.decoder.layers.5.norm1.weight
module.decoder.decoder.layers.5.norm1.bias
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias
module.decoder.decoder.layers.5.cross_attn.value_proj.weight
module.decoder.decoder.layers.5.cross_attn.value_proj.bias
module.decoder.decoder.layers.5.cross_attn.output_proj.weight
module.decoder.decoder.layers.5.cross_attn.output_proj.bias
module.decoder.decoder.layers.5.norm2.weight
module.decoder.decoder.layers.5.norm2.bias
module.decoder.decoder.layers.5.linear1.weight
module.decoder.decoder.layers.5.linear1.bias
module.decoder.decoder.layers.5.linear2.weight
module.decoder.decoder.layers.5.linear2.bias
module.decoder.decoder.layers.5.norm3.weight
module.decoder.decoder.layers.5.norm3.bias
module.decoder.denoising_class_embed.weight
module.decoder.query_pos_head.layers.0.weight
module.decoder.query_pos_head.layers.0.bias
module.decoder.query_pos_head.layers.1.weight
module.decoder.query_pos_head.layers.1.bias
module.decoder.enc_output.0.weight
module.decoder.enc_output.0.bias
module.decoder.enc_output.1.weight
module.decoder.enc_output.1.bias
module.decoder.enc_score_head.weight
module.decoder.enc_score_head.bias
module.decoder.enc_bbox_head.layers.0.weight
module.decoder.enc_bbox_head.layers.0.bias
module.decoder.enc_bbox_head.layers.1.weight
module.decoder.enc_bbox_head.layers.1.bias
module.decoder.enc_bbox_head.layers.2.weight
module.decoder.enc_bbox_head.layers.2.bias
module.decoder.dec_score_head.0.weight
module.decoder.dec_score_head.0.bias
module.decoder.dec_score_head.1.weight
module.decoder.dec_score_head.1.bias
module.decoder.dec_score_head.2.weight
module.decoder.dec_score_head.2.bias
module.decoder.dec_score_head.3.weight
module.decoder.dec_score_head.3.bias
module.decoder.dec_score_head.4.weight
module.decoder.dec_score_head.4.bias
module.decoder.dec_score_head.5.weight
module.decoder.dec_score_head.5.bias
module.decoder.dec_bbox_head.0.layers.0.weight
module.decoder.dec_bbox_head.0.layers.0.bias
module.decoder.dec_bbox_head.0.layers.1.weight
module.decoder.dec_bbox_head.0.layers.1.bias
module.decoder.dec_bbox_head.0.layers.2.weight
module.decoder.dec_bbox_head.0.layers.2.bias
module.decoder.dec_bbox_head.1.layers.0.weight
module.decoder.dec_bbox_head.1.layers.0.bias
module.decoder.dec_bbox_head.1.layers.1.weight
module.decoder.dec_bbox_head.1.layers.1.bias
module.decoder.dec_bbox_head.1.layers.2.weight
module.decoder.dec_bbox_head.1.layers.2.bias
module.decoder.dec_bbox_head.2.layers.0.weight
module.decoder.dec_bbox_head.2.layers.0.bias
module.decoder.dec_bbox_head.2.layers.1.weight
module.decoder.dec_bbox_head.2.layers.1.bias
module.decoder.dec_bbox_head.2.layers.2.weight
module.decoder.dec_bbox_head.2.layers.2.bias
module.decoder.dec_bbox_head.3.layers.0.weight
module.decoder.dec_bbox_head.3.layers.0.bias
module.decoder.dec_bbox_head.3.layers.1.weight
module.decoder.dec_bbox_head.3.layers.1.bias
module.decoder.dec_bbox_head.3.layers.2.weight
module.decoder.dec_bbox_head.3.layers.2.bias
module.decoder.dec_bbox_head.4.layers.0.weight
module.decoder.dec_bbox_head.4.layers.0.bias
module.decoder.dec_bbox_head.4.layers.1.weight
module.decoder.dec_bbox_head.4.layers.1.bias
module.decoder.dec_bbox_head.4.layers.2.weight
module.decoder.dec_bbox_head.4.layers.2.bias
module.decoder.dec_bbox_head.5.layers.0.weight
module.decoder.dec_bbox_head.5.layers.0.bias
module.decoder.dec_bbox_head.5.layers.1.weight
module.decoder.dec_bbox_head.5.layers.1.bias
module.decoder.dec_bbox_head.5.layers.2.weight
module.decoder.dec_bbox_head.5.layers.2.bias
module.encoder.input_proj.0.0.weight
module.encoder.input_proj.0.1.weight
module.encoder.input_proj.0.1.bias
module.encoder.input_proj.1.0.weight
module.encoder.input_proj.1.1.weight
module.encoder.input_proj.1.1.bias
module.encoder.input_proj.2.0.weight
module.encoder.input_proj.2.1.weight
module.encoder.input_proj.2.1.bias
module.encoder.input_proj_swinT.0.0.weight
module.encoder.input_proj_swinT.0.1.weight
module.encoder.input_proj_swinT.0.1.bias
module.encoder.input_proj_swinT.1.0.weight
module.encoder.input_proj_swinT.1.1.weight
module.encoder.input_proj_swinT.1.1.bias
module.encoder.input_proj_swinT.2.0.weight
module.encoder.input_proj_swinT.2.1.weight
module.encoder.input_proj_swinT.2.1.bias
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias
module.encoder.encoder.0.layers.0.linear1.weight
module.encoder.encoder.0.layers.0.linear1.bias
module.encoder.encoder.0.layers.0.linear2.weight
module.encoder.encoder.0.layers.0.linear2.bias
module.encoder.encoder.0.layers.0.norm1.weight
module.encoder.encoder.0.layers.0.norm1.bias
module.encoder.encoder.0.layers.0.norm2.weight
module.encoder.encoder.0.layers.0.norm2.bias
module.encoder.lateral_convs.0.conv.weight
module.encoder.lateral_convs.0.norm.weight
module.encoder.lateral_convs.0.norm.bias
module.encoder.lateral_convs.1.conv.weight
module.encoder.lateral_convs.1.norm.weight
module.encoder.lateral_convs.1.norm.bias
module.encoder.fpn_blocks.0.conv1.conv.weight
module.encoder.fpn_blocks.0.conv1.norm.weight
module.encoder.fpn_blocks.0.conv1.norm.bias
module.encoder.fpn_blocks.0.conv2.conv.weight
module.encoder.fpn_blocks.0.conv2.norm.weight
module.encoder.fpn_blocks.0.conv2.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias
module.encoder.fpn_blocks.1.conv1.conv.weight
module.encoder.fpn_blocks.1.conv1.norm.weight
module.encoder.fpn_blocks.1.conv1.norm.bias
module.encoder.fpn_blocks.1.conv2.conv.weight
module.encoder.fpn_blocks.1.conv2.norm.weight
module.encoder.fpn_blocks.1.conv2.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias
module.encoder.downsample_convs.0.conv.weight
module.encoder.downsample_convs.0.norm.weight
module.encoder.downsample_convs.0.norm.bias
module.encoder.downsample_convs.1.conv.weight
module.encoder.downsample_convs.1.norm.weight
module.encoder.downsample_convs.1.norm.bias
module.encoder.pan_blocks.0.conv1.conv.weight
module.encoder.pan_blocks.0.conv1.norm.weight
module.encoder.pan_blocks.0.conv1.norm.bias
module.encoder.pan_blocks.0.conv2.conv.weight
module.encoder.pan_blocks.0.conv2.norm.weight
module.encoder.pan_blocks.0.conv2.norm.bias
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias
module.encoder.pan_blocks.1.conv1.conv.weight
module.encoder.pan_blocks.1.conv1.norm.weight
module.encoder.pan_blocks.1.conv1.norm.bias
module.encoder.pan_blocks.1.conv2.conv.weight
module.encoder.pan_blocks.1.conv2.norm.weight
module.encoder.pan_blocks.1.conv2.norm.bias
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias
start creating model... (in yaml_config.py)
start creating model... (in yaml_config.py)
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.26s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
(in det_solver.py) model params, #params : 
module.backbone.features.0.0.norm1.weight 96
module.backbone.features.0.0.norm1.bias 96
module.backbone.features.0.0.attn.relative_position_bias_table 507
module.backbone.features.0.0.attn.qkv.weight 27648
module.backbone.features.0.0.attn.qkv.bias 288
module.backbone.features.0.0.attn.proj.weight 9216
module.backbone.features.0.0.attn.proj.bias 96
module.backbone.features.0.0.norm2.weight 96
module.backbone.features.0.0.norm2.bias 96
module.backbone.features.0.0.mlp.0.weight 36864
module.backbone.features.0.0.mlp.0.bias 384
module.backbone.features.0.0.mlp.3.weight 36864
module.backbone.features.0.0.mlp.3.bias 96
module.backbone.features.0.0.norm1_skip.weight 96
module.backbone.features.0.0.norm1_skip.bias 96
module.backbone.features.0.0.norm2_skip.weight 96
module.backbone.features.0.0.norm2_skip.bias 96
module.backbone.features.0.1.norm1.weight 96
module.backbone.features.0.1.norm1.bias 96
module.backbone.features.0.1.attn.relative_position_bias_table 507
module.backbone.features.0.1.attn.qkv.weight 27648
module.backbone.features.0.1.attn.qkv.bias 288
module.backbone.features.0.1.attn.proj.weight 9216
module.backbone.features.0.1.attn.proj.bias 96
module.backbone.features.0.1.norm2.weight 96
module.backbone.features.0.1.norm2.bias 96
module.backbone.features.0.1.mlp.0.weight 36864
module.backbone.features.0.1.mlp.0.bias 384
module.backbone.features.0.1.mlp.3.weight 36864
module.backbone.features.0.1.mlp.3.bias 96
module.backbone.features.1.reduction.weight 73728
module.backbone.features.1.norm.weight 384
module.backbone.features.1.norm.bias 384
module.backbone.features.1.norm_skip.weight 384
module.backbone.features.1.norm_skip.bias 384
module.backbone.features.2.0.norm1.weight 192
module.backbone.features.2.0.norm1.bias 192
module.backbone.features.2.0.attn.relative_position_bias_table 1014
module.backbone.features.2.0.attn.qkv.weight 110592
module.backbone.features.2.0.attn.qkv.bias 576
module.backbone.features.2.0.attn.proj.weight 36864
module.backbone.features.2.0.attn.proj.bias 192
module.backbone.features.2.0.norm2.weight 192
module.backbone.features.2.0.norm2.bias 192
module.backbone.features.2.0.mlp.0.weight 147456
module.backbone.features.2.0.mlp.0.bias 768
module.backbone.features.2.0.mlp.3.weight 147456
module.backbone.features.2.0.mlp.3.bias 192
module.backbone.features.2.0.norm1_skip.weight 192
module.backbone.features.2.0.norm1_skip.bias 192
module.backbone.features.2.0.norm2_skip.weight 192
module.backbone.features.2.0.norm2_skip.bias 192
module.backbone.features.2.1.norm1.weight 192
module.backbone.features.2.1.norm1.bias 192
module.backbone.features.2.1.attn.relative_position_bias_table 1014
module.backbone.features.2.1.attn.qkv.weight 110592
module.backbone.features.2.1.attn.qkv.bias 576
module.backbone.features.2.1.attn.proj.weight 36864
module.backbone.features.2.1.attn.proj.bias 192
module.backbone.features.2.1.norm2.weight 192
module.backbone.features.2.1.norm2.bias 192
module.backbone.features.2.1.mlp.0.weight 147456
module.backbone.features.2.1.mlp.0.bias 768
module.backbone.features.2.1.mlp.3.weight 147456
module.backbone.features.2.1.mlp.3.bias 192
module.backbone.features.3.reduction.weight 294912
module.backbone.features.3.norm.weight 768
module.backbone.features.3.norm.bias 768
module.backbone.features.3.norm_skip.weight 768
module.backbone.features.3.norm_skip.bias 768
module.backbone.features.4.0.norm1.weight 384
module.backbone.features.4.0.norm1.bias 384
module.backbone.features.4.0.attn.relative_position_bias_table 2028
module.backbone.features.4.0.attn.qkv.weight 442368
module.backbone.features.4.0.attn.qkv.bias 1152
module.backbone.features.4.0.attn.proj.weight 147456
module.backbone.features.4.0.attn.proj.bias 384
module.backbone.features.4.0.norm2.weight 384
module.backbone.features.4.0.norm2.bias 384
module.backbone.features.4.0.mlp.0.weight 589824
module.backbone.features.4.0.mlp.0.bias 1536
module.backbone.features.4.0.mlp.3.weight 589824
module.backbone.features.4.0.mlp.3.bias 384
module.backbone.features.4.0.norm1_skip.weight 384
module.backbone.features.4.0.norm1_skip.bias 384
module.backbone.features.4.0.norm2_skip.weight 384
module.backbone.features.4.0.norm2_skip.bias 384
module.backbone.features.4.1.norm1.weight 384
module.backbone.features.4.1.norm1.bias 384
module.backbone.features.4.1.attn.relative_position_bias_table 2028
module.backbone.features.4.1.attn.qkv.weight 442368
module.backbone.features.4.1.attn.qkv.bias 1152
module.backbone.features.4.1.attn.proj.weight 147456
module.backbone.features.4.1.attn.proj.bias 384
module.backbone.features.4.1.norm2.weight 384
module.backbone.features.4.1.norm2.bias 384
module.backbone.features.4.1.mlp.0.weight 589824
module.backbone.features.4.1.mlp.0.bias 1536
module.backbone.features.4.1.mlp.3.weight 589824
module.backbone.features.4.1.mlp.3.bias 384
module.backbone.features.4.1.norm1_skip.weight 384
module.backbone.features.4.1.norm1_skip.bias 384
module.backbone.features.4.1.norm2_skip.weight 384
module.backbone.features.4.1.norm2_skip.bias 384
module.backbone.features.4.2.norm1.weight 384
module.backbone.features.4.2.norm1.bias 384
module.backbone.features.4.2.attn.relative_position_bias_table 2028
module.backbone.features.4.2.attn.qkv.weight 442368
module.backbone.features.4.2.attn.qkv.bias 1152
module.backbone.features.4.2.attn.proj.weight 147456
module.backbone.features.4.2.attn.proj.bias 384
module.backbone.features.4.2.norm2.weight 384
module.backbone.features.4.2.norm2.bias 384
module.backbone.features.4.2.mlp.0.weight 589824
module.backbone.features.4.2.mlp.0.bias 1536
module.backbone.features.4.2.mlp.3.weight 589824
module.backbone.features.4.2.mlp.3.bias 384
module.backbone.features.4.2.norm1_skip.weight 384
module.backbone.features.4.2.norm1_skip.bias 384
module.backbone.features.4.2.norm2_skip.weight 384
module.backbone.features.4.2.norm2_skip.bias 384
module.backbone.features.4.3.norm1.weight 384
module.backbone.features.4.3.norm1.bias 384
module.backbone.features.4.3.attn.relative_position_bias_table 2028
module.backbone.features.4.3.attn.qkv.weight 442368
module.backbone.features.4.3.attn.qkv.bias 1152
module.backbone.features.4.3.attn.proj.weight 147456
module.backbone.features.4.3.attn.proj.bias 384
module.backbone.features.4.3.norm2.weight 384
module.backbone.features.4.3.norm2.bias 384
module.backbone.features.4.3.mlp.0.weight 589824
module.backbone.features.4.3.mlp.0.bias 1536
module.backbone.features.4.3.mlp.3.weight 589824
module.backbone.features.4.3.mlp.3.bias 384
module.backbone.features.4.4.norm1.weight 384
module.backbone.features.4.4.norm1.bias 384
module.backbone.features.4.4.attn.relative_position_bias_table 2028
module.backbone.features.4.4.attn.qkv.weight 442368
module.backbone.features.4.4.attn.qkv.bias 1152
module.backbone.features.4.4.attn.proj.weight 147456
module.backbone.features.4.4.attn.proj.bias 384
module.backbone.features.4.4.norm2.weight 384
module.backbone.features.4.4.norm2.bias 384
module.backbone.features.4.4.mlp.0.weight 589824
module.backbone.features.4.4.mlp.0.bias 1536
module.backbone.features.4.4.mlp.3.weight 589824
module.backbone.features.4.4.mlp.3.bias 384
module.backbone.features.4.5.norm1.weight 384
module.backbone.features.4.5.norm1.bias 384
module.backbone.features.4.5.attn.relative_position_bias_table 2028
module.backbone.features.4.5.attn.qkv.weight 442368
module.backbone.features.4.5.attn.qkv.bias 1152
module.backbone.features.4.5.attn.proj.weight 147456
module.backbone.features.4.5.attn.proj.bias 384
module.backbone.features.4.5.norm2.weight 384
module.backbone.features.4.5.norm2.bias 384
module.backbone.features.4.5.mlp.0.weight 589824
module.backbone.features.4.5.mlp.0.bias 1536
module.backbone.features.4.5.mlp.3.weight 589824
module.backbone.features.4.5.mlp.3.bias 384
module.backbone.features.5.reduction.weight 1179648
module.backbone.features.5.norm.weight 1536
module.backbone.features.5.norm.bias 1536
module.backbone.features.5.norm_skip.weight 1536
module.backbone.features.5.norm_skip.bias 1536
module.backbone.features.6.0.norm1.weight 768
module.backbone.features.6.0.norm1.bias 768
module.backbone.features.6.0.attn.relative_position_bias_table 4056
module.backbone.features.6.0.attn.qkv.weight 1769472
module.backbone.features.6.0.attn.qkv.bias 2304
module.backbone.features.6.0.attn.proj.weight 589824
module.backbone.features.6.0.attn.proj.bias 768
module.backbone.features.6.0.norm2.weight 768
module.backbone.features.6.0.norm2.bias 768
module.backbone.features.6.0.mlp.0.weight 2359296
module.backbone.features.6.0.mlp.0.bias 3072
module.backbone.features.6.0.mlp.3.weight 2359296
module.backbone.features.6.0.mlp.3.bias 768
module.backbone.features.6.0.norm1_skip.weight 768
module.backbone.features.6.0.norm1_skip.bias 768
module.backbone.features.6.0.norm2_skip.weight 768
module.backbone.features.6.0.norm2_skip.bias 768
module.backbone.features.6.1.norm1.weight 768
module.backbone.features.6.1.norm1.bias 768
module.backbone.features.6.1.attn.relative_position_bias_table 4056
module.backbone.features.6.1.attn.qkv.weight 1769472
module.backbone.features.6.1.attn.qkv.bias 2304
module.backbone.features.6.1.attn.proj.weight 589824
module.backbone.features.6.1.attn.proj.bias 768
module.backbone.features.6.1.norm2.weight 768
module.backbone.features.6.1.norm2.bias 768
module.backbone.features.6.1.mlp.0.weight 2359296
module.backbone.features.6.1.mlp.0.bias 3072
module.backbone.features.6.1.mlp.3.weight 2359296
module.backbone.features.6.1.mlp.3.bias 768
module.backbone.norm.weight 768
module.backbone.norm.bias 768
module.decoder.input_proj.0.conv.weight 65536
module.decoder.input_proj.0.norm.weight 256
module.decoder.input_proj.0.norm.bias 256
module.decoder.input_proj.1.conv.weight 65536
module.decoder.input_proj.1.norm.weight 256
module.decoder.input_proj.1.norm.bias 256
module.decoder.input_proj.2.conv.weight 65536
module.decoder.input_proj.2.norm.weight 256
module.decoder.input_proj.2.norm.bias 256
module.decoder.decoder.layers.0.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.0.self_attn.in_proj_bias 768
module.decoder.decoder.layers.0.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.0.self_attn.out_proj.bias 256
module.decoder.decoder.layers.0.norm1.weight 256
module.decoder.decoder.layers.0.norm1.bias 256
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.0.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.0.norm2.weight 256
module.decoder.decoder.layers.0.norm2.bias 256
module.decoder.decoder.layers.0.linear1.weight 262144
module.decoder.decoder.layers.0.linear1.bias 1024
module.decoder.decoder.layers.0.linear2.weight 262144
module.decoder.decoder.layers.0.linear2.bias 256
module.decoder.decoder.layers.0.norm3.weight 256
module.decoder.decoder.layers.0.norm3.bias 256
module.decoder.decoder.layers.1.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.1.self_attn.in_proj_bias 768
module.decoder.decoder.layers.1.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.1.self_attn.out_proj.bias 256
module.decoder.decoder.layers.1.norm1.weight 256
module.decoder.decoder.layers.1.norm1.bias 256
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.1.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.1.norm2.weight 256
module.decoder.decoder.layers.1.norm2.bias 256
module.decoder.decoder.layers.1.linear1.weight 262144
module.decoder.decoder.layers.1.linear1.bias 1024
module.decoder.decoder.layers.1.linear2.weight 262144
module.decoder.decoder.layers.1.linear2.bias 256
module.decoder.decoder.layers.1.norm3.weight 256
module.decoder.decoder.layers.1.norm3.bias 256
module.decoder.decoder.layers.2.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.2.self_attn.in_proj_bias 768
module.decoder.decoder.layers.2.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.2.self_attn.out_proj.bias 256
module.decoder.decoder.layers.2.norm1.weight 256
module.decoder.decoder.layers.2.norm1.bias 256
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.2.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.2.norm2.weight 256
module.decoder.decoder.layers.2.norm2.bias 256
module.decoder.decoder.layers.2.linear1.weight 262144
module.decoder.decoder.layers.2.linear1.bias 1024
module.decoder.decoder.layers.2.linear2.weight 262144
module.decoder.decoder.layers.2.linear2.bias 256
module.decoder.decoder.layers.2.norm3.weight 256
module.decoder.decoder.layers.2.norm3.bias 256
module.decoder.decoder.layers.3.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.3.self_attn.in_proj_bias 768
module.decoder.decoder.layers.3.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.3.self_attn.out_proj.bias 256
module.decoder.decoder.layers.3.norm1.weight 256
module.decoder.decoder.layers.3.norm1.bias 256
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.3.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.3.norm2.weight 256
module.decoder.decoder.layers.3.norm2.bias 256
module.decoder.decoder.layers.3.linear1.weight 262144
module.decoder.decoder.layers.3.linear1.bias 1024
module.decoder.decoder.layers.3.linear2.weight 262144
module.decoder.decoder.layers.3.linear2.bias 256
module.decoder.decoder.layers.3.norm3.weight 256
module.decoder.decoder.layers.3.norm3.bias 256
module.decoder.decoder.layers.4.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.4.self_attn.in_proj_bias 768
module.decoder.decoder.layers.4.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.4.self_attn.out_proj.bias 256
module.decoder.decoder.layers.4.norm1.weight 256
module.decoder.decoder.layers.4.norm1.bias 256
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.4.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.4.norm2.weight 256
module.decoder.decoder.layers.4.norm2.bias 256
module.decoder.decoder.layers.4.linear1.weight 262144
module.decoder.decoder.layers.4.linear1.bias 1024
module.decoder.decoder.layers.4.linear2.weight 262144
module.decoder.decoder.layers.4.linear2.bias 256
module.decoder.decoder.layers.4.norm3.weight 256
module.decoder.decoder.layers.4.norm3.bias 256
module.decoder.decoder.layers.5.self_attn.in_proj_weight 196608
module.decoder.decoder.layers.5.self_attn.in_proj_bias 768
module.decoder.decoder.layers.5.self_attn.out_proj.weight 65536
module.decoder.decoder.layers.5.self_attn.out_proj.bias 256
module.decoder.decoder.layers.5.norm1.weight 256
module.decoder.decoder.layers.5.norm1.bias 256
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
module.decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
module.decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
module.decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
module.decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.value_proj.bias 256
module.decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
module.decoder.decoder.layers.5.cross_attn.output_proj.bias 256
module.decoder.decoder.layers.5.norm2.weight 256
module.decoder.decoder.layers.5.norm2.bias 256
module.decoder.decoder.layers.5.linear1.weight 262144
module.decoder.decoder.layers.5.linear1.bias 1024
module.decoder.decoder.layers.5.linear2.weight 262144
module.decoder.decoder.layers.5.linear2.bias 256
module.decoder.decoder.layers.5.norm3.weight 256
module.decoder.decoder.layers.5.norm3.bias 256
module.decoder.denoising_class_embed.weight 20736
module.decoder.query_pos_head.layers.0.weight 2048
module.decoder.query_pos_head.layers.0.bias 512
module.decoder.query_pos_head.layers.1.weight 131072
module.decoder.query_pos_head.layers.1.bias 256
module.decoder.enc_output.0.weight 65536
module.decoder.enc_output.0.bias 256
module.decoder.enc_output.1.weight 256
module.decoder.enc_output.1.bias 256
module.decoder.enc_score_head.weight 20480
module.decoder.enc_score_head.bias 80
module.decoder.enc_bbox_head.layers.0.weight 65536
module.decoder.enc_bbox_head.layers.0.bias 256
module.decoder.enc_bbox_head.layers.1.weight 65536
module.decoder.enc_bbox_head.layers.1.bias 256
module.decoder.enc_bbox_head.layers.2.weight 1024
module.decoder.enc_bbox_head.layers.2.bias 4
module.decoder.dec_score_head.0.weight 20480
module.decoder.dec_score_head.0.bias 80
module.decoder.dec_score_head.1.weight 20480
module.decoder.dec_score_head.1.bias 80
module.decoder.dec_score_head.2.weight 20480
module.decoder.dec_score_head.2.bias 80
module.decoder.dec_score_head.3.weight 20480
module.decoder.dec_score_head.3.bias 80
module.decoder.dec_score_head.4.weight 20480
module.decoder.dec_score_head.4.bias 80
module.decoder.dec_score_head.5.weight 20480
module.decoder.dec_score_head.5.bias 80
module.decoder.dec_bbox_head.0.layers.0.weight 65536
module.decoder.dec_bbox_head.0.layers.0.bias 256
module.decoder.dec_bbox_head.0.layers.1.weight 65536
module.decoder.dec_bbox_head.0.layers.1.bias 256
module.decoder.dec_bbox_head.0.layers.2.weight 1024
module.decoder.dec_bbox_head.0.layers.2.bias 4
module.decoder.dec_bbox_head.1.layers.0.weight 65536
module.decoder.dec_bbox_head.1.layers.0.bias 256
module.decoder.dec_bbox_head.1.layers.1.weight 65536
module.decoder.dec_bbox_head.1.layers.1.bias 256
module.decoder.dec_bbox_head.1.layers.2.weight 1024
module.decoder.dec_bbox_head.1.layers.2.bias 4
module.decoder.dec_bbox_head.2.layers.0.weight 65536
module.decoder.dec_bbox_head.2.layers.0.bias 256
module.decoder.dec_bbox_head.2.layers.1.weight 65536
module.decoder.dec_bbox_head.2.layers.1.bias 256
module.decoder.dec_bbox_head.2.layers.2.weight 1024
module.decoder.dec_bbox_head.2.layers.2.bias 4
module.decoder.dec_bbox_head.3.layers.0.weight 65536
module.decoder.dec_bbox_head.3.layers.0.bias 256
module.decoder.dec_bbox_head.3.layers.1.weight 65536
module.decoder.dec_bbox_head.3.layers.1.bias 256
module.decoder.dec_bbox_head.3.layers.2.weight 1024
module.decoder.dec_bbox_head.3.layers.2.bias 4
module.decoder.dec_bbox_head.4.layers.0.weight 65536
module.decoder.dec_bbox_head.4.layers.0.bias 256
module.decoder.dec_bbox_head.4.layers.1.weight 65536
module.decoder.dec_bbox_head.4.layers.1.bias 256
module.decoder.dec_bbox_head.4.layers.2.weight 1024
module.decoder.dec_bbox_head.4.layers.2.bias 4
module.decoder.dec_bbox_head.5.layers.0.weight 65536
module.decoder.dec_bbox_head.5.layers.0.bias 256
module.decoder.dec_bbox_head.5.layers.1.weight 65536
module.decoder.dec_bbox_head.5.layers.1.bias 256
module.decoder.dec_bbox_head.5.layers.2.weight 1024
module.decoder.dec_bbox_head.5.layers.2.bias 4
module.encoder.input_proj.0.0.weight 131072
module.encoder.input_proj.0.1.weight 256
module.encoder.input_proj.0.1.bias 256
module.encoder.input_proj.1.0.weight 262144
module.encoder.input_proj.1.1.weight 256
module.encoder.input_proj.1.1.bias 256
module.encoder.input_proj.2.0.weight 524288
module.encoder.input_proj.2.1.weight 256
module.encoder.input_proj.2.1.bias 256
module.encoder.input_proj_swinT.0.0.weight 49152
module.encoder.input_proj_swinT.0.1.weight 256
module.encoder.input_proj_swinT.0.1.bias 256
module.encoder.input_proj_swinT.1.0.weight 98304
module.encoder.input_proj_swinT.1.1.weight 256
module.encoder.input_proj_swinT.1.1.bias 256
module.encoder.input_proj_swinT.2.0.weight 196608
module.encoder.input_proj_swinT.2.1.weight 256
module.encoder.input_proj_swinT.2.1.bias 256
module.encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
module.encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
module.encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
module.encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
module.encoder.encoder.0.layers.0.linear1.weight 262144
module.encoder.encoder.0.layers.0.linear1.bias 1024
module.encoder.encoder.0.layers.0.linear2.weight 262144
module.encoder.encoder.0.layers.0.linear2.bias 256
module.encoder.encoder.0.layers.0.norm1.weight 256
module.encoder.encoder.0.layers.0.norm1.bias 256
module.encoder.encoder.0.layers.0.norm2.weight 256
module.encoder.encoder.0.layers.0.norm2.bias 256
module.encoder.lateral_convs.0.conv.weight 65536
module.encoder.lateral_convs.0.norm.weight 256
module.encoder.lateral_convs.0.norm.bias 256
module.encoder.lateral_convs.1.conv.weight 65536
module.encoder.lateral_convs.1.norm.weight 256
module.encoder.lateral_convs.1.norm.bias 256
module.encoder.fpn_blocks.0.conv1.conv.weight 131072
module.encoder.fpn_blocks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.conv2.conv.weight 131072
module.encoder.fpn_blocks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.fpn_blocks.1.conv1.conv.weight 131072
module.encoder.fpn_blocks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.conv2.conv.weight 131072
module.encoder.fpn_blocks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
module.encoder.downsample_convs.0.conv.weight 589824
module.encoder.downsample_convs.0.norm.weight 256
module.encoder.downsample_convs.0.norm.bias 256
module.encoder.downsample_convs.1.conv.weight 589824
module.encoder.downsample_convs.1.norm.weight 256
module.encoder.downsample_convs.1.norm.bias 256
module.encoder.pan_blocks.0.conv1.conv.weight 131072
module.encoder.pan_blocks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.conv2.conv.weight 131072
module.encoder.pan_blocks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
module.encoder.pan_blocks.1.conv1.conv.weight 131072
module.encoder.pan_blocks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.conv2.conv.weight 131072
module.encoder.pan_blocks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
module.encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 47291622
super_config : [False, False, False, False]
base_config : [True, True, True, True]
Epoch: [0]  [    0/14786]  eta: 7:55:27  lr: 0.000010  loss: 47.8626 (47.8626)  loss_bbox: 1.9309 (1.9309)  loss_bbox_aux_0: 2.0021 (2.0021)  loss_bbox_aux_1: 1.9276 (1.9276)  loss_bbox_aux_2: 1.9703 (1.9703)  loss_bbox_aux_3: 1.9841 (1.9841)  loss_bbox_aux_4: 1.9400 (1.9400)  loss_bbox_aux_5: 1.9504 (1.9504)  loss_bbox_dn_0: 1.0449 (1.0449)  loss_bbox_dn_1: 1.0449 (1.0449)  loss_bbox_dn_2: 1.0449 (1.0449)  loss_bbox_dn_3: 1.0449 (1.0449)  loss_bbox_dn_4: 1.0449 (1.0449)  loss_bbox_dn_5: 1.0449 (1.0449)  loss_giou: 1.7748 (1.7748)  loss_giou_aux_0: 1.7951 (1.7951)  loss_giou_aux_1: 1.7459 (1.7459)  loss_giou_aux_2: 1.7415 (1.7415)  loss_giou_aux_3: 1.7452 (1.7452)  loss_giou_aux_4: 1.7402 (1.7402)  loss_giou_aux_5: 1.7688 (1.7688)  loss_giou_dn_0: 1.2918 (1.2918)  loss_giou_dn_1: 1.2918 (1.2918)  loss_giou_dn_2: 1.2918 (1.2918)  loss_giou_dn_3: 1.2918 (1.2918)  loss_giou_dn_4: 1.2918 (1.2918)  loss_giou_dn_5: 1.2918 (1.2918)  loss_vfl: 0.3084 (0.3084)  loss_vfl_aux_0: 0.3027 (0.3027)  loss_vfl_aux_1: 0.3573 (0.3573)  loss_vfl_aux_2: 0.3252 (0.3252)  loss_vfl_aux_3: 0.3173 (0.3173)  loss_vfl_aux_4: 0.3281 (0.3281)  loss_vfl_aux_5: 0.3293 (0.3293)  loss_vfl_dn_0: 0.9885 (0.9885)  loss_vfl_dn_1: 0.9400 (0.9400)  loss_vfl_dn_2: 0.9189 (0.9189)  loss_vfl_dn_3: 0.9396 (0.9396)  loss_vfl_dn_4: 0.8729 (0.8729)  loss_vfl_dn_5: 0.8971 (0.8971)  time: 1.9293  data: 0.5983  max mem: 8056
Epoch: [0]  [  100/14786]  eta: 1:20:52  lr: 0.000010  loss: 37.0445 (40.3600)  loss_bbox: 0.9057 (1.1962)  loss_bbox_aux_0: 0.9407 (1.2572)  loss_bbox_aux_1: 0.9330 (1.2372)  loss_bbox_aux_2: 0.9240 (1.2208)  loss_bbox_aux_3: 0.9155 (1.2092)  loss_bbox_aux_4: 0.9119 (1.2025)  loss_bbox_aux_5: 0.9348 (1.3002)  loss_bbox_dn_0: 0.7115 (0.8779)  loss_bbox_dn_1: 0.7220 (0.8854)  loss_bbox_dn_2: 0.7327 (0.8918)  loss_bbox_dn_3: 0.7402 (0.8984)  loss_bbox_dn_4: 0.7451 (0.9034)  loss_bbox_dn_5: 0.7508 (0.9090)  loss_giou: 1.4809 (1.5110)  loss_giou_aux_0: 1.4952 (1.5425)  loss_giou_aux_1: 1.5021 (1.5278)  loss_giou_aux_2: 1.4842 (1.5224)  loss_giou_aux_3: 1.4835 (1.5163)  loss_giou_aux_4: 1.4971 (1.5140)  loss_giou_aux_5: 1.5250 (1.5661)  loss_giou_dn_0: 1.3237 (1.3324)  loss_giou_dn_1: 1.3206 (1.3332)  loss_giou_dn_2: 1.3191 (1.3351)  loss_giou_dn_3: 1.3292 (1.3400)  loss_giou_dn_4: 1.3391 (1.3453)  loss_giou_dn_5: 1.3492 (1.3517)  loss_vfl: 0.6227 (0.5858)  loss_vfl_aux_0: 0.5379 (0.5269)  loss_vfl_aux_1: 0.5806 (0.5487)  loss_vfl_aux_2: 0.5794 (0.5669)  loss_vfl_aux_3: 0.6232 (0.5775)  loss_vfl_aux_4: 0.6361 (0.5870)  loss_vfl_aux_5: 0.5153 (0.4970)  loss_vfl_dn_0: 0.5653 (0.6409)  loss_vfl_dn_1: 0.5396 (0.6206)  loss_vfl_dn_2: 0.5342 (0.6160)  loss_vfl_dn_3: 0.5541 (0.6288)  loss_vfl_dn_4: 0.5530 (0.6187)  loss_vfl_dn_5: 0.5463 (0.6182)  time: 0.3081  data: 0.0098  max mem: 9307
Epoch: [0]  [  200/14786]  eta: 1:16:57  lr: 0.000010  loss: 35.2854 (38.8018)  loss_bbox: 0.6603 (0.9939)  loss_bbox_aux_0: 0.7127 (1.0555)  loss_bbox_aux_1: 0.6864 (1.0307)  loss_bbox_aux_2: 0.6670 (1.0153)  loss_bbox_aux_3: 0.6672 (1.0037)  loss_bbox_aux_4: 0.6619 (0.9979)  loss_bbox_aux_5: 0.7627 (1.0975)  loss_bbox_dn_0: 0.8008 (0.8665)  loss_bbox_dn_1: 0.7870 (0.8666)  loss_bbox_dn_2: 0.7850 (0.8693)  loss_bbox_dn_3: 0.7845 (0.8709)  loss_bbox_dn_4: 0.7850 (0.8731)  loss_bbox_dn_5: 0.7865 (0.8754)  loss_giou: 1.2456 (1.4206)  loss_giou_aux_0: 1.2577 (1.4654)  loss_giou_aux_1: 1.2356 (1.4429)  loss_giou_aux_2: 1.2217 (1.4338)  loss_giou_aux_3: 1.2313 (1.4277)  loss_giou_aux_4: 1.2346 (1.4240)  loss_giou_aux_5: 1.2793 (1.4925)  loss_giou_dn_0: 1.3150 (1.3296)  loss_giou_dn_1: 1.3106 (1.3291)  loss_giou_dn_2: 1.3165 (1.3328)  loss_giou_dn_3: 1.3178 (1.3389)  loss_giou_dn_4: 1.3190 (1.3445)  loss_giou_dn_5: 1.3226 (1.3515)  loss_vfl: 0.9215 (0.7250)  loss_vfl_aux_0: 0.7635 (0.6371)  loss_vfl_aux_1: 0.8444 (0.6708)  loss_vfl_aux_2: 0.8871 (0.6949)  loss_vfl_aux_3: 0.8635 (0.7098)  loss_vfl_aux_4: 0.8847 (0.7262)  loss_vfl_aux_5: 0.7122 (0.6030)  loss_vfl_dn_0: 0.4938 (0.5826)  loss_vfl_dn_1: 0.4989 (0.5744)  loss_vfl_dn_2: 0.5000 (0.5762)  loss_vfl_dn_3: 0.5099 (0.5877)  loss_vfl_dn_4: 0.5178 (0.5827)  loss_vfl_dn_5: 0.5106 (0.5818)  time: 0.2994  data: 0.0090  max mem: 9307
Epoch: [0]  [  300/14786]  eta: 1:15:22  lr: 0.000010  loss: 34.2554 (37.8729)  loss_bbox: 0.5209 (0.8847)  loss_bbox_aux_0: 0.5767 (0.9481)  loss_bbox_aux_1: 0.5613 (0.9189)  loss_bbox_aux_2: 0.5447 (0.9037)  loss_bbox_aux_3: 0.5419 (0.8930)  loss_bbox_aux_4: 0.5327 (0.8878)  loss_bbox_aux_5: 0.6838 (1.0042)  loss_bbox_dn_0: 0.7640 (0.8654)  loss_bbox_dn_1: 0.7435 (0.8583)  loss_bbox_dn_2: 0.7299 (0.8582)  loss_bbox_dn_3: 0.7231 (0.8580)  loss_bbox_dn_4: 0.7220 (0.8593)  loss_bbox_dn_5: 0.7209 (0.8607)  loss_giou: 1.1228 (1.3457)  loss_giou_aux_0: 1.1848 (1.3930)  loss_giou_aux_1: 1.1391 (1.3674)  loss_giou_aux_2: 1.1353 (1.3578)  loss_giou_aux_3: 1.1258 (1.3526)  loss_giou_aux_4: 1.1215 (1.3487)  loss_giou_aux_5: 1.2441 (1.4338)  loss_giou_dn_0: 1.2904 (1.3194)  loss_giou_dn_1: 1.2588 (1.3151)  loss_giou_dn_2: 1.2549 (1.3178)  loss_giou_dn_3: 1.2516 (1.3231)  loss_giou_dn_4: 1.2497 (1.3278)  loss_giou_dn_5: 1.2446 (1.3337)  loss_vfl: 1.0616 (0.8228)  loss_vfl_aux_0: 0.9541 (0.7161)  loss_vfl_aux_1: 0.9832 (0.7568)  loss_vfl_aux_2: 0.9787 (0.7824)  loss_vfl_aux_3: 1.0308 (0.8015)  loss_vfl_aux_4: 1.0470 (0.8204)  loss_vfl_aux_5: 0.8929 (0.6644)  loss_vfl_dn_0: 0.4964 (0.5512)  loss_vfl_dn_1: 0.5132 (0.5531)  loss_vfl_dn_2: 0.5149 (0.5597)  loss_vfl_dn_3: 0.5465 (0.5707)  loss_vfl_dn_4: 0.5387 (0.5687)  loss_vfl_dn_5: 0.5468 (0.5688)  time: 0.2965  data: 0.0098  max mem: 9307
Epoch: [0]  [  400/14786]  eta: 1:14:30  lr: 0.000010  loss: 34.0379 (37.2024)  loss_bbox: 0.5531 (0.8140)  loss_bbox_aux_0: 0.6997 (0.8797)  loss_bbox_aux_1: 0.6145 (0.8484)  loss_bbox_aux_2: 0.5919 (0.8319)  loss_bbox_aux_3: 0.5676 (0.8217)  loss_bbox_aux_4: 0.5668 (0.8171)  loss_bbox_aux_5: 0.7566 (0.9429)  loss_bbox_dn_0: 0.7680 (0.8554)  loss_bbox_dn_1: 0.7419 (0.8421)  loss_bbox_dn_2: 0.7217 (0.8396)  loss_bbox_dn_3: 0.7146 (0.8384)  loss_bbox_dn_4: 0.7109 (0.8392)  loss_bbox_dn_5: 0.7089 (0.8401)  loss_giou: 1.1484 (1.2906)  loss_giou_aux_0: 1.2097 (1.3444)  loss_giou_aux_1: 1.1793 (1.3149)  loss_giou_aux_2: 1.1608 (1.3035)  loss_giou_aux_3: 1.1611 (1.2975)  loss_giou_aux_4: 1.1578 (1.2935)  loss_giou_aux_5: 1.3069 (1.3954)  loss_giou_dn_0: 1.2514 (1.3065)  loss_giou_dn_1: 1.2196 (1.2951)  loss_giou_dn_2: 1.2123 (1.2960)  loss_giou_dn_3: 1.2236 (1.3004)  loss_giou_dn_4: 1.2266 (1.3048)  loss_giou_dn_5: 1.2288 (1.3100)  loss_vfl: 0.9241 (0.8944)  loss_vfl_aux_0: 0.7851 (0.7675)  loss_vfl_aux_1: 0.8601 (0.8161)  loss_vfl_aux_2: 0.8809 (0.8476)  loss_vfl_aux_3: 0.9009 (0.8696)  loss_vfl_aux_4: 0.9130 (0.8882)  loss_vfl_aux_5: 0.6965 (0.7060)  loss_vfl_dn_0: 0.4803 (0.5360)  loss_vfl_dn_1: 0.5220 (0.5475)  loss_vfl_dn_2: 0.5379 (0.5588)  loss_vfl_dn_3: 0.5428 (0.5700)  loss_vfl_dn_4: 0.5487 (0.5684)  loss_vfl_dn_5: 0.5469 (0.5691)  time: 0.3014  data: 0.0091  max mem: 9308
Epoch: [0]  [  500/14786]  eta: 1:14:22  lr: 0.000010  loss: 33.3533 (36.5979)  loss_bbox: 0.4967 (0.7621)  loss_bbox_aux_0: 0.5171 (0.8282)  loss_bbox_aux_1: 0.5092 (0.7957)  loss_bbox_aux_2: 0.4908 (0.7791)  loss_bbox_aux_3: 0.4895 (0.7693)  loss_bbox_aux_4: 0.4861 (0.7649)  loss_bbox_aux_5: 0.6093 (0.8960)  loss_bbox_dn_0: 0.7519 (0.8392)  loss_bbox_dn_1: 0.7085 (0.8216)  loss_bbox_dn_2: 0.6988 (0.8175)  loss_bbox_dn_3: 0.7002 (0.8157)  loss_bbox_dn_4: 0.7014 (0.8163)  loss_bbox_dn_5: 0.7022 (0.8170)  loss_giou: 1.1023 (1.2524)  loss_giou_aux_0: 1.1599 (1.3110)  loss_giou_aux_1: 1.1030 (1.2785)  loss_giou_aux_2: 1.1001 (1.2657)  loss_giou_aux_3: 1.1010 (1.2590)  loss_giou_aux_4: 1.1070 (1.2550)  loss_giou_aux_5: 1.2127 (1.3705)  loss_giou_dn_0: 1.2394 (1.2929)  loss_giou_dn_1: 1.2011 (1.2762)  loss_giou_dn_2: 1.2013 (1.2753)  loss_giou_dn_3: 1.2055 (1.2785)  loss_giou_dn_4: 1.2058 (1.2822)  loss_giou_dn_5: 1.2063 (1.2867)  loss_vfl: 1.0220 (0.9407)  loss_vfl_aux_0: 0.9233 (0.8015)  loss_vfl_aux_1: 0.9521 (0.8543)  loss_vfl_aux_2: 1.0000 (0.8883)  loss_vfl_aux_3: 1.0209 (0.9135)  loss_vfl_aux_4: 1.0135 (0.9307)  loss_vfl_aux_5: 0.8635 (0.7310)  loss_vfl_dn_0: 0.4645 (0.5245)  loss_vfl_dn_1: 0.5182 (0.5424)  loss_vfl_dn_2: 0.5327 (0.5573)  loss_vfl_dn_3: 0.5254 (0.5690)  loss_vfl_dn_4: 0.5342 (0.5684)  loss_vfl_dn_5: 0.5276 (0.5696)  time: 0.3059  data: 0.0090  max mem: 9311
Epoch: [0]  [  600/14786]  eta: 1:13:23  lr: 0.000010  loss: 34.0682 (36.1939)  loss_bbox: 0.4920 (0.7305)  loss_bbox_aux_0: 0.5852 (0.7958)  loss_bbox_aux_1: 0.5021 (0.7628)  loss_bbox_aux_2: 0.4851 (0.7467)  loss_bbox_aux_3: 0.5114 (0.7378)  loss_bbox_aux_4: 0.5032 (0.7336)  loss_bbox_aux_5: 0.6995 (0.8684)  loss_bbox_dn_0: 0.7509 (0.8244)  loss_bbox_dn_1: 0.7178 (0.8036)  loss_bbox_dn_2: 0.7117 (0.7986)  loss_bbox_dn_3: 0.7132 (0.7964)  loss_bbox_dn_4: 0.7150 (0.7968)  loss_bbox_dn_5: 0.7166 (0.7974)  loss_giou: 0.9969 (1.2333)  loss_giou_aux_0: 1.0903 (1.2945)  loss_giou_aux_1: 1.0191 (1.2601)  loss_giou_aux_2: 1.0050 (1.2464)  loss_giou_aux_3: 1.0220 (1.2398)  loss_giou_aux_4: 0.9776 (1.2359)  loss_giou_aux_5: 1.2165 (1.3613)  loss_giou_dn_0: 1.1867 (1.2815)  loss_giou_dn_1: 1.1217 (1.2600)  loss_giou_dn_2: 1.0977 (1.2573)  loss_giou_dn_3: 1.0995 (1.2594)  loss_giou_dn_4: 1.0997 (1.2626)  loss_giou_dn_5: 1.1013 (1.2666)  loss_vfl: 1.1703 (0.9677)  loss_vfl_aux_0: 1.0341 (0.8238)  loss_vfl_aux_1: 1.1086 (0.8774)  loss_vfl_aux_2: 1.1207 (0.9124)  loss_vfl_aux_3: 1.1303 (0.9366)  loss_vfl_aux_4: 1.1756 (0.9547)  loss_vfl_aux_5: 0.8628 (0.7459)  loss_vfl_dn_0: 0.4749 (0.5170)  loss_vfl_dn_1: 0.5406 (0.5400)  loss_vfl_dn_2: 0.5874 (0.5573)  loss_vfl_dn_3: 0.5855 (0.5691)  loss_vfl_dn_4: 0.6025 (0.5691)  loss_vfl_dn_5: 0.6166 (0.5714)  time: 0.2900  data: 0.0090  max mem: 9312
Epoch: [0]  [  700/14786]  eta: 1:12:29  lr: 0.000010  loss: 32.1471 (35.7521)  loss_bbox: 0.4022 (0.6978)  loss_bbox_aux_0: 0.5117 (0.7640)  loss_bbox_aux_1: 0.4675 (0.7306)  loss_bbox_aux_2: 0.4464 (0.7139)  loss_bbox_aux_3: 0.4258 (0.7051)  loss_bbox_aux_4: 0.4053 (0.7010)  loss_bbox_aux_5: 0.5890 (0.8394)  loss_bbox_dn_0: 0.7190 (0.8135)  loss_bbox_dn_1: 0.6841 (0.7904)  loss_bbox_dn_2: 0.6738 (0.7848)  loss_bbox_dn_3: 0.6724 (0.7824)  loss_bbox_dn_4: 0.6720 (0.7827)  loss_bbox_dn_5: 0.6714 (0.7832)  loss_giou: 1.0095 (1.2055)  loss_giou_aux_0: 1.0851 (1.2695)  loss_giou_aux_1: 1.0688 (1.2335)  loss_giou_aux_2: 1.0261 (1.2188)  loss_giou_aux_3: 1.0445 (1.2119)  loss_giou_aux_4: 1.0078 (1.2079)  loss_giou_aux_5: 1.2043 (1.3431)  loss_giou_dn_0: 1.2018 (1.2704)  loss_giou_dn_1: 1.1298 (1.2444)  loss_giou_dn_2: 1.1032 (1.2397)  loss_giou_dn_3: 1.0990 (1.2409)  loss_giou_dn_4: 1.1020 (1.2437)  loss_giou_dn_5: 1.1012 (1.2474)  loss_vfl: 1.1475 (0.9924)  loss_vfl_aux_0: 0.9654 (0.8459)  loss_vfl_aux_1: 1.0161 (0.8998)  loss_vfl_aux_2: 1.0422 (0.9354)  loss_vfl_aux_3: 1.0923 (0.9599)  loss_vfl_aux_4: 1.1236 (0.9775)  loss_vfl_aux_5: 0.8628 (0.7643)  loss_vfl_dn_0: 0.4607 (0.5104)  loss_vfl_dn_1: 0.5073 (0.5365)  loss_vfl_dn_2: 0.5442 (0.5563)  loss_vfl_dn_3: 0.5581 (0.5681)  loss_vfl_dn_4: 0.5643 (0.5688)  loss_vfl_dn_5: 0.5627 (0.5713)  time: 0.3045  data: 0.0080  max mem: 9312
Epoch: [0]  [  800/14786]  eta: 1:11:51  lr: 0.000010  loss: 31.6239 (35.3975)  loss_bbox: 0.4368 (0.6712)  loss_bbox_aux_0: 0.5142 (0.7376)  loss_bbox_aux_1: 0.4510 (0.7039)  loss_bbox_aux_2: 0.4450 (0.6873)  loss_bbox_aux_3: 0.4345 (0.6787)  loss_bbox_aux_4: 0.4431 (0.6748)  loss_bbox_aux_5: 0.6525 (0.8172)  loss_bbox_dn_0: 0.7524 (0.8077)  loss_bbox_dn_1: 0.6878 (0.7822)  loss_bbox_dn_2: 0.6822 (0.7760)  loss_bbox_dn_3: 0.6818 (0.7736)  loss_bbox_dn_4: 0.6819 (0.7738)  loss_bbox_dn_5: 0.6822 (0.7743)  loss_giou: 0.9198 (1.1778)  loss_giou_aux_0: 1.0124 (1.2450)  loss_giou_aux_1: 0.9571 (1.2071)  loss_giou_aux_2: 0.9562 (1.1917)  loss_giou_aux_3: 0.9351 (1.1844)  loss_giou_aux_4: 0.9134 (1.1802)  loss_giou_aux_5: 1.1748 (1.3239)  loss_giou_dn_0: 1.1537 (1.2592)  loss_giou_dn_1: 1.0771 (1.2293)  loss_giou_dn_2: 1.0573 (1.2228)  loss_giou_dn_3: 1.0519 (1.2229)  loss_giou_dn_4: 1.0526 (1.2254)  loss_giou_dn_5: 1.0538 (1.2286)  loss_vfl: 1.1642 (1.0174)  loss_vfl_aux_0: 0.9972 (0.8684)  loss_vfl_aux_1: 1.0718 (0.9232)  loss_vfl_aux_2: 1.0466 (0.9583)  loss_vfl_aux_3: 1.0753 (0.9832)  loss_vfl_aux_4: 1.1140 (1.0006)  loss_vfl_aux_5: 0.8316 (0.7851)  loss_vfl_dn_0: 0.4839 (0.5062)  loss_vfl_dn_1: 0.5282 (0.5341)  loss_vfl_dn_2: 0.5662 (0.5560)  loss_vfl_dn_3: 0.5723 (0.5679)  loss_vfl_dn_4: 0.5915 (0.5691)  loss_vfl_dn_5: 0.6062 (0.5718)  time: 0.3151  data: 0.0098  max mem: 9312
