Not init distributed mode.
Start training
start creating model... (in yaml_config.py)
make layer i: 0, num_blocks: 3, num_shared: 2, num_skippable: 1
make layer i: 1, num_blocks: 4, num_shared: 2, num_skippable: 2
make layer i: 2, num_blocks: 6, num_shared: 3, num_skippable: 3
make layer i: 3, num_blocks: 3, num_shared: 2, num_skippable: 1
Load ResNet50-ADN state_dict from /home/hslee/Desktop/Embedded_AI/INU_4-1/RISE/02_AdaptiveDepthNetwork/pretrained/resnet50_adn_model_145.pth
self.model (in solver.py): 
RTDETR(
  (backbone): ResNetADN(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): FrozenBatchNorm2d(64, eps=1e-05)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): SkippableSequentialBlocks(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): FrozenBatchNorm2d(256, eps=1e-05)
        )
        (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
        (bn1_skip): FrozenBatchNorm2d(64, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(64, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(256, eps=1e-05)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(64, eps=1e-05)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(64, eps=1e-05)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(256, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): SkippableSequentialBlocks(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(512, eps=1e-05)
        )
        (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
        (bn1_skip): FrozenBatchNorm2d(128, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(128, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(512, eps=1e-05)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(128, eps=1e-05)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(128, eps=1e-05)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(512, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): SkippableSequentialBlocks(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(1024, eps=1e-05)
        )
        (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
        (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
        (bn1_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(256, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(1024, eps=1e-05)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(256, eps=1e-05)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(256, eps=1e-05)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(1024, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): SkippableSequentialBlocks(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d(2048, eps=1e-05)
        )
        (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
        (bn1_skip): FrozenBatchNorm2d(512, eps=1e-05)
        (bn2_skip): FrozenBatchNorm2d(512, eps=1e-05)
        (bn3_skip): FrozenBatchNorm2d(2048, eps=1e-05)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d(512, eps=1e-05)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d(512, eps=1e-05)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d(2048, eps=1e-05)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (decoder): RTDETRTransformer(
    (input_proj): ModuleList(
      (0-2): 3 x Sequential(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.0, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn): MSDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
            (attention_weights): Linear(in_features=256, out_features=96, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Dropout(p=0.0, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout3): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (dropout4): Dropout(p=0.0, inplace=False)
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (denoising_class_embed): Embedding(81, 256, padding_idx=80)
    (query_pos_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=4, out_features=512, bias=True)
        (1): Linear(in_features=512, out_features=256, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (enc_output): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (enc_score_head): Linear(in_features=256, out_features=80, bias=True)
    (enc_bbox_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU(inplace=True)
    )
    (dec_score_head): ModuleList(
      (0-5): 6 x Linear(in_features=256, out_features=80, bias=True)
    )
    (dec_bbox_head): ModuleList(
      (0-5): 6 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
        (act): ReLU(inplace=True)
      )
    )
  )
  (encoder): HybridEncoder(
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (encoder): ModuleList(
      (0): TransformerEncoder(
        (layers): ModuleList(
          (0): TransformerEncoderLayer(
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.0, inplace=False)
            (activation): GELU(approximate='none')
          )
        )
      )
    )
    (lateral_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (fpn_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
    (downsample_convs): ModuleList(
      (0-1): 2 x ConvNormLayer(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act): SiLU(inplace=True)
      )
    )
    (pan_blocks): ModuleList(
      (0-1): 2 x CSPRepLayer(
        (conv1): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (conv2): ConvNormLayer(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): SiLU(inplace=True)
        )
        (bottlenecks): Sequential(
          (0): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (1): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
          (2): RepVggBlock(
            (conv1): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (conv2): ConvNormLayer(
              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act): Identity()
            )
            (act): SiLU(inplace=True)
          )
        )
        (conv3): Identity()
      )
    )
  )
)
start creating model... (in yaml_config.py)
start creating model... (in yaml_config.py)
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=6.76s)
creating index...
index created!
loading annotations into memory...
Done (t=0.23s)
creating index...
index created!
(in det_solver.py) model params, #params : 
backbone.layer1.0.conv1.weight 4096
backbone.layer1.0.conv2.weight 36864
backbone.layer1.0.conv3.weight 16384
backbone.layer1.0.downsample.0.weight 16384
backbone.layer1.1.conv1.weight 16384
backbone.layer1.1.conv2.weight 36864
backbone.layer1.1.conv3.weight 16384
backbone.layer1.2.conv1.weight 16384
backbone.layer1.2.conv2.weight 36864
backbone.layer1.2.conv3.weight 16384
backbone.layer2.0.conv1.weight 32768
backbone.layer2.0.conv2.weight 147456
backbone.layer2.0.conv3.weight 65536
backbone.layer2.0.downsample.0.weight 131072
backbone.layer2.1.conv1.weight 65536
backbone.layer2.1.conv2.weight 147456
backbone.layer2.1.conv3.weight 65536
backbone.layer2.2.conv1.weight 65536
backbone.layer2.2.conv2.weight 147456
backbone.layer2.2.conv3.weight 65536
backbone.layer2.3.conv1.weight 65536
backbone.layer2.3.conv2.weight 147456
backbone.layer2.3.conv3.weight 65536
backbone.layer3.0.conv1.weight 131072
backbone.layer3.0.conv2.weight 589824
backbone.layer3.0.conv3.weight 262144
backbone.layer3.0.downsample.0.weight 524288
backbone.layer3.1.conv1.weight 262144
backbone.layer3.1.conv2.weight 589824
backbone.layer3.1.conv3.weight 262144
backbone.layer3.2.conv1.weight 262144
backbone.layer3.2.conv2.weight 589824
backbone.layer3.2.conv3.weight 262144
backbone.layer3.3.conv1.weight 262144
backbone.layer3.3.conv2.weight 589824
backbone.layer3.3.conv3.weight 262144
backbone.layer3.4.conv1.weight 262144
backbone.layer3.4.conv2.weight 589824
backbone.layer3.4.conv3.weight 262144
backbone.layer3.5.conv1.weight 262144
backbone.layer3.5.conv2.weight 589824
backbone.layer3.5.conv3.weight 262144
backbone.layer4.0.conv1.weight 524288
backbone.layer4.0.conv2.weight 2359296
backbone.layer4.0.conv3.weight 1048576
backbone.layer4.0.downsample.0.weight 2097152
backbone.layer4.1.conv1.weight 1048576
backbone.layer4.1.conv2.weight 2359296
backbone.layer4.1.conv3.weight 1048576
backbone.layer4.2.conv1.weight 1048576
backbone.layer4.2.conv2.weight 2359296
backbone.layer4.2.conv3.weight 1048576
decoder.input_proj.0.conv.weight 65536
decoder.input_proj.0.norm.weight 256
decoder.input_proj.0.norm.bias 256
decoder.input_proj.1.conv.weight 65536
decoder.input_proj.1.norm.weight 256
decoder.input_proj.1.norm.bias 256
decoder.input_proj.2.conv.weight 65536
decoder.input_proj.2.norm.weight 256
decoder.input_proj.2.norm.bias 256
decoder.decoder.layers.0.self_attn.in_proj_weight 196608
decoder.decoder.layers.0.self_attn.in_proj_bias 768
decoder.decoder.layers.0.self_attn.out_proj.weight 65536
decoder.decoder.layers.0.self_attn.out_proj.bias 256
decoder.decoder.layers.0.norm1.weight 256
decoder.decoder.layers.0.norm1.bias 256
decoder.decoder.layers.0.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.0.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.0.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.0.cross_attn.attention_weights.bias 96
decoder.decoder.layers.0.cross_attn.value_proj.weight 65536
decoder.decoder.layers.0.cross_attn.value_proj.bias 256
decoder.decoder.layers.0.cross_attn.output_proj.weight 65536
decoder.decoder.layers.0.cross_attn.output_proj.bias 256
decoder.decoder.layers.0.norm2.weight 256
decoder.decoder.layers.0.norm2.bias 256
decoder.decoder.layers.0.linear1.weight 262144
decoder.decoder.layers.0.linear1.bias 1024
decoder.decoder.layers.0.linear2.weight 262144
decoder.decoder.layers.0.linear2.bias 256
decoder.decoder.layers.0.norm3.weight 256
decoder.decoder.layers.0.norm3.bias 256
decoder.decoder.layers.1.self_attn.in_proj_weight 196608
decoder.decoder.layers.1.self_attn.in_proj_bias 768
decoder.decoder.layers.1.self_attn.out_proj.weight 65536
decoder.decoder.layers.1.self_attn.out_proj.bias 256
decoder.decoder.layers.1.norm1.weight 256
decoder.decoder.layers.1.norm1.bias 256
decoder.decoder.layers.1.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.1.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.1.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.1.cross_attn.attention_weights.bias 96
decoder.decoder.layers.1.cross_attn.value_proj.weight 65536
decoder.decoder.layers.1.cross_attn.value_proj.bias 256
decoder.decoder.layers.1.cross_attn.output_proj.weight 65536
decoder.decoder.layers.1.cross_attn.output_proj.bias 256
decoder.decoder.layers.1.norm2.weight 256
decoder.decoder.layers.1.norm2.bias 256
decoder.decoder.layers.1.linear1.weight 262144
decoder.decoder.layers.1.linear1.bias 1024
decoder.decoder.layers.1.linear2.weight 262144
decoder.decoder.layers.1.linear2.bias 256
decoder.decoder.layers.1.norm3.weight 256
decoder.decoder.layers.1.norm3.bias 256
decoder.decoder.layers.2.self_attn.in_proj_weight 196608
decoder.decoder.layers.2.self_attn.in_proj_bias 768
decoder.decoder.layers.2.self_attn.out_proj.weight 65536
decoder.decoder.layers.2.self_attn.out_proj.bias 256
decoder.decoder.layers.2.norm1.weight 256
decoder.decoder.layers.2.norm1.bias 256
decoder.decoder.layers.2.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.2.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.2.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.2.cross_attn.attention_weights.bias 96
decoder.decoder.layers.2.cross_attn.value_proj.weight 65536
decoder.decoder.layers.2.cross_attn.value_proj.bias 256
decoder.decoder.layers.2.cross_attn.output_proj.weight 65536
decoder.decoder.layers.2.cross_attn.output_proj.bias 256
decoder.decoder.layers.2.norm2.weight 256
decoder.decoder.layers.2.norm2.bias 256
decoder.decoder.layers.2.linear1.weight 262144
decoder.decoder.layers.2.linear1.bias 1024
decoder.decoder.layers.2.linear2.weight 262144
decoder.decoder.layers.2.linear2.bias 256
decoder.decoder.layers.2.norm3.weight 256
decoder.decoder.layers.2.norm3.bias 256
decoder.decoder.layers.3.self_attn.in_proj_weight 196608
decoder.decoder.layers.3.self_attn.in_proj_bias 768
decoder.decoder.layers.3.self_attn.out_proj.weight 65536
decoder.decoder.layers.3.self_attn.out_proj.bias 256
decoder.decoder.layers.3.norm1.weight 256
decoder.decoder.layers.3.norm1.bias 256
decoder.decoder.layers.3.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.3.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.3.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.3.cross_attn.attention_weights.bias 96
decoder.decoder.layers.3.cross_attn.value_proj.weight 65536
decoder.decoder.layers.3.cross_attn.value_proj.bias 256
decoder.decoder.layers.3.cross_attn.output_proj.weight 65536
decoder.decoder.layers.3.cross_attn.output_proj.bias 256
decoder.decoder.layers.3.norm2.weight 256
decoder.decoder.layers.3.norm2.bias 256
decoder.decoder.layers.3.linear1.weight 262144
decoder.decoder.layers.3.linear1.bias 1024
decoder.decoder.layers.3.linear2.weight 262144
decoder.decoder.layers.3.linear2.bias 256
decoder.decoder.layers.3.norm3.weight 256
decoder.decoder.layers.3.norm3.bias 256
decoder.decoder.layers.4.self_attn.in_proj_weight 196608
decoder.decoder.layers.4.self_attn.in_proj_bias 768
decoder.decoder.layers.4.self_attn.out_proj.weight 65536
decoder.decoder.layers.4.self_attn.out_proj.bias 256
decoder.decoder.layers.4.norm1.weight 256
decoder.decoder.layers.4.norm1.bias 256
decoder.decoder.layers.4.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.4.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.4.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.4.cross_attn.attention_weights.bias 96
decoder.decoder.layers.4.cross_attn.value_proj.weight 65536
decoder.decoder.layers.4.cross_attn.value_proj.bias 256
decoder.decoder.layers.4.cross_attn.output_proj.weight 65536
decoder.decoder.layers.4.cross_attn.output_proj.bias 256
decoder.decoder.layers.4.norm2.weight 256
decoder.decoder.layers.4.norm2.bias 256
decoder.decoder.layers.4.linear1.weight 262144
decoder.decoder.layers.4.linear1.bias 1024
decoder.decoder.layers.4.linear2.weight 262144
decoder.decoder.layers.4.linear2.bias 256
decoder.decoder.layers.4.norm3.weight 256
decoder.decoder.layers.4.norm3.bias 256
decoder.decoder.layers.5.self_attn.in_proj_weight 196608
decoder.decoder.layers.5.self_attn.in_proj_bias 768
decoder.decoder.layers.5.self_attn.out_proj.weight 65536
decoder.decoder.layers.5.self_attn.out_proj.bias 256
decoder.decoder.layers.5.norm1.weight 256
decoder.decoder.layers.5.norm1.bias 256
decoder.decoder.layers.5.cross_attn.sampling_offsets.weight 49152
decoder.decoder.layers.5.cross_attn.sampling_offsets.bias 192
decoder.decoder.layers.5.cross_attn.attention_weights.weight 24576
decoder.decoder.layers.5.cross_attn.attention_weights.bias 96
decoder.decoder.layers.5.cross_attn.value_proj.weight 65536
decoder.decoder.layers.5.cross_attn.value_proj.bias 256
decoder.decoder.layers.5.cross_attn.output_proj.weight 65536
decoder.decoder.layers.5.cross_attn.output_proj.bias 256
decoder.decoder.layers.5.norm2.weight 256
decoder.decoder.layers.5.norm2.bias 256
decoder.decoder.layers.5.linear1.weight 262144
decoder.decoder.layers.5.linear1.bias 1024
decoder.decoder.layers.5.linear2.weight 262144
decoder.decoder.layers.5.linear2.bias 256
decoder.decoder.layers.5.norm3.weight 256
decoder.decoder.layers.5.norm3.bias 256
decoder.denoising_class_embed.weight 20736
decoder.query_pos_head.layers.0.weight 2048
decoder.query_pos_head.layers.0.bias 512
decoder.query_pos_head.layers.1.weight 131072
decoder.query_pos_head.layers.1.bias 256
decoder.enc_output.0.weight 65536
decoder.enc_output.0.bias 256
decoder.enc_output.1.weight 256
decoder.enc_output.1.bias 256
decoder.enc_score_head.weight 20480
decoder.enc_score_head.bias 80
decoder.enc_bbox_head.layers.0.weight 65536
decoder.enc_bbox_head.layers.0.bias 256
decoder.enc_bbox_head.layers.1.weight 65536
decoder.enc_bbox_head.layers.1.bias 256
decoder.enc_bbox_head.layers.2.weight 1024
decoder.enc_bbox_head.layers.2.bias 4
decoder.dec_score_head.0.weight 20480
decoder.dec_score_head.0.bias 80
decoder.dec_score_head.1.weight 20480
decoder.dec_score_head.1.bias 80
decoder.dec_score_head.2.weight 20480
decoder.dec_score_head.2.bias 80
decoder.dec_score_head.3.weight 20480
decoder.dec_score_head.3.bias 80
decoder.dec_score_head.4.weight 20480
decoder.dec_score_head.4.bias 80
decoder.dec_score_head.5.weight 20480
decoder.dec_score_head.5.bias 80
decoder.dec_bbox_head.0.layers.0.weight 65536
decoder.dec_bbox_head.0.layers.0.bias 256
decoder.dec_bbox_head.0.layers.1.weight 65536
decoder.dec_bbox_head.0.layers.1.bias 256
decoder.dec_bbox_head.0.layers.2.weight 1024
decoder.dec_bbox_head.0.layers.2.bias 4
decoder.dec_bbox_head.1.layers.0.weight 65536
decoder.dec_bbox_head.1.layers.0.bias 256
decoder.dec_bbox_head.1.layers.1.weight 65536
decoder.dec_bbox_head.1.layers.1.bias 256
decoder.dec_bbox_head.1.layers.2.weight 1024
decoder.dec_bbox_head.1.layers.2.bias 4
decoder.dec_bbox_head.2.layers.0.weight 65536
decoder.dec_bbox_head.2.layers.0.bias 256
decoder.dec_bbox_head.2.layers.1.weight 65536
decoder.dec_bbox_head.2.layers.1.bias 256
decoder.dec_bbox_head.2.layers.2.weight 1024
decoder.dec_bbox_head.2.layers.2.bias 4
decoder.dec_bbox_head.3.layers.0.weight 65536
decoder.dec_bbox_head.3.layers.0.bias 256
decoder.dec_bbox_head.3.layers.1.weight 65536
decoder.dec_bbox_head.3.layers.1.bias 256
decoder.dec_bbox_head.3.layers.2.weight 1024
decoder.dec_bbox_head.3.layers.2.bias 4
decoder.dec_bbox_head.4.layers.0.weight 65536
decoder.dec_bbox_head.4.layers.0.bias 256
decoder.dec_bbox_head.4.layers.1.weight 65536
decoder.dec_bbox_head.4.layers.1.bias 256
decoder.dec_bbox_head.4.layers.2.weight 1024
decoder.dec_bbox_head.4.layers.2.bias 4
decoder.dec_bbox_head.5.layers.0.weight 65536
decoder.dec_bbox_head.5.layers.0.bias 256
decoder.dec_bbox_head.5.layers.1.weight 65536
decoder.dec_bbox_head.5.layers.1.bias 256
decoder.dec_bbox_head.5.layers.2.weight 1024
decoder.dec_bbox_head.5.layers.2.bias 4
encoder.input_proj.0.0.weight 131072
encoder.input_proj.0.1.weight 256
encoder.input_proj.0.1.bias 256
encoder.input_proj.1.0.weight 262144
encoder.input_proj.1.1.weight 256
encoder.input_proj.1.1.bias 256
encoder.input_proj.2.0.weight 524288
encoder.input_proj.2.1.weight 256
encoder.input_proj.2.1.bias 256
encoder.encoder.0.layers.0.self_attn.in_proj_weight 196608
encoder.encoder.0.layers.0.self_attn.in_proj_bias 768
encoder.encoder.0.layers.0.self_attn.out_proj.weight 65536
encoder.encoder.0.layers.0.self_attn.out_proj.bias 256
encoder.encoder.0.layers.0.linear1.weight 262144
encoder.encoder.0.layers.0.linear1.bias 1024
encoder.encoder.0.layers.0.linear2.weight 262144
encoder.encoder.0.layers.0.linear2.bias 256
encoder.encoder.0.layers.0.norm1.weight 256
encoder.encoder.0.layers.0.norm1.bias 256
encoder.encoder.0.layers.0.norm2.weight 256
encoder.encoder.0.layers.0.norm2.bias 256
encoder.lateral_convs.0.conv.weight 65536
encoder.lateral_convs.0.norm.weight 256
encoder.lateral_convs.0.norm.bias 256
encoder.lateral_convs.1.conv.weight 65536
encoder.lateral_convs.1.norm.weight 256
encoder.lateral_convs.1.norm.bias 256
encoder.fpn_blocks.0.conv1.conv.weight 131072
encoder.fpn_blocks.0.conv1.norm.weight 256
encoder.fpn_blocks.0.conv1.norm.bias 256
encoder.fpn_blocks.0.conv2.conv.weight 131072
encoder.fpn_blocks.0.conv2.norm.weight 256
encoder.fpn_blocks.0.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.0.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.0.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.0.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.0.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.1.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.1.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.1.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.1.conv2.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.2.conv1.conv.weight 589824
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.2.conv1.norm.bias 256
encoder.fpn_blocks.0.bottlenecks.2.conv2.conv.weight 65536
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.weight 256
encoder.fpn_blocks.0.bottlenecks.2.conv2.norm.bias 256
encoder.fpn_blocks.1.conv1.conv.weight 131072
encoder.fpn_blocks.1.conv1.norm.weight 256
encoder.fpn_blocks.1.conv1.norm.bias 256
encoder.fpn_blocks.1.conv2.conv.weight 131072
encoder.fpn_blocks.1.conv2.norm.weight 256
encoder.fpn_blocks.1.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.0.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.0.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.0.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.0.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.1.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.1.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.1.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.1.conv2.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.2.conv1.conv.weight 589824
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.2.conv1.norm.bias 256
encoder.fpn_blocks.1.bottlenecks.2.conv2.conv.weight 65536
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.weight 256
encoder.fpn_blocks.1.bottlenecks.2.conv2.norm.bias 256
encoder.downsample_convs.0.conv.weight 589824
encoder.downsample_convs.0.norm.weight 256
encoder.downsample_convs.0.norm.bias 256
encoder.downsample_convs.1.conv.weight 589824
encoder.downsample_convs.1.norm.weight 256
encoder.downsample_convs.1.norm.bias 256
encoder.pan_blocks.0.conv1.conv.weight 131072
encoder.pan_blocks.0.conv1.norm.weight 256
encoder.pan_blocks.0.conv1.norm.bias 256
encoder.pan_blocks.0.conv2.conv.weight 131072
encoder.pan_blocks.0.conv2.norm.weight 256
encoder.pan_blocks.0.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.0.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.0.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.0.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.0.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.1.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.1.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.1.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.1.conv2.norm.bias 256
encoder.pan_blocks.0.bottlenecks.2.conv1.conv.weight 589824
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.weight 256
encoder.pan_blocks.0.bottlenecks.2.conv1.norm.bias 256
encoder.pan_blocks.0.bottlenecks.2.conv2.conv.weight 65536
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.weight 256
encoder.pan_blocks.0.bottlenecks.2.conv2.norm.bias 256
encoder.pan_blocks.1.conv1.conv.weight 131072
encoder.pan_blocks.1.conv1.norm.weight 256
encoder.pan_blocks.1.conv1.norm.bias 256
encoder.pan_blocks.1.conv2.conv.weight 131072
encoder.pan_blocks.1.conv2.norm.weight 256
encoder.pan_blocks.1.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.0.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.0.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.0.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.0.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.1.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.1.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.1.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.1.conv2.norm.bias 256
encoder.pan_blocks.1.bottlenecks.2.conv1.conv.weight 589824
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.weight 256
encoder.pan_blocks.1.bottlenecks.2.conv1.norm.bias 256
encoder.pan_blocks.1.bottlenecks.2.conv2.conv.weight 65536
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.weight 256
encoder.pan_blocks.1.bottlenecks.2.conv2.norm.bias 256
number of params: 42862860
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Could not load library libcudnn_cnn_train.so.8. Error: /home/hslee/anaconda3/envs/DL/bin/../lib/libcudnn_cnn_train.so.8: undefined symbol: _ZN5cudnn3cnn34layerNormFwd_execute_internal_implERKNS_7backend11VariantPackEP11CUstream_stRNS0_18LayerNormFwdParamsERNS1_12OperationSetERP12cudnnContextmb, version libcudnn_cnn_infer.so.8
Epoch: [0]  [    0/29571]  eta: 8:05:14  lr: 0.000010  loss: 46.5215 (46.5215)  loss_vfl: 0.5080 (0.5080)  loss_bbox: 1.7457 (1.7457)  loss_giou: 1.5367 (1.5367)  loss_vfl_aux_0: 0.4470 (0.4470)  loss_bbox_aux_0: 1.8542 (1.8542)  loss_giou_aux_0: 1.5906 (1.5906)  loss_vfl_aux_1: 0.5492 (0.5492)  loss_bbox_aux_1: 1.7506 (1.7506)  loss_giou_aux_1: 1.5807 (1.5807)  loss_vfl_aux_2: 0.6048 (0.6048)  loss_bbox_aux_2: 1.7206 (1.7206)  loss_giou_aux_2: 1.5066 (1.5066)  loss_vfl_aux_3: 0.4502 (0.4502)  loss_bbox_aux_3: 1.7692 (1.7692)  loss_giou_aux_3: 1.5093 (1.5093)  loss_vfl_aux_4: 0.5883 (0.5883)  loss_bbox_aux_4: 1.7053 (1.7053)  loss_giou_aux_4: 1.5164 (1.5164)  loss_vfl_aux_5: 0.4898 (0.4898)  loss_bbox_aux_5: 1.7909 (1.7909)  loss_giou_aux_5: 1.5398 (1.5398)  loss_vfl_dn_0: 1.0565 (1.0565)  loss_bbox_dn_0: 1.0154 (1.0154)  loss_giou_dn_0: 1.2712 (1.2712)  loss_vfl_dn_1: 0.9789 (0.9789)  loss_bbox_dn_1: 1.0154 (1.0154)  loss_giou_dn_1: 1.2712 (1.2712)  loss_vfl_dn_2: 0.9990 (0.9990)  loss_bbox_dn_2: 1.0154 (1.0154)  loss_giou_dn_2: 1.2712 (1.2712)  loss_vfl_dn_3: 0.9957 (0.9957)  loss_bbox_dn_3: 1.0154 (1.0154)  loss_giou_dn_3: 1.2712 (1.2712)  loss_vfl_dn_4: 1.0607 (1.0607)  loss_bbox_dn_4: 1.0154 (1.0154)  loss_giou_dn_4: 1.2712 (1.2712)  loss_vfl_dn_5: 0.9573 (0.9573)  loss_bbox_dn_5: 1.0154 (1.0154)  loss_giou_dn_5: 1.2712 (1.2712)  time: 0.9846  data: 0.2931  max mem: 3515
Epoch: [0]  [  100/29571]  eta: 1:49:40  lr: 0.000010  loss: 39.5225 (42.2813)  loss_vfl: 0.7842 (0.5658)  loss_bbox: 1.1558 (1.3695)  loss_giou: 1.4175 (1.5384)  loss_vfl_aux_0: 0.7023 (0.4814)  loss_bbox_aux_0: 1.1776 (1.4327)  loss_giou_aux_0: 1.4591 (1.5800)  loss_vfl_aux_1: 0.7342 (0.5139)  loss_bbox_aux_1: 1.1566 (1.4034)  loss_giou_aux_1: 1.4327 (1.5599)  loss_vfl_aux_2: 0.7294 (0.5331)  loss_bbox_aux_2: 1.1617 (1.3888)  loss_giou_aux_2: 1.4337 (1.5498)  loss_vfl_aux_3: 0.7425 (0.5450)  loss_bbox_aux_3: 1.1658 (1.3831)  loss_giou_aux_3: 1.4021 (1.5442)  loss_vfl_aux_4: 0.7685 (0.5626)  loss_bbox_aux_4: 1.1617 (1.3749)  loss_giou_aux_4: 1.4207 (1.5416)  loss_vfl_aux_5: 0.6193 (0.4460)  loss_bbox_aux_5: 1.1325 (1.4886)  loss_giou_aux_5: 1.4672 (1.6125)  loss_vfl_dn_0: 0.6222 (0.6633)  loss_bbox_dn_0: 0.9237 (0.9561)  loss_giou_dn_0: 1.3027 (1.3282)  loss_vfl_dn_1: 0.6461 (0.6482)  loss_bbox_dn_1: 0.9324 (0.9724)  loss_giou_dn_1: 1.3016 (1.3320)  loss_vfl_dn_2: 0.6341 (0.6512)  loss_bbox_dn_2: 0.9350 (0.9856)  loss_giou_dn_2: 1.3006 (1.3365)  loss_vfl_dn_3: 0.6471 (0.6404)  loss_bbox_dn_3: 0.9360 (0.9973)  loss_giou_dn_3: 1.3001 (1.3408)  loss_vfl_dn_4: 0.6645 (0.6497)  loss_bbox_dn_4: 0.9358 (1.0081)  loss_giou_dn_4: 1.2996 (1.3461)  loss_vfl_dn_5: 0.6618 (0.6440)  loss_bbox_dn_5: 0.9343 (1.0155)  loss_giou_dn_5: 1.2997 (1.3505)  time: 0.2129  data: 0.0052  max mem: 5412
Epoch: [0]  [  200/29571]  eta: 1:47:10  lr: 0.000010  loss: 38.0393 (41.5753)  loss_vfl: 0.5168 (0.6498)  loss_bbox: 0.9688 (1.2534)  loss_giou: 1.5933 (1.5171)  loss_vfl_aux_0: 0.4588 (0.5684)  loss_bbox_aux_0: 0.9826 (1.2935)  loss_giou_aux_0: 1.6246 (1.5448)  loss_vfl_aux_1: 0.4800 (0.6006)  loss_bbox_aux_1: 0.9987 (1.2763)  loss_giou_aux_1: 1.6132 (1.5295)  loss_vfl_aux_2: 0.4787 (0.6124)  loss_bbox_aux_2: 0.9828 (1.2659)  loss_giou_aux_2: 1.6115 (1.5235)  loss_vfl_aux_3: 0.5297 (0.6356)  loss_bbox_aux_3: 0.9821 (1.2613)  loss_giou_aux_3: 1.6212 (1.5208)  loss_vfl_aux_4: 0.5063 (0.6468)  loss_bbox_aux_4: 0.9715 (1.2564)  loss_giou_aux_4: 1.6052 (1.5185)  loss_vfl_aux_5: 0.4265 (0.5393)  loss_bbox_aux_5: 0.9796 (1.3318)  loss_giou_aux_5: 1.6314 (1.5682)  loss_vfl_dn_0: 0.5431 (0.6194)  loss_bbox_dn_0: 0.6881 (0.9649)  loss_giou_dn_0: 1.3347 (1.3284)  loss_vfl_dn_1: 0.5414 (0.6159)  loss_bbox_dn_1: 0.6883 (0.9738)  loss_giou_dn_1: 1.3352 (1.3311)  loss_vfl_dn_2: 0.5666 (0.6283)  loss_bbox_dn_2: 0.6844 (0.9802)  loss_giou_dn_2: 1.3408 (1.3349)  loss_vfl_dn_3: 0.5653 (0.6219)  loss_bbox_dn_3: 0.6808 (0.9857)  loss_giou_dn_3: 1.3416 (1.3390)  loss_vfl_dn_4: 0.5855 (0.6307)  loss_bbox_dn_4: 0.6770 (0.9908)  loss_giou_dn_4: 1.3413 (1.3443)  loss_vfl_dn_5: 0.5747 (0.6275)  loss_bbox_dn_5: 0.6741 (0.9942)  loss_giou_dn_5: 1.3430 (1.3501)  time: 0.2128  data: 0.0050  max mem: 5412
Epoch: [0]  [  300/29571]  eta: 1:46:54  lr: 0.000010  loss: 40.1431 (41.3087)  loss_vfl: 0.7489 (0.6984)  loss_bbox: 1.0568 (1.2006)  loss_giou: 1.3878 (1.5113)  loss_vfl_aux_0: 0.6950 (0.6116)  loss_bbox_aux_0: 1.0611 (1.2336)  loss_giou_aux_0: 1.3793 (1.5325)  loss_vfl_aux_1: 0.7676 (0.6459)  loss_bbox_aux_1: 1.0393 (1.2199)  loss_giou_aux_1: 1.3919 (1.5205)  loss_vfl_aux_2: 0.7467 (0.6608)  loss_bbox_aux_2: 1.0371 (1.2110)  loss_giou_aux_2: 1.3970 (1.5160)  loss_vfl_aux_3: 0.7226 (0.6848)  loss_bbox_aux_3: 1.0410 (1.2056)  loss_giou_aux_3: 1.3949 (1.5146)  loss_vfl_aux_4: 0.7414 (0.6990)  loss_bbox_aux_4: 1.0346 (1.2029)  loss_giou_aux_4: 1.3814 (1.5116)  loss_vfl_aux_5: 0.6399 (0.5732)  loss_bbox_aux_5: 1.0345 (1.2681)  loss_giou_aux_5: 1.4093 (1.5536)  loss_vfl_dn_0: 0.5083 (0.5856)  loss_bbox_dn_0: 1.1277 (0.9737)  loss_giou_dn_0: 1.3290 (1.3279)  loss_vfl_dn_1: 0.5158 (0.5887)  loss_bbox_dn_1: 1.1297 (0.9793)  loss_giou_dn_1: 1.3279 (1.3298)  loss_vfl_dn_2: 0.5673 (0.6065)  loss_bbox_dn_2: 1.1285 (0.9834)  loss_giou_dn_2: 1.3290 (1.3335)  loss_vfl_dn_3: 0.5798 (0.6047)  loss_bbox_dn_3: 1.1270 (0.9869)  loss_giou_dn_3: 1.3306 (1.3374)  loss_vfl_dn_4: 0.5863 (0.6120)  loss_bbox_dn_4: 1.1256 (0.9902)  loss_giou_dn_4: 1.3316 (1.3421)  loss_vfl_dn_5: 0.5921 (0.6120)  loss_bbox_dn_5: 1.1247 (0.9925)  loss_giou_dn_5: 1.3332 (1.3469)  time: 0.2473  data: 0.0053  max mem: 5413
Epoch: [0]  [  400/29571]  eta: 1:45:51  lr: 0.000010  loss: 37.9034 (40.8858)  loss_vfl: 0.7175 (0.7171)  loss_bbox: 0.9743 (1.1579)  loss_giou: 1.4741 (1.5076)  loss_vfl_aux_0: 0.6903 (0.6277)  loss_bbox_aux_0: 0.9811 (1.1880)  loss_giou_aux_0: 1.4752 (1.5257)  loss_vfl_aux_1: 0.6995 (0.6601)  loss_bbox_aux_1: 0.9661 (1.1754)  loss_giou_aux_1: 1.4996 (1.5161)  loss_vfl_aux_2: 0.6775 (0.6761)  loss_bbox_aux_2: 0.9935 (1.1679)  loss_giou_aux_2: 1.4708 (1.5112)  loss_vfl_aux_3: 0.7175 (0.7001)  loss_bbox_aux_3: 0.9895 (1.1628)  loss_giou_aux_3: 1.4767 (1.5101)  loss_vfl_aux_4: 0.7082 (0.7149)  loss_bbox_aux_4: 0.9849 (1.1601)  loss_giou_aux_4: 1.4660 (1.5076)  loss_vfl_aux_5: 0.6008 (0.5878)  loss_bbox_aux_5: 1.0020 (1.2195)  loss_giou_aux_5: 1.5149 (1.5459)  loss_vfl_dn_0: 0.4568 (0.5584)  loss_bbox_dn_0: 0.8137 (0.9623)  loss_giou_dn_0: 1.3522 (1.3294)  loss_vfl_dn_1: 0.4827 (0.5665)  loss_bbox_dn_1: 0.8178 (0.9660)  loss_giou_dn_1: 1.3405 (1.3313)  loss_vfl_dn_2: 0.5212 (0.5885)  loss_bbox_dn_2: 0.8189 (0.9688)  loss_giou_dn_2: 1.3359 (1.3351)  loss_vfl_dn_3: 0.5441 (0.5914)  loss_bbox_dn_3: 0.8186 (0.9713)  loss_giou_dn_3: 1.3365 (1.3388)  loss_vfl_dn_4: 0.5553 (0.5988)  loss_bbox_dn_4: 0.8174 (0.9738)  loss_giou_dn_4: 1.3355 (1.3429)  loss_vfl_dn_5: 0.5457 (0.6003)  loss_bbox_dn_5: 0.8169 (0.9754)  loss_giou_dn_5: 1.3354 (1.3473)  time: 0.2202  data: 0.0053  max mem: 5413
